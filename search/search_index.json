{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to cyber\u2022Search knowledge base. Right now these docs cover eight open source projects: cybernode - smart node manager and transaction crawler cyber-search - transaction parser for cybernode cyber-markets - toolchain for parsing of orders and trades cyb-js - cyber-search javascript library cyb - indexing and searching information stored in different chains chaingear - create your own Registry of general purpose entries on Ethereum blockchain cyberd - research on the cyber protocol cybercongress - community of of scientists, developers, engineers and craftsmen","title":"Home"},{"location":"contribute/","text":"Current wiki is built on top of mkdocs.org engine with Material for MkDocs extensions pack. Required Installations \u00b6 https://hub.docker.com/r/squidfunk/mkdocs-material/ Commands Cheat Sheet \u00b6 docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build docker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs squidfunk/mkdocs-material gh-deploy Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Contribute"},{"location":"contribute/#required-installations","text":"https://hub.docker.com/r/squidfunk/mkdocs-material/","title":"Required Installations"},{"location":"contribute/#commands-cheat-sheet","text":"docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build docker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs squidfunk/mkdocs-material gh-deploy","title":"Commands Cheat Sheet"},{"location":"contribute/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"cyb/requirements/","text":"Tha main purpose of this document is to describe requirements and the structure of cyb and apps for MVP. Project description \u00b6 Cyber browser - an entrance point to start working with blockchains. Requirements \u00b6 1. Common functional requirements \u00b6 Integration with Metamask IPFS integration Prefered type of data structure - tables 2. Common non-functional requirements \u00b6 Simple and attractive design Intuitive UI Adaptive design for mobile devices Display IPFS hash for all objects All hashes should be easy to copy 3. System requirements \u00b6 less than 1 second for loading page less than 3 seconds for loading all data Web version of browser (React) Desktop version (Electron + React) Mobile web version Browser structure: \u00b6 0 Shell. \u00b6 Search bar, App bar, State bar, Context bar. 0.1 Search (Path) bar \u00b6 Top of the page search in existing list of apps search in blockchain explorer app search in token monitor app search path 0.2 App bar: \u00b6 Left menu list of installed apps (4 blockchain explorers, chaingear, token monitor) add/delete app 0.3 State bar: \u00b6 desync state metamask instelled state (Y/N) tx processing state 0.4 Context bar: \u00b6 account logo account address account balance settings: language location currency (USD/ETH/BTC) activity type (private/anonymous/public) USE CASES: no metamask - install metamask non auth user- log in metamask auth user - account logo 1. Main Page \u00b6 Purpose: accent the user's attention on all browser apps. Design & UI features: simple and attractive design, hints to start usage of cyber products. There is a status text below search panel which describes technical information about cyber.Search products: Search in 134 M transactions in 2 blockchain systems with 135 parsed tokens. Database size : 369 GBs Where: Transactions [number] - number of all indexed transactions from all blockchains connected to Cybernode. Blockchain systems [number] - all blockchains processed by Cybernode. Tokens [numger] - all unique tokens from all blockchains indexed. Database size [number + Gb] - size of Cassandra (index) database. There are 3 main widgets below the status string describing the cryptoeconomy, registers and portfolio. Their apperance depends on user type: User without Metamask Total market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD. Call to action: install Metamask and transit to tokens page . Chaingear registers [number] - number of created registers in Chaingear. Call to action: install Metamask and transit to Chaingear page . Apps [number] - number of browser apps. Call to action: deploy app . User with Metamask (no activities in system) Total market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD. Call to action: transit to tokens page . Chaingear registers [number] - number of created registers in Chaingear. Call to action: transit to Chaingear page, hint to create register or record . Apps [number] - number of browser apps. Call to action: deploy app . User with Metamask (no activities in system) Total market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD. Call to action: transit to tokens page . Chaingear registers [number] - number of created registers/records in Chaingear. Call to action: Chaingear page . Apps [number] - number of browser apps. Call to action: deploy app . At the bottom of the page 5 project links are placed: 1. GitHub - GitHub repository of cyber.Congress [https://github.com/cybercongress] 2. Roadmap - roadmap for cyber.Search project [https://github.com/orgs/cybercongress/projects/1] 3. Cybernode - Cybernode stats page [cybersearch.live] 4. Dashboard - a link user's custom dashboard [] 5. Knowledge - a link to knowledge database of cyber.Search project [cybersearch.io] 2. App list \u00b6 Purpose: accent the user's attention on all browser apps. Design & UI features: simple and attractive design, hints to start usage of apps. There is list of browser apps with categories (with brief overview): Blockchain explorers - Search in 135 million transactions in 4 blockchains Ethereum Explorer Ethereum Classic explorer Bitcoin Explorer Bitcoin Cash explorer Cybernomics - Watch and anayze 135 parsed tokens in 7 exchanges Token Monitor Exchange Monitor Blockchain Monitor Registers - Store your records in blockchains Chaingear Accounting - Manage your cryptoassets Portfolio There are 2 versions of apps: stable released apps developing apps Proposals: donation button for developing apps install to App bar button (in app is not installed) Applications \u00b6 Blockchain explorer Bitcoin Bitcoin Cash Ethereum Ethereum Classic 2. Search Results Page \u00b6 Purpose: provide easy and quick functionality for working with blockchain search. Design & UI features: strictly logical UI, adaptive preferenses of filtration and sorting. Objects of search. \u00b6 There are 2 types of search provided by browser: 1. Global search (searching in whole ecosystem of indexed objects) 2. Context search (searching the data in certain pages) There are 4 systems (blockchains) in which you can find data: Bitcoin Bitcoin Cash Ethereum Ethereum Classic There are 4 types of objects that can be foung in listed systems: Contracts Transactions Blocks Uncle blocks Objects can be found by entering next types of queries: Full hash (address, block, uncle, transaction) Number (block/uncle) The search pannel in general should include next functions: 1. Global and local search (GitHub style) 2. Autocomplete function Search results. \u00b6 Left menu includes next hardcoded functions: Display listed systems (blockchains) Display listed objects Search results in general are shown as a list of object preview. Each object preview its own structure: Transaction: Time (finalization, or time of confirmation or \"Mempool\" status) Hash Value Block + number Time of creation Hash Number of transactions Uncle + number Time of creation Hash Uncle position Contract Time of creation Hash Value Each preview has clickable hash string, that leads to block/uncle, contract or transaction page. There is a infinite page scroll function on results page. 2.1 Contract Page \u00b6 Currently browser shows 2 types of contract pages: Ethereum (Ethereum Classic) contract page Bitcoin (Bitcoin Cash) contract page 2.1.1 Bitcoin contract \u00b6 Displayed data: - Robohash logo - QR code of address hash General UTC Time [date] - time of contract getting into blockchain Balance [number + currency] - BTC available to withdraw Hash [string] - hash of address Cashflow Transactions [number] - number of transactions in contract Unconfirmed transactions [number] - number of transactions in mempool Accumulated income [number + currency] - received BTC Pending income [number + currency] - BTC in mempool transactions Charts: Valuation tab (Regular graph, all above zero): Valuation / Time - balance of contract on each period of time Transactions tab (incoming tx - above zero, outcoming - below): Transactions / Time - activity of transactions by contract on each period of time Transactions and blocks; Transactions tab: UTC Time [date] - time of getting transaction to mempool Hash [string] - hash of transaction Block [number] - number of block Sender[number]- number of inputs Value[number + currency] - total input balance in BTC Receiver [number]- number of outputs Value [number + currency] - total output balance in BTC Fee [number + currency] - accumulated fees in BTC State [string] - \"Confirmed\", \"Mempool\", \"Finalized\" Mined blocks (for miner address only): UTC Time [date] - time of block generation Block [number] - number of mined block Transactions [number] - number of transactions in mined block Reward [number + currency] - rewards for block in BTC Code: - bitcoin scripts Actions: Info by pointing: Time (UTC) - show age of transaction (current time minus mempool) Address - show label of address Clicking Hash string - copy string in buffer Transaction row - expand transaction details (inputs and outputs) Labeling labeling via button \"label it\" 2.1.2 Ethereum contract \u00b6 Displayed data: - Robohash logo - QR code of address hash General: UTC Time [date] - time of contract getting into blockchain Balance [number + currency] - ETH available to withdraw Hash [string] - hash of address Cashflow: Transactions [number] - number of transactions in contract Unconfirmed transactions [number] - number of transactions in mempool Accumulated income [number + currency] - received ETH Pending income [number + currency] - ETH in mempool transactions Charts: Valuation tab (Regular graph, all above zero): Valuation / Time - balance of contract on each period of time Transactions tab (incoming tx - above zero, outcoming - below): Transactions / Time - activity of transactions by contract on each period of time Transactions and blocks; Transactions tab: UTC Time [date] - time of getting transaction to mempool Hash [string] - hash of transaction Block [number] - number of block Sender[hash]- hash of \"from\" address Receiver [hash]- hash of \"to\" address Value [number + currency] - tx value in ETH Fee [number + currency] - accumulated fees in ETH State [string] - \"Confirmed\", \"Mempool\", \"Finalized\" Operations tab (if available): UTC Time [date] - time of getting transaction to mempool Type [string] - type of internal tx (call, delegate call, destroy, create) Sender[hash]- hash of \"from\" address Receiver [hash]- hash of \"to\" address Value [number + currency] - total input balance in ETH Gas used [number] - gas used Gas limit [number] - gas limit State [string] - \"Failed\", \"Reverted\", \"Successful\" Tokens tab: Token [string] - token name Hash [string] - transaction hash Sender [number + currency] - sent tokens Receiver [number + currency] - received tokens Value [number + currency] - sent minus received tokens Mined blocks tab (for miner address only): UTC Time [date] - time of block generation Block [number] - number of mined block Transactions [number] - number of transactions in mined block Reward [number + currency] - rewards for block in ETH Mined uncles tab (if available): UTC Time [date] - time of block generation Hash [string] - uncle hash Block [number] - number of block with uncle Uncle [number] - number of mined uncle Reward [number + currency] - rewards for uncle in ETH Code: Contract code: - Contract name [string] - name of contract - Compiler version [string] - version of compiler Source code - code of contract ABI - contract ABI Swarm code - link in ethereum swarm Actions: Info by pointing: Time - show age of transaction (current time minus mempool) Address - show label of address Clicking Hash string - copy string in buffer Labeling labeling via button \"label it\" 2.2 Transaction Page \u00b6 Currently browser shows 2 types of transaction pages: Ethereum (Ethereum Classic) transaction page Bitcoin (Bitcoin Cash) transaction page 2.2.1 Bitcoin transaction \u00b6 Displayed data: General UTC Time [date] - time of getting transaction to mempool Hash [string] - hash of transaction Value [number + currency] - total transaction value in BTC State [string] - \"Confirmed\", \"Mempool\", \"Finalized\" Blockchain specific Block [number] - number of block Size - [number + bytes] - size of transaction in bytes Confirmations [number] - number of confirmations (for confirmed or finalized transactions) Inputs [number] - number of input addresses Outputs [number] - number of output addresses Fees Fee [number + currency] - accumulated fees in BTC Fee per byte [number + satoshi/Byte] - fee/size Fee per weight unit [number + satoshi/WU] - fee/weight unit Address table. Headers: Senders [string]- input hashes: Value [number + currency] - input value in BTC Receivers [string]- output hashes: Value [number + currency] - output value in BTC Transaction data: Input data [string] - input scripts Output data [string] - output scripts Actions: Info by pointing: Time (UTC) - show age of transaction (current time minus mempool) Confirmations - first confirmation time minus mempool time Clicking Hash string - copy string in buffer Labeling labeling via button \"label it\" 2.2.2 Ethereum transaction \u00b6 Displayed data: General UTC Time [date] - time of getting transaction to mempool Hash [string] - hash of transaction Value [number + currency] - total transaction value in ETH State [string] - \"Confirmed\", \"Mempool\", \"Finalized\" Blockchain specific Block [number] - number of block Nonce [number] - nonce of transaction Size - [number + bytes] - size of transaction in bytes Confirmations [number] - number of confirmations (for confirmed or finalized transactions) Fees Fee [number + currency] - accumulated fees in ETH Gas price [number + wei] - price of gas Gas used [number] - used gas Gas limit [number] - limit of gas Address table. Headers: Sender [string]- input hashes Receiver [string]- output hashes: Value [number + currency] - tx value in ETH Transaction data: Input data [string] - hash Logs [string] - logs of transaction Actions: Info by pointing: Time - show age of transaction (current time minus mempool) Confirmations - first confirmation time minus mempool time Clicking Hash string - copy string in buffer Labeling labeling via button \"label it\" 2.3 Block Page \u00b6 Currently browser shows 3 types of block pages: Ethereum (Ethereum Classic) block page Ethereum (Ethereum Classic) uncle block page Bitcoin (Bitcoin Cash) block page 2.3.1 Bitcoin block \u00b6 Displayed data: Block number [number] (header of page) General UTC Time [date] - time of block generation Hash [string] - hash of block Size [number + bytes] - size of block in bytes Nonce [string] - answer to PoW Transactions[number] - number of transactions in block Blockchain specific Merkle root [string] - hash of merkle tree Version [number] - number of block Mining Miner [string] - miner hash Difficulty [number] - mining difficulty Rewards Static block reward [number + currency] - static reward for block mining in BTC Fees [number + currency] - accumulated fees in BTC Total blobk reward [number + currency] - sum of static reward and fees in BTC Transaction table. Headers: UTC Time [date] - time of getting into block or confirmation minus time of getting into mempool Hash [string] - transaction hash Senders [number] - number in inputs Value [number + currency] - summ of all input values in BTC Receivers [number] - number in outputs Value [number + currency] - summ of all output values in BTC Fee [number + currency] - fees per transaction in BTC Actions: Info by pointing: Time (UTC) - show age of block (current time minus block generation time) Miner - show label of miner Total block reward - string \"Static block reward + Fees\" Clicking Transaction table - expanding tx inputs and outputs by clicking on transaction row (like https://tradeblock.com/bitcoin/block/400000) Next & Previous buttons (top of the page) Hash string - copy string in buffer 2.3.2 Ethereum block \u00b6 Displayed data: Ethereum block number [number] (header of page) General Time [date] - time of block generation Hash [string] - hash of block Size [number + bytes] - size of block in bytes Nonce [string] - answer to PoW Transactions [number] - number of transactions in block Blockchain specific Sha3Uncles [string] - hash of uncles Extra Data [string] - extra mining date Uncles [number] - number of uncle blocks Gas used [number] - used gas Gas limit [number] - limit of gas Mining Miner [string] - miner hash Difficulty [number] - mining difficulty Rewards Static block reward [number + currency] - static reward for block mining in ETH Fees [number + currency] - accumulated fees in ETH Uncle inclusion rewards [number + currency] - rewards in ETH for uncle inclusion Total blobk reward [number + currency] - sum of static reward, fees and uncle inclusion Transaction table. Headers: UTC Time [number + seconds] - time of getting into block or confirmation minus time of getting into mempool Hash [string] - transaction hash Sender [hash] - hash of input Receiver [hash] - hash of output Value [number + currency] - transaction value in ETH Fee [number + currency] - fees per transaction in ETH Uncle table. Headers: Hash [string] - uncle hash Level [number] - uncle position Miner [hash] - hash of miner Reward [number + currency] - rewards of uncle Actions: Info by pointing: Time (UTC) - show age of block (current time minus block generation time) Extra - show converted hash Miner - show label of miner Gas used - 100% * (gas used / gas limit ) Total block reward - string \"Static block reward + Uncle block reward + Fees\" Clicking Next & Previous buttons (top of the page) Hash string - copy string in buffer 2.3.3 Ethereum uncle block \u00b6 Displayed data: Ethereum uncle block number [number] (header of page) Time [date] - time of block generation Hash [string] - hash of uncle block Parent block [number] - number of parent block Parent hash [string] - hash of parent block Level [number] - uncle position Miner [hash] - hash of miner Uncle inclusion rewards [number + currency] - rewards in ETH for uncle inclusion Actions: Info by pointing: Time - show age of block (current time minus block generation time) Miner - show label of miner Clicking Next & Previous buttons (top of the page) Hash string - copy string in buffer 3. Blockchain Monitor \u00b6 4. Token Monitor \u00b6 Token Table: Headers: Name [string] - token name Market cap [number + currency] - market cap of token (price * supply) Price [number + currency] - weightrd price of token Volume (24h) [number + currency] - 24h volume of token trades Supply [number + token ticker] - number of tokens Change (24h) [number + percent] - 24h chdnge of price Price graph (7d) [img] - weekly price change 5. Exchange Monitor \u00b6 6. Chaingear \u00b6 Purpose: provide easy integration with Chaingear. Design & UI features: simple UI, autoupdate register data, preview of changes. All functionality is available after Metamask authorization. Main functions of the page: Watch and label created registers. Create register Edit register (entry) Delete register Transfer the rights of usage to another account Upload content to register via IPFS JSON import of custom fields Real time calculation of registry creation costs Data import from smart contract 8. Labels \u00b6 9. Portfolio \u00b6","title":"Requirements"},{"location":"cyb/requirements/#project-description","text":"Cyber browser - an entrance point to start working with blockchains.","title":"Project description"},{"location":"cyb/requirements/#requirements","text":"","title":"Requirements"},{"location":"cyb/requirements/#1-common-functional-requirements","text":"Integration with Metamask IPFS integration Prefered type of data structure - tables","title":"1. Common functional requirements"},{"location":"cyb/requirements/#2-common-non-functional-requirements","text":"Simple and attractive design Intuitive UI Adaptive design for mobile devices Display IPFS hash for all objects All hashes should be easy to copy","title":"2. Common non-functional requirements"},{"location":"cyb/requirements/#3-system-requirements","text":"less than 1 second for loading page less than 3 seconds for loading all data Web version of browser (React) Desktop version (Electron + React) Mobile web version","title":"3. System requirements"},{"location":"cyb/requirements/#browser-structure","text":"","title":"Browser structure:"},{"location":"cyb/requirements/#0-shell","text":"Search bar, App bar, State bar, Context bar.","title":"0 Shell."},{"location":"cyb/requirements/#01-search-path-bar","text":"Top of the page search in existing list of apps search in blockchain explorer app search in token monitor app search path","title":"0.1 Search (Path) bar"},{"location":"cyb/requirements/#02-app-bar","text":"Left menu list of installed apps (4 blockchain explorers, chaingear, token monitor) add/delete app","title":"0.2 App bar:"},{"location":"cyb/requirements/#03-state-bar","text":"desync state metamask instelled state (Y/N) tx processing state","title":"0.3 State bar:"},{"location":"cyb/requirements/#04-context-bar","text":"account logo account address account balance settings: language location currency (USD/ETH/BTC) activity type (private/anonymous/public) USE CASES: no metamask - install metamask non auth user- log in metamask auth user - account logo","title":"0.4 Context bar:"},{"location":"cyb/requirements/#1-main-page","text":"Purpose: accent the user's attention on all browser apps. Design & UI features: simple and attractive design, hints to start usage of cyber products. There is a status text below search panel which describes technical information about cyber.Search products: Search in 134 M transactions in 2 blockchain systems with 135 parsed tokens. Database size : 369 GBs Where: Transactions [number] - number of all indexed transactions from all blockchains connected to Cybernode. Blockchain systems [number] - all blockchains processed by Cybernode. Tokens [numger] - all unique tokens from all blockchains indexed. Database size [number + Gb] - size of Cassandra (index) database. There are 3 main widgets below the status string describing the cryptoeconomy, registers and portfolio. Their apperance depends on user type: User without Metamask Total market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD. Call to action: install Metamask and transit to tokens page . Chaingear registers [number] - number of created registers in Chaingear. Call to action: install Metamask and transit to Chaingear page . Apps [number] - number of browser apps. Call to action: deploy app . User with Metamask (no activities in system) Total market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD. Call to action: transit to tokens page . Chaingear registers [number] - number of created registers in Chaingear. Call to action: transit to Chaingear page, hint to create register or record . Apps [number] - number of browser apps. Call to action: deploy app . User with Metamask (no activities in system) Total market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD. Call to action: transit to tokens page . Chaingear registers [number] - number of created registers/records in Chaingear. Call to action: Chaingear page . Apps [number] - number of browser apps. Call to action: deploy app . At the bottom of the page 5 project links are placed: 1. GitHub - GitHub repository of cyber.Congress [https://github.com/cybercongress] 2. Roadmap - roadmap for cyber.Search project [https://github.com/orgs/cybercongress/projects/1] 3. Cybernode - Cybernode stats page [cybersearch.live] 4. Dashboard - a link user's custom dashboard [] 5. Knowledge - a link to knowledge database of cyber.Search project [cybersearch.io]","title":"1. Main Page"},{"location":"cyb/requirements/#2-app-list","text":"Purpose: accent the user's attention on all browser apps. Design & UI features: simple and attractive design, hints to start usage of apps. There is list of browser apps with categories (with brief overview): Blockchain explorers - Search in 135 million transactions in 4 blockchains Ethereum Explorer Ethereum Classic explorer Bitcoin Explorer Bitcoin Cash explorer Cybernomics - Watch and anayze 135 parsed tokens in 7 exchanges Token Monitor Exchange Monitor Blockchain Monitor Registers - Store your records in blockchains Chaingear Accounting - Manage your cryptoassets Portfolio There are 2 versions of apps: stable released apps developing apps Proposals: donation button for developing apps install to App bar button (in app is not installed)","title":"2. App list"},{"location":"cyb/requirements/#applications","text":"Blockchain explorer Bitcoin Bitcoin Cash Ethereum Ethereum Classic","title":"Applications"},{"location":"cyb/requirements/#2-search-results-page","text":"Purpose: provide easy and quick functionality for working with blockchain search. Design & UI features: strictly logical UI, adaptive preferenses of filtration and sorting.","title":"2. Search Results Page"},{"location":"cyb/requirements/#objects-of-search","text":"There are 2 types of search provided by browser: 1. Global search (searching in whole ecosystem of indexed objects) 2. Context search (searching the data in certain pages) There are 4 systems (blockchains) in which you can find data: Bitcoin Bitcoin Cash Ethereum Ethereum Classic There are 4 types of objects that can be foung in listed systems: Contracts Transactions Blocks Uncle blocks Objects can be found by entering next types of queries: Full hash (address, block, uncle, transaction) Number (block/uncle) The search pannel in general should include next functions: 1. Global and local search (GitHub style) 2. Autocomplete function","title":"Objects of search."},{"location":"cyb/requirements/#search-results","text":"Left menu includes next hardcoded functions: Display listed systems (blockchains) Display listed objects Search results in general are shown as a list of object preview. Each object preview its own structure: Transaction: Time (finalization, or time of confirmation or \"Mempool\" status) Hash Value Block + number Time of creation Hash Number of transactions Uncle + number Time of creation Hash Uncle position Contract Time of creation Hash Value Each preview has clickable hash string, that leads to block/uncle, contract or transaction page. There is a infinite page scroll function on results page.","title":"Search results."},{"location":"cyb/requirements/#21-contract-page","text":"Currently browser shows 2 types of contract pages: Ethereum (Ethereum Classic) contract page Bitcoin (Bitcoin Cash) contract page","title":"2.1 Contract Page"},{"location":"cyb/requirements/#211-bitcoin-contract","text":"Displayed data: - Robohash logo - QR code of address hash General UTC Time [date] - time of contract getting into blockchain Balance [number + currency] - BTC available to withdraw Hash [string] - hash of address Cashflow Transactions [number] - number of transactions in contract Unconfirmed transactions [number] - number of transactions in mempool Accumulated income [number + currency] - received BTC Pending income [number + currency] - BTC in mempool transactions Charts: Valuation tab (Regular graph, all above zero): Valuation / Time - balance of contract on each period of time Transactions tab (incoming tx - above zero, outcoming - below): Transactions / Time - activity of transactions by contract on each period of time Transactions and blocks; Transactions tab: UTC Time [date] - time of getting transaction to mempool Hash [string] - hash of transaction Block [number] - number of block Sender[number]- number of inputs Value[number + currency] - total input balance in BTC Receiver [number]- number of outputs Value [number + currency] - total output balance in BTC Fee [number + currency] - accumulated fees in BTC State [string] - \"Confirmed\", \"Mempool\", \"Finalized\" Mined blocks (for miner address only): UTC Time [date] - time of block generation Block [number] - number of mined block Transactions [number] - number of transactions in mined block Reward [number + currency] - rewards for block in BTC Code: - bitcoin scripts Actions: Info by pointing: Time (UTC) - show age of transaction (current time minus mempool) Address - show label of address Clicking Hash string - copy string in buffer Transaction row - expand transaction details (inputs and outputs) Labeling labeling via button \"label it\"","title":"2.1.1 Bitcoin contract"},{"location":"cyb/requirements/#212-ethereum-contract","text":"Displayed data: - Robohash logo - QR code of address hash General: UTC Time [date] - time of contract getting into blockchain Balance [number + currency] - ETH available to withdraw Hash [string] - hash of address Cashflow: Transactions [number] - number of transactions in contract Unconfirmed transactions [number] - number of transactions in mempool Accumulated income [number + currency] - received ETH Pending income [number + currency] - ETH in mempool transactions Charts: Valuation tab (Regular graph, all above zero): Valuation / Time - balance of contract on each period of time Transactions tab (incoming tx - above zero, outcoming - below): Transactions / Time - activity of transactions by contract on each period of time Transactions and blocks; Transactions tab: UTC Time [date] - time of getting transaction to mempool Hash [string] - hash of transaction Block [number] - number of block Sender[hash]- hash of \"from\" address Receiver [hash]- hash of \"to\" address Value [number + currency] - tx value in ETH Fee [number + currency] - accumulated fees in ETH State [string] - \"Confirmed\", \"Mempool\", \"Finalized\" Operations tab (if available): UTC Time [date] - time of getting transaction to mempool Type [string] - type of internal tx (call, delegate call, destroy, create) Sender[hash]- hash of \"from\" address Receiver [hash]- hash of \"to\" address Value [number + currency] - total input balance in ETH Gas used [number] - gas used Gas limit [number] - gas limit State [string] - \"Failed\", \"Reverted\", \"Successful\" Tokens tab: Token [string] - token name Hash [string] - transaction hash Sender [number + currency] - sent tokens Receiver [number + currency] - received tokens Value [number + currency] - sent minus received tokens Mined blocks tab (for miner address only): UTC Time [date] - time of block generation Block [number] - number of mined block Transactions [number] - number of transactions in mined block Reward [number + currency] - rewards for block in ETH Mined uncles tab (if available): UTC Time [date] - time of block generation Hash [string] - uncle hash Block [number] - number of block with uncle Uncle [number] - number of mined uncle Reward [number + currency] - rewards for uncle in ETH Code: Contract code: - Contract name [string] - name of contract - Compiler version [string] - version of compiler Source code - code of contract ABI - contract ABI Swarm code - link in ethereum swarm Actions: Info by pointing: Time - show age of transaction (current time minus mempool) Address - show label of address Clicking Hash string - copy string in buffer Labeling labeling via button \"label it\"","title":"2.1.2 Ethereum contract"},{"location":"cyb/requirements/#22-transaction-page","text":"Currently browser shows 2 types of transaction pages: Ethereum (Ethereum Classic) transaction page Bitcoin (Bitcoin Cash) transaction page","title":"2.2 Transaction Page"},{"location":"cyb/requirements/#221-bitcoin-transaction","text":"Displayed data: General UTC Time [date] - time of getting transaction to mempool Hash [string] - hash of transaction Value [number + currency] - total transaction value in BTC State [string] - \"Confirmed\", \"Mempool\", \"Finalized\" Blockchain specific Block [number] - number of block Size - [number + bytes] - size of transaction in bytes Confirmations [number] - number of confirmations (for confirmed or finalized transactions) Inputs [number] - number of input addresses Outputs [number] - number of output addresses Fees Fee [number + currency] - accumulated fees in BTC Fee per byte [number + satoshi/Byte] - fee/size Fee per weight unit [number + satoshi/WU] - fee/weight unit Address table. Headers: Senders [string]- input hashes: Value [number + currency] - input value in BTC Receivers [string]- output hashes: Value [number + currency] - output value in BTC Transaction data: Input data [string] - input scripts Output data [string] - output scripts Actions: Info by pointing: Time (UTC) - show age of transaction (current time minus mempool) Confirmations - first confirmation time minus mempool time Clicking Hash string - copy string in buffer Labeling labeling via button \"label it\"","title":"2.2.1 Bitcoin transaction"},{"location":"cyb/requirements/#222-ethereum-transaction","text":"Displayed data: General UTC Time [date] - time of getting transaction to mempool Hash [string] - hash of transaction Value [number + currency] - total transaction value in ETH State [string] - \"Confirmed\", \"Mempool\", \"Finalized\" Blockchain specific Block [number] - number of block Nonce [number] - nonce of transaction Size - [number + bytes] - size of transaction in bytes Confirmations [number] - number of confirmations (for confirmed or finalized transactions) Fees Fee [number + currency] - accumulated fees in ETH Gas price [number + wei] - price of gas Gas used [number] - used gas Gas limit [number] - limit of gas Address table. Headers: Sender [string]- input hashes Receiver [string]- output hashes: Value [number + currency] - tx value in ETH Transaction data: Input data [string] - hash Logs [string] - logs of transaction Actions: Info by pointing: Time - show age of transaction (current time minus mempool) Confirmations - first confirmation time minus mempool time Clicking Hash string - copy string in buffer Labeling labeling via button \"label it\"","title":"2.2.2 Ethereum transaction"},{"location":"cyb/requirements/#23-block-page","text":"Currently browser shows 3 types of block pages: Ethereum (Ethereum Classic) block page Ethereum (Ethereum Classic) uncle block page Bitcoin (Bitcoin Cash) block page","title":"2.3 Block Page"},{"location":"cyb/requirements/#231-bitcoin-block","text":"Displayed data: Block number [number] (header of page) General UTC Time [date] - time of block generation Hash [string] - hash of block Size [number + bytes] - size of block in bytes Nonce [string] - answer to PoW Transactions[number] - number of transactions in block Blockchain specific Merkle root [string] - hash of merkle tree Version [number] - number of block Mining Miner [string] - miner hash Difficulty [number] - mining difficulty Rewards Static block reward [number + currency] - static reward for block mining in BTC Fees [number + currency] - accumulated fees in BTC Total blobk reward [number + currency] - sum of static reward and fees in BTC Transaction table. Headers: UTC Time [date] - time of getting into block or confirmation minus time of getting into mempool Hash [string] - transaction hash Senders [number] - number in inputs Value [number + currency] - summ of all input values in BTC Receivers [number] - number in outputs Value [number + currency] - summ of all output values in BTC Fee [number + currency] - fees per transaction in BTC Actions: Info by pointing: Time (UTC) - show age of block (current time minus block generation time) Miner - show label of miner Total block reward - string \"Static block reward + Fees\" Clicking Transaction table - expanding tx inputs and outputs by clicking on transaction row (like https://tradeblock.com/bitcoin/block/400000) Next & Previous buttons (top of the page) Hash string - copy string in buffer","title":"2.3.1 Bitcoin block"},{"location":"cyb/requirements/#232-ethereum-block","text":"Displayed data: Ethereum block number [number] (header of page) General Time [date] - time of block generation Hash [string] - hash of block Size [number + bytes] - size of block in bytes Nonce [string] - answer to PoW Transactions [number] - number of transactions in block Blockchain specific Sha3Uncles [string] - hash of uncles Extra Data [string] - extra mining date Uncles [number] - number of uncle blocks Gas used [number] - used gas Gas limit [number] - limit of gas Mining Miner [string] - miner hash Difficulty [number] - mining difficulty Rewards Static block reward [number + currency] - static reward for block mining in ETH Fees [number + currency] - accumulated fees in ETH Uncle inclusion rewards [number + currency] - rewards in ETH for uncle inclusion Total blobk reward [number + currency] - sum of static reward, fees and uncle inclusion Transaction table. Headers: UTC Time [number + seconds] - time of getting into block or confirmation minus time of getting into mempool Hash [string] - transaction hash Sender [hash] - hash of input Receiver [hash] - hash of output Value [number + currency] - transaction value in ETH Fee [number + currency] - fees per transaction in ETH Uncle table. Headers: Hash [string] - uncle hash Level [number] - uncle position Miner [hash] - hash of miner Reward [number + currency] - rewards of uncle Actions: Info by pointing: Time (UTC) - show age of block (current time minus block generation time) Extra - show converted hash Miner - show label of miner Gas used - 100% * (gas used / gas limit ) Total block reward - string \"Static block reward + Uncle block reward + Fees\" Clicking Next & Previous buttons (top of the page) Hash string - copy string in buffer","title":"2.3.2 Ethereum block"},{"location":"cyb/requirements/#233-ethereum-uncle-block","text":"Displayed data: Ethereum uncle block number [number] (header of page) Time [date] - time of block generation Hash [string] - hash of uncle block Parent block [number] - number of parent block Parent hash [string] - hash of parent block Level [number] - uncle position Miner [hash] - hash of miner Uncle inclusion rewards [number + currency] - rewards in ETH for uncle inclusion Actions: Info by pointing: Time - show age of block (current time minus block generation time) Miner - show label of miner Clicking Next & Previous buttons (top of the page) Hash string - copy string in buffer","title":"2.3.3 Ethereum uncle block"},{"location":"cyb/requirements/#3-blockchain-monitor","text":"","title":"3. Blockchain Monitor"},{"location":"cyb/requirements/#4-token-monitor","text":"Token Table: Headers: Name [string] - token name Market cap [number + currency] - market cap of token (price * supply) Price [number + currency] - weightrd price of token Volume (24h) [number + currency] - 24h volume of token trades Supply [number + token ticker] - number of tokens Change (24h) [number + percent] - 24h chdnge of price Price graph (7d) [img] - weekly price change","title":"4. Token Monitor"},{"location":"cyb/requirements/#5-exchange-monitor","text":"","title":"5. Exchange Monitor"},{"location":"cyb/requirements/#6-chaingear","text":"Purpose: provide easy integration with Chaingear. Design & UI features: simple UI, autoupdate register data, preview of changes. All functionality is available after Metamask authorization. Main functions of the page: Watch and label created registers. Create register Edit register (entry) Delete register Transfer the rights of usage to another account Upload content to register via IPFS JSON import of custom fields Real time calculation of registry creation costs Data import from smart contract","title":"6. Chaingear"},{"location":"cyb/requirements/#8-labels","text":"","title":"8. Labels"},{"location":"cyb/requirements/#9-portfolio","text":"","title":"9. Portfolio"},{"location":"cyb/staging/","text":"Staging server \u00b6 To avoid conflicts with other services on the same ports, we run our services on the following ports: cyber-ui :32500 chaingear-api :32600 cyber-search-api :32700 cyber-markets-api :32800 Staging runs 4 components : cyber-ui cyber-search cyber-markets chaingear Each component is composed from different containers . Some containers are shared to save resources. Server setup \u00b6 This is only needed to be done once. SSH to staging server. Make sure you're in docker group: $ groups anatoli docker wheel cyber Checkout cyber-ui repo with staging setup: $ git clone https://github.com/cyberFund/cyber-ui Update running containers \u00b6 Update running containers: $ cd cyber-ui && git pull $ ./devops/staging/up.sh This uses docker-compose to start containers from DockerHub. It doesn't rebuild them. Rebuilding \u00b6 TODO: Move stuff out of CircleCI configs into build scripts + conventions.","title":"Staging"},{"location":"cyb/staging/#staging-server","text":"To avoid conflicts with other services on the same ports, we run our services on the following ports: cyber-ui :32500 chaingear-api :32600 cyber-search-api :32700 cyber-markets-api :32800 Staging runs 4 components : cyber-ui cyber-search cyber-markets chaingear Each component is composed from different containers . Some containers are shared to save resources.","title":"Staging server"},{"location":"cyb/staging/#server-setup","text":"This is only needed to be done once. SSH to staging server. Make sure you're in docker group: $ groups anatoli docker wheel cyber Checkout cyber-ui repo with staging setup: $ git clone https://github.com/cyberFund/cyber-ui","title":"Server setup"},{"location":"cyb/staging/#update-running-containers","text":"Update running containers: $ cd cyber-ui && git pull $ ./devops/staging/up.sh This uses docker-compose to start containers from DockerHub. It doesn't rebuild them.","title":"Update running containers"},{"location":"cyb/staging/#rebuilding","text":"TODO: Move stuff out of CircleCI configs into build scripts + conventions.","title":"Rebuilding"},{"location":"cyb/whitepaper/","text":"cyb Front door to blockchain universe Overview \u00b6 Cyb is a blockchain browser with integrated DApp platform. Browser consists of two main parts: Shell, which alows: search in blockchains and Dapps, and IPFS content deploy and manage Dapps for users show desync state of user and blockchain manage user's account data (sign transactions, create custom feed) DApps, which: use cyber.Search services for data obtaining (cyber.Search, cyber.Markets, cybernode) are stored in IPFS can interract with user and each other Principles \u00b6 There are sereral key principles of cyb behavior, design and development. 1. Users \u00b6 We focus on developers and advanced blockchain users (Metamask users). But cyb is also friendly for everyone who wants to interract with blockchains and decentralized systems. 1.1. User activity types \u00b6 We respect user's attitude and principles. So we offer 3 clear custom types of user behavior: public activity [address + events tracking] private activity [abstract ID + events tracking ] anonymous activity [no tracking at all] For successful browser development we need to collect at least public and private data. Thus users of these 2 groups well be incentivized by giving nice perks. 1.2 User activity incentivization \u00b6 We use full tokenization of processes (data obtain and transfer, content generation and adding). 1.3 Contribution \u00b6 1.4 Donation mechanism \u00b6 1.5 Feeedback and bug collection \u00b6 We use user's feedback to make products better. So we provide options for bug reporting and feedback leaving on every page. 2. Design \u00b6 Cyb design process follows Web3 design principles 2.1. State rules \u00b6 We use simple colored states for states of transactions or operations: mempool/failed - red confirmed/reverted - yellow finalized/succesful - green 2.2 Data visualisation \u00b6 Data should be visualised in a simple and attractive way. No overloaded plots an diagrams, we use animation instead. 2.3 Blockchain objects presenting \u00b6 Blockchain data is too complicated and sometimes not obvious for people. Thus we use adaptive tricks to make work process more convenient: Logical grouping for objects. Every app page has common groups of data (general, blockchain specific) for inheritance of views and better navigation or data observing. Classical accounting terms used for balance and cashflow operations. Blockahains use econimic principles for interaction between subjects thus we can describe such processes in established terms. Robohash logo for contracts entities. Contracts can act by themself, have and algorithms, so it's more natural to perceive them like robots instead of pieces of code. 3. Development \u00b6 3.1 Shell development \u00b6 3.2 DApps development \u00b6 3.3 DApps deployment \u00b6 3.4 DApps interaction \u00b6","title":"Whitepaper"},{"location":"cyb/whitepaper/#overview","text":"Cyb is a blockchain browser with integrated DApp platform. Browser consists of two main parts: Shell, which alows: search in blockchains and Dapps, and IPFS content deploy and manage Dapps for users show desync state of user and blockchain manage user's account data (sign transactions, create custom feed) DApps, which: use cyber.Search services for data obtaining (cyber.Search, cyber.Markets, cybernode) are stored in IPFS can interract with user and each other","title":"Overview"},{"location":"cyb/whitepaper/#principles","text":"There are sereral key principles of cyb behavior, design and development.","title":"Principles"},{"location":"cyb/whitepaper/#1-users","text":"We focus on developers and advanced blockchain users (Metamask users). But cyb is also friendly for everyone who wants to interract with blockchains and decentralized systems.","title":"1. Users"},{"location":"cyb/whitepaper/#11-user-activity-types","text":"We respect user's attitude and principles. So we offer 3 clear custom types of user behavior: public activity [address + events tracking] private activity [abstract ID + events tracking ] anonymous activity [no tracking at all] For successful browser development we need to collect at least public and private data. Thus users of these 2 groups well be incentivized by giving nice perks.","title":"1.1. User activity types"},{"location":"cyb/whitepaper/#12-user-activity-incentivization","text":"We use full tokenization of processes (data obtain and transfer, content generation and adding).","title":"1.2 User activity incentivization"},{"location":"cyb/whitepaper/#13-contribution","text":"","title":"1.3 Contribution"},{"location":"cyb/whitepaper/#14-donation-mechanism","text":"","title":"1.4 Donation mechanism"},{"location":"cyb/whitepaper/#15-feeedback-and-bug-collection","text":"We use user's feedback to make products better. So we provide options for bug reporting and feedback leaving on every page.","title":"1.5 Feeedback and bug collection"},{"location":"cyb/whitepaper/#2-design","text":"Cyb design process follows Web3 design principles","title":"2. Design"},{"location":"cyb/whitepaper/#21-state-rules","text":"We use simple colored states for states of transactions or operations: mempool/failed - red confirmed/reverted - yellow finalized/succesful - green","title":"2.1. State rules"},{"location":"cyb/whitepaper/#22-data-visualisation","text":"Data should be visualised in a simple and attractive way. No overloaded plots an diagrams, we use animation instead.","title":"2.2 Data visualisation"},{"location":"cyb/whitepaper/#23-blockchain-objects-presenting","text":"Blockchain data is too complicated and sometimes not obvious for people. Thus we use adaptive tricks to make work process more convenient: Logical grouping for objects. Every app page has common groups of data (general, blockchain specific) for inheritance of views and better navigation or data observing. Classical accounting terms used for balance and cashflow operations. Blockahains use econimic principles for interaction between subjects thus we can describe such processes in established terms. Robohash logo for contracts entities. Contracts can act by themself, have and algorithms, so it's more natural to perceive them like robots instead of pieces of code.","title":"2.3 Blockchain objects presenting"},{"location":"cyb/whitepaper/#3-development","text":"","title":"3. Development"},{"location":"cyb/whitepaper/#31-shell-development","text":"","title":"3.1 Shell development"},{"location":"cyb/whitepaper/#32-dapps-development","text":"","title":"3.2 DApps development"},{"location":"cyb/whitepaper/#33-dapps-deployment","text":"","title":"3.3 DApps deployment"},{"location":"cyb/whitepaper/#34-dapps-interaction","text":"","title":"3.4 DApps interaction"},{"location":"cyb-js/build/","text":"How to write a NPM module using TypeScript \u00b6 import {DefaultSearchApi, SearchApi} from 'cyber-search-js'; const searchApi: SearchApi = new DefaultSearchApi('http://api.search.cyber.fund'); How to run \u00b6 npm run build publish \u00b6 npm publish","title":"How to write a NPM module using TypeScript"},{"location":"cyb-js/build/#how-to-write-a-npm-module-using-typescript","text":"import {DefaultSearchApi, SearchApi} from 'cyber-search-js'; const searchApi: SearchApi = new DefaultSearchApi('http://api.search.cyber.fund');","title":"How to write a NPM module using TypeScript"},{"location":"cyb-js/build/#how-to-run","text":"npm run build","title":"How to run"},{"location":"cyb-js/build/#publish","text":"npm publish","title":"publish"},{"location":"cyb-js/overwiev/","text":"mission simplify development of distribution and decentralized application for javascript development. Fast cheap and scalable. Integration between blockchain and client side. cli utils? readmap cyber seach integration payment chanel integration bitcoin ethereum buling with cybernode light clents cyber.js is sample wrapper around cyber infrastructure - search, markets and changing also it provide method for work with ipfs and ethereum smart contracts.","title":"Overwiev"},{"location":"cyber-markets/cyber-markets/","text":"About Markets \u00b6","title":"About Markets"},{"location":"cyber-markets/cyber-markets/#about-markets","text":"","title":"About Markets"},{"location":"cyber-markets/api/Readme/","text":"Build and Run docs locally \u00b6 docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Readme"},{"location":"cyber-markets/api/Readme/#build-and-run-docs-locally","text":"docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Build and Run docs locally"},{"location":"cyber-markets/components/overview/","text":"Markets Components \u00b6 Component Scale Status Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back Exchanges Connector 1 - N Connect to CEXs/DEXs for raw data 8080:/actuator/metrics Tickers real-time 1 - N Calculate Tickers From Raw real-time data Tickers historical 1 Calculate Tickers From Raw historical data Storer 1 Write raw data to database Markets REST API 1 - N Rest Api To Markets Entities y Markets Stream API 1 - N not implemented Web Socket Api To Markets Entities y Markets Api Docs 1 - N Markets Api Docs Based On Swagger y Kafka \u00b6 Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality. Elassandra (Elastic + Cassandra) \u00b6 Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data. Exchanges Connector \u00b6 Collect raw data from centralized and decentralized exchanges such as trades, order books and put it to kafka. Storer \u00b6 Writes data from kafka topics or directly from exchanges-connector to cassandra cluster Tickers real-time \u00b6 Calculate tickers from trades which collecting exchanges connectors module. By default, the recalculation occurs every 3 seconds. Trades that do not hit the gap in 3 seconds are skipped. Tickers historical \u00b6 Calculate tickers, prices, etc and aggregate raw data which collecting exchanges connectors module. Tickers are calculated from data which stored in the Elassandra.","title":"Markets Components"},{"location":"cyber-markets/components/overview/#markets-components","text":"Component Scale Status Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back Exchanges Connector 1 - N Connect to CEXs/DEXs for raw data 8080:/actuator/metrics Tickers real-time 1 - N Calculate Tickers From Raw real-time data Tickers historical 1 Calculate Tickers From Raw historical data Storer 1 Write raw data to database Markets REST API 1 - N Rest Api To Markets Entities y Markets Stream API 1 - N not implemented Web Socket Api To Markets Entities y Markets Api Docs 1 - N Markets Api Docs Based On Swagger y","title":"Markets Components"},{"location":"cyber-markets/components/overview/#kafka","text":"Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality.","title":"Kafka"},{"location":"cyber-markets/components/overview/#elassandra-elastic-cassandra","text":"Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data.","title":"Elassandra (Elastic + Cassandra)"},{"location":"cyber-markets/components/overview/#exchanges-connector","text":"Collect raw data from centralized and decentralized exchanges such as trades, order books and put it to kafka.","title":"Exchanges Connector"},{"location":"cyber-markets/components/overview/#storer","text":"Writes data from kafka topics or directly from exchanges-connector to cassandra cluster","title":"Storer"},{"location":"cyber-markets/components/overview/#tickers-real-time","text":"Calculate tickers from trades which collecting exchanges connectors module. By default, the recalculation occurs every 3 seconds. Trades that do not hit the gap in 3 seconds are skipped.","title":"Tickers real-time"},{"location":"cyber-markets/components/overview/#tickers-historical","text":"Calculate tickers, prices, etc and aggregate raw data which collecting exchanges connectors module. Tickers are calculated from data which stored in the Elassandra.","title":"Tickers historical"},{"location":"cyber-markets/contributing/cheat-sheet/","text":"Kafka \u00b6 Stop kafka and delete kafka data(cheat sheet) \u00b6 docker stop fast-data-dev-markets docker rm fast-data-dev-markets Elassandra \u00b6 Stop elassandra and delete elassandra data(cheat sheet) \u00b6 docker stop elassandra-markets docker rm elassandra-markets Chains \u00b6 Run parity node(cheat sheet) \u00b6 sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4","title":"Cheat sheet"},{"location":"cyber-markets/contributing/cheat-sheet/#kafka","text":"","title":"Kafka"},{"location":"cyber-markets/contributing/cheat-sheet/#stop-kafka-and-delete-kafka-datacheat-sheet","text":"docker stop fast-data-dev-markets docker rm fast-data-dev-markets","title":"Stop kafka and delete kafka data(cheat sheet)"},{"location":"cyber-markets/contributing/cheat-sheet/#elassandra","text":"","title":"Elassandra"},{"location":"cyber-markets/contributing/cheat-sheet/#stop-elassandra-and-delete-elassandra-datacheat-sheet","text":"docker stop elassandra-markets docker rm elassandra-markets","title":"Stop elassandra and delete elassandra data(cheat sheet)"},{"location":"cyber-markets/contributing/cheat-sheet/#chains","text":"","title":"Chains"},{"location":"cyber-markets/contributing/cheat-sheet/#run-parity-nodecheat-sheet","text":"sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4","title":"Run parity node(cheat sheet)"},{"location":"cyber-markets/contributing/contributing/","text":"Contributing to Cyber Markets \u00b6 Thank you for considering a contribution to Cyber Markets! This guide explains how to: Get started Development workflow * Get help if you encounter trouble Get in touch \u00b6 Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time) Development Workflow \u00b6 Development Setup \u00b6 Please, use development environment setup guide . Make Changes \u00b6 Use this Architecture Overview as a start point for making changes. Local Check \u00b6 Several checks should passed to succeed build. Detekt code analyze tool should not report any issues JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows Creating Commits And Writing Commit Messages \u00b6 The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section Submitting Your Change \u00b6 After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy. Getting Help \u00b6 If you run into any trouble, please reach out to us on the issue you are working on. Our Thanks \u00b6 We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Contributing to Cyber Markets"},{"location":"cyber-markets/contributing/contributing/#contributing-to-cyber-markets","text":"Thank you for considering a contribution to Cyber Markets! This guide explains how to: Get started Development workflow * Get help if you encounter trouble","title":"Contributing to Cyber Markets"},{"location":"cyber-markets/contributing/contributing/#get-in-touch","text":"Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time)","title":"Get in touch"},{"location":"cyber-markets/contributing/contributing/#development-workflow","text":"","title":"Development Workflow"},{"location":"cyber-markets/contributing/contributing/#development-setup","text":"Please, use development environment setup guide .","title":"Development Setup"},{"location":"cyber-markets/contributing/contributing/#make-changes","text":"Use this Architecture Overview as a start point for making changes.","title":"Make Changes"},{"location":"cyber-markets/contributing/contributing/#local-check","text":"Several checks should passed to succeed build. Detekt code analyze tool should not report any issues JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows","title":"Local Check"},{"location":"cyber-markets/contributing/contributing/#creating-commits-and-writing-commit-messages","text":"The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section","title":"Creating Commits And Writing Commit Messages"},{"location":"cyber-markets/contributing/contributing/#submitting-your-change","text":"After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy.","title":"Submitting Your Change"},{"location":"cyber-markets/contributing/contributing/#getting-help","text":"If you run into any trouble, please reach out to us on the issue you are working on.","title":"Getting Help"},{"location":"cyber-markets/contributing/contributing/#our-thanks","text":"We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Our Thanks"},{"location":"cyber-markets/contributing/dev-environment/","text":"Development environment \u00b6 Useful Links \u00b6 cheat sheet Prestart \u00b6 Instal Java 8 JDK Install Docker and Docker Compose Install Intellij Idea Run Kafka, Elassandra, Prometheus and Grafana \u00b6 Start containers(required) \u00b6 For mac: docker-compose -f dev-environment/env-mac.yml up -d For linux family: docker-compose -f dev-environment/env.yml up -d Bootstrap Elassandra with keyspaces(required) \u00b6 docker cp dev-environment/elassandra-bootstrap.cql elassandra-markets:/elassandra-bootstrap.cql docker exec -it elassandra-markets bash cqlsh -f elassandra-bootstrap.cql Import project to Intellij Idea \u00b6 Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation Run Exchanges Connector, Tickers, or API from intellij Idea \u00b6 Go to ExchangesConnectorApplication.kt and press green triangle on left to the code (on example line 16): If, you use parity endpoint different rather that localhost:8545, than Etherdelta connector will fail due to lack of environment property PARITY_URL. Let's define it: Select \"Edit Configurations\" Add next properties: Now, run exchanges connector one more time then etherdelta connector should start. You can add environment variables in the same way for Tickers, APIs and etc.","title":"Development environment"},{"location":"cyber-markets/contributing/dev-environment/#development-environment","text":"","title":"Development environment"},{"location":"cyber-markets/contributing/dev-environment/#useful-links","text":"cheat sheet","title":"Useful Links"},{"location":"cyber-markets/contributing/dev-environment/#prestart","text":"Instal Java 8 JDK Install Docker and Docker Compose Install Intellij Idea","title":"Prestart"},{"location":"cyber-markets/contributing/dev-environment/#run-kafka-elassandra-prometheus-and-grafana","text":"","title":"Run Kafka, Elassandra, Prometheus and Grafana"},{"location":"cyber-markets/contributing/dev-environment/#start-containersrequired","text":"For mac: docker-compose -f dev-environment/env-mac.yml up -d For linux family: docker-compose -f dev-environment/env.yml up -d","title":"Start containers(required)"},{"location":"cyber-markets/contributing/dev-environment/#bootstrap-elassandra-with-keyspacesrequired","text":"docker cp dev-environment/elassandra-bootstrap.cql elassandra-markets:/elassandra-bootstrap.cql docker exec -it elassandra-markets bash cqlsh -f elassandra-bootstrap.cql","title":"Bootstrap Elassandra with keyspaces(required)"},{"location":"cyber-markets/contributing/dev-environment/#import-project-to-intellij-idea","text":"Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation","title":"Import project to Intellij Idea"},{"location":"cyber-markets/contributing/dev-environment/#run-exchanges-connector-tickers-or-api-from-intellij-idea","text":"Go to ExchangesConnectorApplication.kt and press green triangle on left to the code (on example line 16): If, you use parity endpoint different rather that localhost:8545, than Etherdelta connector will fail due to lack of environment property PARITY_URL. Let's define it: Select \"Edit Configurations\" Add next properties: Now, run exchanges connector one more time then etherdelta connector should start. You can add environment variables in the same way for Tickers, APIs and etc.","title":"Run Exchanges Connector, Tickers, or API from intellij Idea"},{"location":"cyber-search/cyber-search/","text":"About Search \u00b6","title":"About Search"},{"location":"cyber-search/cyber-search/#about-search","text":"","title":"About Search"},{"location":"cyber-search/api/","text":"Build and Run locally \u00b6 docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Index"},{"location":"cyber-search/api/#build-and-run-locally","text":"docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Build and Run locally"},{"location":"cyber-search/components/bitcoin-components/","text":"Bitcoin Components \u00b6 Component Scale Description Cluster Address Metrics External Bitcoin Pump 1 Bitcoin Chain Data Kafka Pump 8080:/actuator/metrics Bitcoin Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Bitcoin Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics Bitcoin Pump \u00b6 Pumps Bitcoin raw data(block,tx,uncles) into Kafka. Bitcoin Cassandra Dump \u00b6 Consumes Bitcoin raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra. Bitcoin Contract Summary \u00b6 Collects summary for each contract stored in Bitcoin chain. Consumes Kafka topics and update data in Elassandra.","title":"Bitcoin Components"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-components","text":"Component Scale Description Cluster Address Metrics External Bitcoin Pump 1 Bitcoin Chain Data Kafka Pump 8080:/actuator/metrics Bitcoin Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Bitcoin Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics","title":"Bitcoin Components"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-pump","text":"Pumps Bitcoin raw data(block,tx,uncles) into Kafka.","title":"Bitcoin Pump"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-cassandra-dump","text":"Consumes Bitcoin raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.","title":"Bitcoin Cassandra Dump"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-contract-summary","text":"Collects summary for each contract stored in Bitcoin chain. Consumes Kafka topics and update data in Elassandra.","title":"Bitcoin Contract Summary"},{"location":"cyber-search/components/cassandra-service/","text":"Cassandra Service \u00b6 Usage of cassandra-service Module \u00b6 \u0421assandra service module could be used in two ways: With specifying CHAIN_FAMILY environment variable Without specifying CHAIN_FAMILY environment variable Specifying CHAIN_FAMILY \u00b6 When you're specifying CHAIN_FAMILY environment variable cassandra-service module will work in the context of one keyspace based on this family. For example if CHAIN_FAMILY=BITCOIN it will create bitcoin keyspace if it doesn't exists and do all migrations for this keyspace if they're not applied. In spring context only repositories beans related to this keyspace will be available All of the cassandra work handled by CassandraRepositoriesConfiguration and it specific inheritor. This kind of interaction with cassandra-service used in dumps and contract summaries modules. Without CHAIN_FAMILY \u00b6 Without specifying CHAIN_FAMILY environment variable cassandra-service module will act like this: It will run no migrations and creations of keyspaces It will look on available keyspaces in current cassandra database based on SearchRepositoriesConfiguration inheritors and if they're exists it will initialize all repositories beans for this keyspaces and put them into spring context. This is useful when you want to interact with all of available keyspaces and indicies. For example in search-api module we need all of repositories beans to build requests.","title":"Cassandra Service"},{"location":"cyber-search/components/cassandra-service/#cassandra-service","text":"","title":"Cassandra Service"},{"location":"cyber-search/components/cassandra-service/#usage-of-cassandra-service-module","text":"\u0421assandra service module could be used in two ways: With specifying CHAIN_FAMILY environment variable Without specifying CHAIN_FAMILY environment variable","title":"Usage of cassandra-service Module"},{"location":"cyber-search/components/cassandra-service/#specifying-chain_family","text":"When you're specifying CHAIN_FAMILY environment variable cassandra-service module will work in the context of one keyspace based on this family. For example if CHAIN_FAMILY=BITCOIN it will create bitcoin keyspace if it doesn't exists and do all migrations for this keyspace if they're not applied. In spring context only repositories beans related to this keyspace will be available All of the cassandra work handled by CassandraRepositoriesConfiguration and it specific inheritor. This kind of interaction with cassandra-service used in dumps and contract summaries modules.","title":"Specifying CHAIN_FAMILY"},{"location":"cyber-search/components/cassandra-service/#without-chain_family","text":"Without specifying CHAIN_FAMILY environment variable cassandra-service module will act like this: It will run no migrations and creations of keyspaces It will look on available keyspaces in current cassandra database based on SearchRepositoriesConfiguration inheritors and if they're exists it will initialize all repositories beans for this keyspaces and put them into spring context. This is useful when you want to interact with all of available keyspaces and indicies. For example in search-api module we need all of repositories beans to build requests.","title":"Without CHAIN_FAMILY"},{"location":"cyber-search/components/custom-chain-name/","text":"Custom Chain Name \u00b6 You have an ability to run our services with custom chain name based on supported chain families. It may be needed if you have your own custom chain and you want API endpoints, Kafka topic names and Cassandra keyspaces to use your chain name. Currently we're supporting BITCOIN and ETHEREUM chain families. To make it work you need to define CHAIN_NAME environment variable along with CHAIN_FAMILY . Also you may need to define your chain node url in PUMP by setting CHAIN_NODE_URL environment variable. For example if you want to index ETHEREUM based chain called MY_PRECIOUS . You have to use following config in all services ( CHAIN_NODE_URL is necessary only for PUMP service): CHAIN_FAMILY = ETHEREUM CHAIN_NAME = MY_PRECIOUS CHAIN_NODE_URL = https: //my_precious_node_url:port Then all Kafka topics will be named as MY_PRECIOUS_TX_PUMP , MY_PRECIOUS_BLOCK_PUMP , MY_PRECIOUS_UNCLE_PUMP . Cassandra keyspace will be named my_precious . And all API url will be build like this http://localhost:8080/my_precious/block/42 and etc.","title":"Custom Chain Name"},{"location":"cyber-search/components/custom-chain-name/#custom-chain-name","text":"You have an ability to run our services with custom chain name based on supported chain families. It may be needed if you have your own custom chain and you want API endpoints, Kafka topic names and Cassandra keyspaces to use your chain name. Currently we're supporting BITCOIN and ETHEREUM chain families. To make it work you need to define CHAIN_NAME environment variable along with CHAIN_FAMILY . Also you may need to define your chain node url in PUMP by setting CHAIN_NODE_URL environment variable. For example if you want to index ETHEREUM based chain called MY_PRECIOUS . You have to use following config in all services ( CHAIN_NODE_URL is necessary only for PUMP service): CHAIN_FAMILY = ETHEREUM CHAIN_NAME = MY_PRECIOUS CHAIN_NODE_URL = https: //my_precious_node_url:port Then all Kafka topics will be named as MY_PRECIOUS_TX_PUMP , MY_PRECIOUS_BLOCK_PUMP , MY_PRECIOUS_UNCLE_PUMP . Cassandra keyspace will be named my_precious . And all API url will be build like this http://localhost:8080/my_precious/block/42 and etc.","title":"Custom Chain Name"},{"location":"cyber-search/components/ethereum-components/","text":"Ethereum Components \u00b6 Component Scale Description Cluster Address Metrics External Ethereum Pump 1 Ethereum Chain Data Kafka Pump 8080:/actuator/metrics Ethereum Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Ethereum Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics Ethereum Pump \u00b6 Pumps Ethereum raw data(block,tx,uncles) into Kafka. Ethereum Cassandra Dump \u00b6 Consumes Ethereum raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra. Ethereum Contract Summary \u00b6 Collects summary for each contract stored in Ethereum chain. Consumes Kafka topics and update data in Elassandra.","title":"Ethereum Components"},{"location":"cyber-search/components/ethereum-components/#ethereum-components","text":"Component Scale Description Cluster Address Metrics External Ethereum Pump 1 Ethereum Chain Data Kafka Pump 8080:/actuator/metrics Ethereum Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Ethereum Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics","title":"Ethereum Components"},{"location":"cyber-search/components/ethereum-components/#ethereum-pump","text":"Pumps Ethereum raw data(block,tx,uncles) into Kafka.","title":"Ethereum Pump"},{"location":"cyber-search/components/ethereum-components/#ethereum-cassandra-dump","text":"Consumes Ethereum raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.","title":"Ethereum Cassandra Dump"},{"location":"cyber-search/components/ethereum-components/#ethereum-contract-summary","text":"Collects summary for each contract stored in Ethereum chain. Consumes Kafka topics and update data in Elassandra.","title":"Ethereum Contract Summary"},{"location":"cyber-search/components/search-common-components/","text":"Search Common Components \u00b6 Component Scale Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back And Search elassandra.search.svc:9042 Search Api 1 - N Api To Search Chain Entities search-api.search.svc:80 8080:/actuator/metrics y Search Api Docs 1 - N Search Api Docs Based On Swagger search-api-docs.search.svc:80 y Chains Components \u00b6 Ethereum Bitcoin Kafka \u00b6 Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality. Elassandra (Elastic + Cassandra) \u00b6 Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data. Search Api \u00b6 Main search access endpoint. See api calls documentations .","title":"Search Common Components"},{"location":"cyber-search/components/search-common-components/#search-common-components","text":"Component Scale Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back And Search elassandra.search.svc:9042 Search Api 1 - N Api To Search Chain Entities search-api.search.svc:80 8080:/actuator/metrics y Search Api Docs 1 - N Search Api Docs Based On Swagger search-api-docs.search.svc:80 y","title":"Search Common Components"},{"location":"cyber-search/components/search-common-components/#chains-components","text":"Ethereum Bitcoin","title":"Chains Components"},{"location":"cyber-search/components/search-common-components/#kafka","text":"Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality.","title":"Kafka"},{"location":"cyber-search/components/search-common-components/#elassandra-elastic-cassandra","text":"Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data.","title":"Elassandra (Elastic + Cassandra)"},{"location":"cyber-search/components/search-common-components/#search-api","text":"Main search access endpoint. See api calls documentations .","title":"Search Api"},{"location":"cyber-search/contributing/cheat-sheet/","text":"Kafka \u00b6 Stop kafka and delete kafka data(cheat sheet) \u00b6 docker stop fast-data-dev-search docker rm fast-data-dev-search Elassandra \u00b6 Stop elassandra and delete elassandra data(cheat sheet) \u00b6 docker stop elassandra-search docker rm elassandra-search Get indices info \u00b6 curl -XGET 'localhost:9200/_cat/indices?v&pretty' Chains \u00b6 Run parity node \u00b6 sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4 Run bitcoind node \u00b6 docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0","title":"Cheat sheet"},{"location":"cyber-search/contributing/cheat-sheet/#kafka","text":"","title":"Kafka"},{"location":"cyber-search/contributing/cheat-sheet/#stop-kafka-and-delete-kafka-datacheat-sheet","text":"docker stop fast-data-dev-search docker rm fast-data-dev-search","title":"Stop kafka and delete kafka data(cheat sheet)"},{"location":"cyber-search/contributing/cheat-sheet/#elassandra","text":"","title":"Elassandra"},{"location":"cyber-search/contributing/cheat-sheet/#stop-elassandra-and-delete-elassandra-datacheat-sheet","text":"docker stop elassandra-search docker rm elassandra-search","title":"Stop elassandra and delete elassandra data(cheat sheet)"},{"location":"cyber-search/contributing/cheat-sheet/#get-indices-info","text":"curl -XGET 'localhost:9200/_cat/indices?v&pretty'","title":"Get indices info"},{"location":"cyber-search/contributing/cheat-sheet/#chains","text":"","title":"Chains"},{"location":"cyber-search/contributing/cheat-sheet/#run-parity-node","text":"sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4","title":"Run parity node"},{"location":"cyber-search/contributing/cheat-sheet/#run-bitcoind-node","text":"docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0","title":"Run bitcoind node"},{"location":"cyber-search/contributing/contributing/","text":"Contributing to Cyber Search \u00b6 Thank you for considering a contribution to Cyber Search! This guide explains how to: Get started Development workflow * Get help if you encounter trouble Get in touch \u00b6 Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time) Development Workflow \u00b6 Development Setup \u00b6 Please, use development environment setup guide . Make Changes \u00b6 Use this Architecture Overview as a start point for making changes. Local Check \u00b6 Several checks should passed to succeed build. Detekt code analyze tool should not report any issues JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows Creating Commits And Writing Commit Messages \u00b6 The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: solidity/CONTRIBUTING.md Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section Submitting Your Change \u00b6 After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy. Getting Help \u00b6 If you run into any trouble, please reach out to us on the issue you are working on. Our Thanks \u00b6 We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Contributing to Cyber Search [![GitHub contributors](https://img.shields.io/github/contributors/cybercongress/cyber-search.svg?style=flat-square&longCache=true)](https://github.com/cybercongress/cyber-search)"},{"location":"cyber-search/contributing/contributing/#contributing-to-cyber-search","text":"Thank you for considering a contribution to Cyber Search! This guide explains how to: Get started Development workflow * Get help if you encounter trouble","title":"Contributing to Cyber Search"},{"location":"cyber-search/contributing/contributing/#get-in-touch","text":"Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time)","title":"Get in touch"},{"location":"cyber-search/contributing/contributing/#development-workflow","text":"","title":"Development Workflow"},{"location":"cyber-search/contributing/contributing/#development-setup","text":"Please, use development environment setup guide .","title":"Development Setup"},{"location":"cyber-search/contributing/contributing/#make-changes","text":"Use this Architecture Overview as a start point for making changes.","title":"Make Changes"},{"location":"cyber-search/contributing/contributing/#local-check","text":"Several checks should passed to succeed build. Detekt code analyze tool should not report any issues JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows","title":"Local Check"},{"location":"cyber-search/contributing/contributing/#creating-commits-and-writing-commit-messages","text":"The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: solidity/CONTRIBUTING.md Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section","title":"Creating Commits And Writing Commit Messages"},{"location":"cyber-search/contributing/contributing/#submitting-your-change","text":"After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy.","title":"Submitting Your Change"},{"location":"cyber-search/contributing/contributing/#getting-help","text":"If you run into any trouble, please reach out to us on the issue you are working on.","title":"Getting Help"},{"location":"cyber-search/contributing/contributing/#our-thanks","text":"We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Our Thanks"},{"location":"cyber-search/contributing/dev-environment/","text":"Development environment \u00b6 Useful Links \u00b6 cheat sheet Prestart \u00b6 Install Java 8 JDK Install Docker and Docker Compose Install Intellij Idea Run Kafka, Elassandra, Prometheus and Grafana \u00b6 Start containers(required) \u00b6 For mac: cd dev-environment docker-compose -f env-mac.yml up -d For linux family: cd dev-environment docker-compose -f env.yml up -d Run chain node (only for pumps) \u00b6 In order to fetch data from chains pumps need chain node to interact with. To run chain node locally using docker use following commands: Parity for Ethereum sudo docker run -d -p 8545:8545 --name parity_eth \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4 Bitcoind for Bitcoin docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0 Or you could use any public available node (with appropriate settings) by passing CHAIN_NODE_URL environment variable to pump. For example CHAIN_NODE_URL=http://127.0.0.1:8545 . Import project to Intellij Idea \u00b6 Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation Run Ethereum Pump from intellij Idea \u00b6 Go to EthereumPumpApplication.kt and press green triangle on left to the code (on example line 14): Pump will fail due to lack of CHAIN_FAMILY environment property, let's define it: Select \"Edit Configuration\" Add properties: Now, run pump one more time, it should start.","title":"Development environment"},{"location":"cyber-search/contributing/dev-environment/#development-environment","text":"","title":"Development environment"},{"location":"cyber-search/contributing/dev-environment/#useful-links","text":"cheat sheet","title":"Useful Links"},{"location":"cyber-search/contributing/dev-environment/#prestart","text":"Install Java 8 JDK Install Docker and Docker Compose Install Intellij Idea","title":"Prestart"},{"location":"cyber-search/contributing/dev-environment/#run-kafka-elassandra-prometheus-and-grafana","text":"","title":"Run Kafka, Elassandra, Prometheus and Grafana"},{"location":"cyber-search/contributing/dev-environment/#start-containersrequired","text":"For mac: cd dev-environment docker-compose -f env-mac.yml up -d For linux family: cd dev-environment docker-compose -f env.yml up -d","title":"Start containers(required)"},{"location":"cyber-search/contributing/dev-environment/#run-chain-node-only-for-pumps","text":"In order to fetch data from chains pumps need chain node to interact with. To run chain node locally using docker use following commands: Parity for Ethereum sudo docker run -d -p 8545:8545 --name parity_eth \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4 Bitcoind for Bitcoin docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0 Or you could use any public available node (with appropriate settings) by passing CHAIN_NODE_URL environment variable to pump. For example CHAIN_NODE_URL=http://127.0.0.1:8545 .","title":"Run chain node (only for pumps)"},{"location":"cyber-search/contributing/dev-environment/#import-project-to-intellij-idea","text":"Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation","title":"Import project to Intellij Idea"},{"location":"cyber-search/contributing/dev-environment/#run-ethereum-pump-from-intellij-idea","text":"Go to EthereumPumpApplication.kt and press green triangle on left to the code (on example line 14): Pump will fail due to lack of CHAIN_FAMILY environment property, let's define it: Select \"Edit Configuration\" Add properties: Now, run pump one more time, it should start.","title":"Run Ethereum Pump from intellij Idea"},{"location":"cyber-search/contributing/pump-development/","text":"Pump Development Guide \u00b6 To develop your own chain pump you have two deal with two modules: common pumps Some explanations \u00b6 To describe single chain in our system we're using following scheme. Every chain has ChainFamily . For example BITCOIN, ETHEREUM families. ChainFamily has it's own set of entities like TX, BLOCK, UNCLE and default url of chain node. Next, chain has a name. Chain name used everywhere in modules. Kafka topics, Cassandra keyspaces, API endpoints will be based on chain name. By default chain name is equal to ChainFamily . For example if you have ChainFamily == BITCOIN and you not specifying a chain name it will be set to BITCOIN . Main goal of chain name is to separate forks of one chain that have the same structure. For example BITCOIN and BITCOIN_CASH or ETHEREUM and ETHEREUM_CLASSIC . So as far as you can understand single chain pump is chain pump for specific ChainFamily Adding models to common module \u00b6 First of all you have to put models representing your blockchain entities in common module. They should be placed in fund.cyber.search.model.{your_chain_family_name} package. This models will be used to transfer your blockchain data among our microservices (using Kafka). After doing so put description of your chain family to ChainFamily enum. You could find it in fund.cyber.search.model.chains.ChainInfo.kt file. Description includes following info: defaultNodeUrl - default URL of blockchain node to connect for data. entityTypes - map of blockchain entities types (from fund.cyber.search.model.chains.ChainEntity enum) to their class representation. If you can't find needed entity type in fund.cyber.search.model.chains.ChainEntity enum you have to add it by your own. Developing pump \u00b6 Now you're ready to write pump for your chain family. Note that this pump is suitable only for block based chains. We're using Spring Boot (https://projects.spring.io/spring-boot/). So you also should be familiar with spring beans, spring dependency injection and spring configuration. Creating gradle module \u00b6 To start with, create new Gradle submodule with name of your chain in pumps module. Include your module in pumps/build.gradle and in settings.gradle . Example of pumps/build.gradle file with your module: project ( \":pumps:common\" ) { apply plugin: \"io.spring.dependency-management\" jar . archiveName = \"common-pumps\" dependencies { ... } } ... project ( \":pumps:{your_module_name}\" ) { apply plugin: \"org.springframework.boot\" dependencies { compile project ( \":pumps:common\" ) //your module dependencies } } ... Example of settings.gradle file: include \"common\" include \"common-kafka\" include \"cassandra-service\" ... include \"pumps:common\" include \"pumps:bitcoin\" include \"pumps:ethereum\" include \"pumps:{your_chain_name}\" ... All classes should be placed under fund.cyber.pump.{your_chain_name} package (except main class). Creating spring boot main class \u00b6 The next step is creating spring boot main class. It should be placed in your module under fund.cyber package. Example of main class: @SpringBootApplication ( exclude = [ KafkaAutoConfiguration :: class ]) class BitcoinPumpApplication { companion object { @JvmStatic fun main ( args : Array < String >) { val application = SpringApplication ( BitcoinPumpApplication :: class . java ) application . runPump ( args ) } } } Default spring beans in context \u00b6 After spring context start you'll have few already configured beans in context that you could inject in your module classes: org.springframework.retry.support.RetryTemplate - Spring Retry template for retrying failed operations. io.micrometer.core.instrument.MeterRegistry - bean for monitoring. fund.cyber.search.model.chains.ChainInfo - bean with all needed properties of running chain. class ChainInfo ( val family : ChainFamily , val name : String = \"\" , val nodeUrl : String = family . defaultNodeUrl ) { val fullName get () = family . name + if ( name . isEmpty ()) \"\" else \"_$name\" val entityTypes get () = family . entityTypes . keys fun entityClassByType ( type : ChainEntityType ) = family . entityTypes [ type ] } ChainInfo bean constructed from environment properties at start: CHAIN_FAMILY - chain family name (matches ChainFamily enum). For example CHAIN_FAMILY=BITCOIN . Required option . CHAIN_NAME - name of your specific chain. Not necessary . By default equals to CHAIN_FAMILY . All kafka topic will be named accordingly. For example CHAIN_NAME=BITCOIN_CASH then a topic name will be BITCOIN_CASH_TX_PUMP . * CHAIN_NODE_URL - URL of node running your chain. Not necessary . By default will be equal to one that described in your ChainFamily . Implementing pump \u00b6 To integrate your pump with our system you simply have to implement two interfaces: * fund.cyber.pump.common.node.BlockBundle /** * Blockchain block with all dependent entities. Should collect all entities in scope of one block. * For example: transactions, uncles, etc.. */ interface BlockBundle { /** * Hash of the block */ val hash : String /** * Hash of the parent block */ val parentHash : String /** * Number of the block in blockchain */ val number : Long /** * Size of the block in bytes */ val blockSize : Int /** * Get dependent entity values list by entity type. * * @param chainEntityType type of entity (for example: [ChainEntityType.TX]) * @return list of entity values (for example: transactions) */ fun entitiesByType ( chainEntityType : ChainEntityType ): List < ChainEntity > } fund.cyber.pump.common.node.BlockchainInterface /** * Interface representing blockchain * * @param T block bundle of this blockchain */ interface BlockchainInterface < out T : BlockBundle > { /** * Get last number of the block in blockchain network. * * @return block number */ fun lastNetworkBlock (): Long /** * Get [BlockBundle] by block number. * * @param number block number * @return block bundle */ fun blockBundleByNumber ( number : Long ): T } Also you should define spring bean of fund.cyber.pump.common.node.BlockchainInterface implementation either by annotate it with @Component or defining @Bean in spring configuration. For example: @Component class BitcoinBlockchainInterface ( ... ) : BlockchainInterface < BitcoinBlockBundle > { private val downloadSpeedMonitor = monitoring . timer ( \"pump_bundle_download\" ) override fun lastNetworkBlock (): Long = bitcoinJsonRpcClient . getLastBlockNumber () override fun blockBundleByNumber ( number : Long ): BitcoinBlockBundle { return downloadSpeedMonitor . recordCallable { val block = bitcoinJsonRpcClient . getBlockByNumber ( number ) !! return @recordCallable rpcToBundleEntitiesConverter . convertToBundle ( block ) } } } So, as you can see, you're not care of what happening with data next and how to store it. All you need is just to tell BlockBundle how to map it's fields on entities and we'll take care of everything else. Note that you should use toSearchHashFormat() extension function placed in fund.cyber.api.common.Func.kt on all fields in hex format when building your chain entities. Memory Pool Pump \u00b6 You also could add memory pool pumping logic by simply implement PoolInterface interface interface PoolInterface < T : PoolItem > { fun subscribePool (): Flowable < T > } It contains only one method that should return io.reactivex.Flowable of pool items. Create Dockerfile \u00b6 Put Docker file in root folder of your module. Here is template of Dockerfile: # Build Stage # Container with application FROM openjdk:8-jre-slim COPY /build/libs /cyberapp/bin ENTRYPOINT exec java $JAVA_OPTS -jar /cyberapp/bin/${your_chain_name}.jar Update CI \u00b6 Finally, you should update .circleci/config.yml file with steps for building and pushing docker image. * Add deploy job to jobs section. Template: deploy_chain_pumps_${your_chain_name}_image: <<: *defaults steps: - attach_workspace: at: ~/build - setup_remote_docker: version: 17.11.0-ce - run: name: Build ${your_chain_name} Pump Image command: | docker build -t build/pump-${your_chain_name} -f ./pumps/${your_chain_name}/Dockerfile ./pumps/${your_chain_name} docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker push cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:latest docker push cybernode/chain-pump-${your_chain_name}:latest Add deploy job to workflows.search_build.jobs section. Template: - deploy_chain_pumps_${your_chain_name}_image: <<: *release_filter requires: - build_project","title":"Pump Development Guide"},{"location":"cyber-search/contributing/pump-development/#pump-development-guide","text":"To develop your own chain pump you have two deal with two modules: common pumps","title":"Pump Development Guide"},{"location":"cyber-search/contributing/pump-development/#some-explanations","text":"To describe single chain in our system we're using following scheme. Every chain has ChainFamily . For example BITCOIN, ETHEREUM families. ChainFamily has it's own set of entities like TX, BLOCK, UNCLE and default url of chain node. Next, chain has a name. Chain name used everywhere in modules. Kafka topics, Cassandra keyspaces, API endpoints will be based on chain name. By default chain name is equal to ChainFamily . For example if you have ChainFamily == BITCOIN and you not specifying a chain name it will be set to BITCOIN . Main goal of chain name is to separate forks of one chain that have the same structure. For example BITCOIN and BITCOIN_CASH or ETHEREUM and ETHEREUM_CLASSIC . So as far as you can understand single chain pump is chain pump for specific ChainFamily","title":"Some explanations"},{"location":"cyber-search/contributing/pump-development/#adding-models-to-common-module","text":"First of all you have to put models representing your blockchain entities in common module. They should be placed in fund.cyber.search.model.{your_chain_family_name} package. This models will be used to transfer your blockchain data among our microservices (using Kafka). After doing so put description of your chain family to ChainFamily enum. You could find it in fund.cyber.search.model.chains.ChainInfo.kt file. Description includes following info: defaultNodeUrl - default URL of blockchain node to connect for data. entityTypes - map of blockchain entities types (from fund.cyber.search.model.chains.ChainEntity enum) to their class representation. If you can't find needed entity type in fund.cyber.search.model.chains.ChainEntity enum you have to add it by your own.","title":"Adding models to common module"},{"location":"cyber-search/contributing/pump-development/#developing-pump","text":"Now you're ready to write pump for your chain family. Note that this pump is suitable only for block based chains. We're using Spring Boot (https://projects.spring.io/spring-boot/). So you also should be familiar with spring beans, spring dependency injection and spring configuration.","title":"Developing pump"},{"location":"cyber-search/contributing/pump-development/#creating-gradle-module","text":"To start with, create new Gradle submodule with name of your chain in pumps module. Include your module in pumps/build.gradle and in settings.gradle . Example of pumps/build.gradle file with your module: project ( \":pumps:common\" ) { apply plugin: \"io.spring.dependency-management\" jar . archiveName = \"common-pumps\" dependencies { ... } } ... project ( \":pumps:{your_module_name}\" ) { apply plugin: \"org.springframework.boot\" dependencies { compile project ( \":pumps:common\" ) //your module dependencies } } ... Example of settings.gradle file: include \"common\" include \"common-kafka\" include \"cassandra-service\" ... include \"pumps:common\" include \"pumps:bitcoin\" include \"pumps:ethereum\" include \"pumps:{your_chain_name}\" ... All classes should be placed under fund.cyber.pump.{your_chain_name} package (except main class).","title":"Creating gradle module"},{"location":"cyber-search/contributing/pump-development/#creating-spring-boot-main-class","text":"The next step is creating spring boot main class. It should be placed in your module under fund.cyber package. Example of main class: @SpringBootApplication ( exclude = [ KafkaAutoConfiguration :: class ]) class BitcoinPumpApplication { companion object { @JvmStatic fun main ( args : Array < String >) { val application = SpringApplication ( BitcoinPumpApplication :: class . java ) application . runPump ( args ) } } }","title":"Creating spring boot main class"},{"location":"cyber-search/contributing/pump-development/#default-spring-beans-in-context","text":"After spring context start you'll have few already configured beans in context that you could inject in your module classes: org.springframework.retry.support.RetryTemplate - Spring Retry template for retrying failed operations. io.micrometer.core.instrument.MeterRegistry - bean for monitoring. fund.cyber.search.model.chains.ChainInfo - bean with all needed properties of running chain. class ChainInfo ( val family : ChainFamily , val name : String = \"\" , val nodeUrl : String = family . defaultNodeUrl ) { val fullName get () = family . name + if ( name . isEmpty ()) \"\" else \"_$name\" val entityTypes get () = family . entityTypes . keys fun entityClassByType ( type : ChainEntityType ) = family . entityTypes [ type ] } ChainInfo bean constructed from environment properties at start: CHAIN_FAMILY - chain family name (matches ChainFamily enum). For example CHAIN_FAMILY=BITCOIN . Required option . CHAIN_NAME - name of your specific chain. Not necessary . By default equals to CHAIN_FAMILY . All kafka topic will be named accordingly. For example CHAIN_NAME=BITCOIN_CASH then a topic name will be BITCOIN_CASH_TX_PUMP . * CHAIN_NODE_URL - URL of node running your chain. Not necessary . By default will be equal to one that described in your ChainFamily .","title":"Default spring beans in context"},{"location":"cyber-search/contributing/pump-development/#implementing-pump","text":"To integrate your pump with our system you simply have to implement two interfaces: * fund.cyber.pump.common.node.BlockBundle /** * Blockchain block with all dependent entities. Should collect all entities in scope of one block. * For example: transactions, uncles, etc.. */ interface BlockBundle { /** * Hash of the block */ val hash : String /** * Hash of the parent block */ val parentHash : String /** * Number of the block in blockchain */ val number : Long /** * Size of the block in bytes */ val blockSize : Int /** * Get dependent entity values list by entity type. * * @param chainEntityType type of entity (for example: [ChainEntityType.TX]) * @return list of entity values (for example: transactions) */ fun entitiesByType ( chainEntityType : ChainEntityType ): List < ChainEntity > } fund.cyber.pump.common.node.BlockchainInterface /** * Interface representing blockchain * * @param T block bundle of this blockchain */ interface BlockchainInterface < out T : BlockBundle > { /** * Get last number of the block in blockchain network. * * @return block number */ fun lastNetworkBlock (): Long /** * Get [BlockBundle] by block number. * * @param number block number * @return block bundle */ fun blockBundleByNumber ( number : Long ): T } Also you should define spring bean of fund.cyber.pump.common.node.BlockchainInterface implementation either by annotate it with @Component or defining @Bean in spring configuration. For example: @Component class BitcoinBlockchainInterface ( ... ) : BlockchainInterface < BitcoinBlockBundle > { private val downloadSpeedMonitor = monitoring . timer ( \"pump_bundle_download\" ) override fun lastNetworkBlock (): Long = bitcoinJsonRpcClient . getLastBlockNumber () override fun blockBundleByNumber ( number : Long ): BitcoinBlockBundle { return downloadSpeedMonitor . recordCallable { val block = bitcoinJsonRpcClient . getBlockByNumber ( number ) !! return @recordCallable rpcToBundleEntitiesConverter . convertToBundle ( block ) } } } So, as you can see, you're not care of what happening with data next and how to store it. All you need is just to tell BlockBundle how to map it's fields on entities and we'll take care of everything else. Note that you should use toSearchHashFormat() extension function placed in fund.cyber.api.common.Func.kt on all fields in hex format when building your chain entities.","title":"Implementing pump"},{"location":"cyber-search/contributing/pump-development/#memory-pool-pump","text":"You also could add memory pool pumping logic by simply implement PoolInterface interface interface PoolInterface < T : PoolItem > { fun subscribePool (): Flowable < T > } It contains only one method that should return io.reactivex.Flowable of pool items.","title":"Memory Pool Pump"},{"location":"cyber-search/contributing/pump-development/#create-dockerfile","text":"Put Docker file in root folder of your module. Here is template of Dockerfile: # Build Stage # Container with application FROM openjdk:8-jre-slim COPY /build/libs /cyberapp/bin ENTRYPOINT exec java $JAVA_OPTS -jar /cyberapp/bin/${your_chain_name}.jar","title":"Create Dockerfile"},{"location":"cyber-search/contributing/pump-development/#update-ci","text":"Finally, you should update .circleci/config.yml file with steps for building and pushing docker image. * Add deploy job to jobs section. Template: deploy_chain_pumps_${your_chain_name}_image: <<: *defaults steps: - attach_workspace: at: ~/build - setup_remote_docker: version: 17.11.0-ce - run: name: Build ${your_chain_name} Pump Image command: | docker build -t build/pump-${your_chain_name} -f ./pumps/${your_chain_name}/Dockerfile ./pumps/${your_chain_name} docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker push cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:latest docker push cybernode/chain-pump-${your_chain_name}:latest Add deploy job to workflows.search_build.jobs section. Template: - deploy_chain_pumps_${your_chain_name}_image: <<: *release_filter requires: - build_project","title":"Update CI"},{"location":"cyberd/Changelog/","text":"Change Log \u00b6 Unreleased \u00b6 Full Changelog Merged pull requests: Update cyberd/cosmos README #34 ( arturalbov ) 0.0.1 (2018-09-25) \u00b6 Closed issues: Make cyberd docs to be included into common wiki. #32 Write LT/NLT logic to Losion Zeronet #19 Research basic technologies #14 Research basic papers #13 Perfomance testing of Ethermint #12 Perfomance testing of Plasma #11 Perfomance testing of PoA networks #7 Cleanup paper #6 Perfomance testing of EOS #3 Merged pull requests: Genesis zeronet: Cosmos SDK #33 ( arturalbov ) [WIP] 21 calculate spring rank for ethereum #23 ( hleb-albau ) Fixing typo #2 ( trummax ) * This Change Log was automatically generated by github_changelog_generator","title":"Change Log"},{"location":"cyberd/Changelog/#change-log","text":"","title":"Change Log"},{"location":"cyberd/Changelog/#unreleased","text":"Full Changelog Merged pull requests: Update cyberd/cosmos README #34 ( arturalbov )","title":"Unreleased"},{"location":"cyberd/Changelog/#001-2018-09-25","text":"Closed issues: Make cyberd docs to be included into common wiki. #32 Write LT/NLT logic to Losion Zeronet #19 Research basic technologies #14 Research basic papers #13 Perfomance testing of Ethermint #12 Perfomance testing of Plasma #11 Perfomance testing of PoA networks #7 Cleanup paper #6 Perfomance testing of EOS #3 Merged pull requests: Genesis zeronet: Cosmos SDK #33 ( arturalbov ) [WIP] 21 calculate spring rank for ethereum #23 ( hleb-albau ) Fixing typo #2 ( trummax ) * This Change Log was automatically generated by github_changelog_generator","title":"0.0.1 (2018-09-25)"},{"location":"cyberd/Contributing/","text":"","title":"Contributing"},{"location":"cyberd/Roadmap/","text":"/// A file contain a list for discussion during research around cyberd. If some useful considerations sparks around some points an issue with tag discussion must be created. Discussion topics \u00b6 User has tokens (power of linking) and reputation which equal sum of link's weights created/connected to their Q/A. reputation vesting ~ inflation <- function * link weight ~ reputation <- function * broadband ~ reputation <- function * link weight applying with quadratic voting Retire CID in favor for DURI user defined domain registries Discussion on anonimity, complexity of a protocol and expensiveness of computation Discuss decay of links with time language incentivization bias_. In the core of cyberd is quadratic voting. The system needs it to effectively incentivize participants for quality ranking. But that natively leads to weak incentives across different language groups. E.g blockchain Golos was deployed as Russian alternative to Steem because Russian posts acquired only 0.2% of rewards though providing 10% of the content. The idea of deploying cyberd is great if it can be truly global from the start. The only way to overcome this bias is a global deployment from day 0, because otherwise we need significantly increase the complexity of the reward system. We offer good incentives for translation of this white paper to 50 languages worldwide as well as call for action to all blockchain communities across the globe. burning of CYBER in order to increase the rank A conventional advertiser is also has a tool to participate. Any link can be promoted thus increasing cyber\u2022rank. Revenue from the promotion is burning thus decreasing the supply of CYBER and bringing value to all shareholders of a system. We do believe that this approach is fair and don't harm user experience but open opportunity to compete with conventional search ad a bit. incentivization of developers. As our primary focus is developers of decentralized and distributed applications we offer a convenient way of revenue generation in addition to possible revenue on dynamic snippets development. Every search and answer transaction can define a smart contract with up to 6 beneficiaries with a distribution of potential reward from this particular action. This creates a solid foundation for bootstrapping application ecosystem across a conventional web, mobile app stores and upcoming VR platforms. idea: split reward pool in accordance to relevance of terms. Priorities for learning the beast: Arxiv, Chains, Wiki, Github, Bittorent, IPFS We cannot make distributions based on the history of ranking object because we decide to reject an idea of storing the historical state due to tremendous costs.","title":"Roadmap"},{"location":"cyberd/Roadmap/#discussion-topics","text":"User has tokens (power of linking) and reputation which equal sum of link's weights created/connected to their Q/A. reputation vesting ~ inflation <- function * link weight ~ reputation <- function * broadband ~ reputation <- function * link weight applying with quadratic voting Retire CID in favor for DURI user defined domain registries Discussion on anonimity, complexity of a protocol and expensiveness of computation Discuss decay of links with time language incentivization bias_. In the core of cyberd is quadratic voting. The system needs it to effectively incentivize participants for quality ranking. But that natively leads to weak incentives across different language groups. E.g blockchain Golos was deployed as Russian alternative to Steem because Russian posts acquired only 0.2% of rewards though providing 10% of the content. The idea of deploying cyberd is great if it can be truly global from the start. The only way to overcome this bias is a global deployment from day 0, because otherwise we need significantly increase the complexity of the reward system. We offer good incentives for translation of this white paper to 50 languages worldwide as well as call for action to all blockchain communities across the globe. burning of CYBER in order to increase the rank A conventional advertiser is also has a tool to participate. Any link can be promoted thus increasing cyber\u2022rank. Revenue from the promotion is burning thus decreasing the supply of CYBER and bringing value to all shareholders of a system. We do believe that this approach is fair and don't harm user experience but open opportunity to compete with conventional search ad a bit. incentivization of developers. As our primary focus is developers of decentralized and distributed applications we offer a convenient way of revenue generation in addition to possible revenue on dynamic snippets development. Every search and answer transaction can define a smart contract with up to 6 beneficiaries with a distribution of potential reward from this particular action. This creates a solid foundation for bootstrapping application ecosystem across a conventional web, mobile app stores and upcoming VR platforms. idea: split reward pool in accordance to relevance of terms. Priorities for learning the beast: Arxiv, Chains, Wiki, Github, Bittorent, IPFS We cannot make distributions based on the history of ranking object because we decide to reject an idea of storing the historical state due to tremendous costs.","title":"Discussion topics"},{"location":"cyberd/cyberd/","text":"cyberd: A search consensus computer \u00b6 @xhipster, @litvintech v 0.2 August 2018, Iceland and Tolyatti Abstract \u00b6 An incentivized consensus computer would allow to compute provably relevant answers without opinionated blackbox intermediaries such as Google. Stateless content-addressable peer-to-peer communication networks such as IPFS and stateful consensus computers such as Ethereum provide part of the solution but there are at least three problems associated with implementation. Of course, the first problem is subjective nature of relevance. The second problem is that it is hard to scale consensus computer of knowledge graph due to non linear nature of provably working solutions for web search such as PageRank and more than exponentially growing size of a knowledge graph including history of its formation. The third problem is that the quality of such knowledge graph will suffer from different attack surfaces such as sybil and selfish behavior of interacting agents. In this paper we (1) define a protocol for provable consensus computing of relevance between IPFS objects based on some offline observation and theory behind prediction markets, (2) solve a problem of implementation inside consensus computer based on linear SpringRank and propose workaround for pruning historical state and (3) design distribution and incentive scheme based on our experience and known attacks. Also we discuss some considerations on minimalistic architecture of the protocol as we believe that is critical for formation of a network of domain specific search consensus computers. As result of our work some applications never existed before emerge. Introduction \u00b6 Existing general purpose search engines are restrictive centralized databases everybody forced to trust. These search engines were designed primarily for client-server architecture based on DNS, HTTP, and IP protocols. The emergence of a distributed protocol stack creates an opportunity for a new kind of Internet. We call it web3. Protocols such as Ethereum create a challenge and opportunity for a search engine based on developing technologies and specifically designed for them. Surprisingly the permission-less blockchain architecture itself allows organizing general purpose search engine in a way inaccessible for previous architectures. The protocol \u00b6 def state transition take txs a tx format is <peer id> <up to 7 ipfs hashes of links> <signature> <signature> must be valid from <peer id> check that resource consumption of a signer is not exceed limits emit prediction of relevant objects for every valid tx every block calculate spring rank for the whole graph as input for every edge value get signer account's: <CYBER aka non transferable spring rank> plus <CYB aka transferable tokens> - every block distribute 42 CYB based on objects's CYBERs - there are objects with keys and objects without keys: for object with keys distribute payouts based on CYBER weight for objects without keys distribute payouts according to CYBER weight of incoming links with keys - every block apply predictions for links signed with computers threshold - every block write data to key/value store according to storage bound based on size and rank - every N blocks nodes reach consensus around pruned state history via ipfs hash of state blob Ranking as prediction market on links relevance \u00b6 (1) maximization of knowledge graph based, aligned with computational, storage and broadband bound and (2) reduction for attack surfaces such as sybil and selfish behavior Inductive reasoning \u00b6 A useful property of a computer is that it must know nothing about objects except when, who and where some prediction was asked. If we assume that a consensus computer must have some information about linked objects the complexity of such model growth unpredictably, hence a requirements for a computer for memory and computations. That is, deduction of a meaning inside consensus computer is expensive thus our design hardly depend on the blind assumption. Instead of we design incentives around meaning extractions Bad behavior \u00b6 We design a system under assumption that in terms of search such a thing as bad behavior does not exist as nothing bad can be in the intention of finding answers. Ranks is computed on the only fact that something has been searched, thus linked and as result affected predictive model. Good analogy is observing in quantum mechanics. So no negative voting is implemented. Doing this we remove subjectivity out of the protocol and can define one possible method for proof of relevance. Also this approach significantly reduce attack surface. About when \u00b6 Proof-of-history + Tendermint About who \u00b6 Digital signatures and zero knowledge proofs About where \u00b6 Mining triangulations and attaching proof of location for every search query can significantly Link chains \u00b6 Explain the concept of link chains and demonstrate a case of semantic linking based on interpretation of an ERC-20 transfer transaction or something similar Problem of ranking using consensus computer \u00b6 Consensus computers bring serious resource bounds It's possible to compute a ranks for the whole MerkleDAG. But there are two problems with it: An amount of links in MerkleDAG grows O(n^2) . That is not either conventional web pages with 20-100 links per page. For 1 Mb file can be thousands of links. Once the network starts to take off, complexity inevitably increases. Even if we address some algorithm to extract relevances from these links we should address even more algorithms to extract a meaning. What we need is to find a way to incentivize extraction from this data fog a meaning that is relevant to users queries . We can represent ther knowledge graph as directed acyclic graph where vertices are ipfs hashes and edges are directed links between them. Our knowledge graph include users Need verification: We consider introducing consensus variable, in addition to a block size, in order to target processing capacity of the network. Let's call it a computing target of documents per block or CTD. Any witness will be able to set a number of documents the network should recompute every block. The blockchain takes as input computing target of legitimate witnesses and computes CTD as daily moving average. Based on CTD blockchain can schedule the range of IPFS hashes that should be recomputed by every witness per round. cyber\u2022Rank \u00b6 Nebulas fail. No rank computed inside consensus computing => no possibility to incentivize network participants to form predictions on relevance. A problem here is that computational complexity of conventional ranks grow sublineary with the growth of the network. So we need to find deterministic algorithm that allow to compute a rank for continuously appended network to scale the consensus computer to orders of magnitude that of Google. Also an algorithm must have good prediction capability for existence of relevant to an object links. Spring ranks cons: linear & better ranking (original work, case proof with Steem) State grow history problem \u00b6 Every N blocks cybernodes prune blockchain/history and calculate IPFS hash for them/publish to IPFS, add hash to block and validate them with consensus. This state/blob economicaly finalized and new node start from them. Cybernodes motivated to store/provide this blob cause this cause network grow. Dynamically recalculate N with economy, newtork size, rank score... ~ vs direct IPFS DAG Motivation on request's processing problem \u00b6 Explain an economic difference and censorship impact between read search queries and write search queries Idea: All nodes run payment channels to serve request for their users and take tokens for request processing. Because this is heavy computation (only cybernodes will serve this) nodes will serve this only with payments for them. Cybernodes run protocol and earn tokens for read requests. Solution is payment channels based on HLTC and proof verification which unlocks amount earned for already served request (new signatures post via requester/user to cybernode via whisper/ipfs-pub-sub) Prediction by a consensus computer \u00b6 A consensus computer is able to continuously build a knowledge graph by itself predicting existence of links and applying this predictions to a state of itself. Idea: Everything that has been earned by a consensus computer can form a validators budget. Universal oracle \u00b6 A consensus computer is able to store the most relevant data in key value store. She is doing it by making a decision every block about what record he want to prune and what he want to apply. This key-value store can be ... Smart contracts \u00b6 Selfish linking \u00b6 Manipulating the rank: Sybil Attack: Quadratic voting. Rank squared. Decay from linear in the beginning. Spam protection \u00b6 In the center of spam protection system is an assumption that write operations can be executed only by those who have vested interest in the success of a consensus computer. Every 1% of stake in consensus computer gives the ability to use 1% of possible network broadband and computing capabilities. As nobody uses all possessed broadband we use fractional reserves while limiting broadband like ISPs do. Distribution Mechanism \u00b6 Describe drops and other built in incentives Incentive Structure \u00b6 To make cyber\u2022rank economically resistant to Sybil attack and to incentivize all participant for rational behavior a system uses CYBER token. Since inception, a network prints 42 CYBER every block. Reward pool is defined as 100% of emission and split among the contracts according to (?): Validators Linkers Applications \u00b6 Web3 browsers . It easy to imagine the emergence of a full-blown blockchain browser. Currently, there are several efforts for developing browsers around blockchains and distributed tech. Among them are Beaker, Mist and Brave .. All of them suffer from very limited functionality. Our developments can be useful for teams who are developing such tools. Programmable semantic cores . Relevance everywhere means that on any given user input string in any application relevant answer can be computed either globally, in the context of an app or in the context of a user. Actions in search . Proposed design enable native support for blockchain asset related activity. It is trivial to design an applications which are (1) owned by creators, (2) appear right in search results and (3) allow a transact-able call to actions with (4) provable attribution of a conversion to search query. e-Commerce has never been so easy for everybody. Shared mempools . Offline search . IPFS make possible easy retrieval of documents from surroundings without the internet connection. cyberd itself can be distributed using IPFS. That create a possibility for ubiquitous offline search. Smart Command Tools . Command line tools can rely on relevant and structured answers from a search engine. That practically means that the following CLI tool is possible to implement > cyberd earn using 100 gb hdd Enjoy the following predictions: - apt install siad /// 0.0001 btc per month per GB - apt install storjd /// 0.00008 btc per month per GB - apt install filecoind /// 0.00006 btc per month According to the best prediction I made a decision try `apt install siad` Git clone ... Building siad Starting siad Creating wallet using your standard seed You address is .... Placing bids ... Waiting for incoming storage requests ... Search from CLI tools will inevitably create a highly competitive market of a dedicated semantic core for bots. Autonomous robots . Blockchain technology enables creation of devices which are able to earn, store, spend and invest digital assets by themselves. If a robot can earn, store, spend and invest she can do everything you can do What is needed is simple yet powerful API about the state of reality evaluated in transact-able assets. Our solution offers minimalistic but continuously self-improving API that provides necessary tools for programming economically rational robots. Language convergence . A programmer should not care about what language do the user use. We don't need to have knowledge of what language user is searching in. Entire UTF-8 spectrum is at work. A semantic core is open so competition for answering can become distributed across different domain-specific areas, including semantic cores of different languages. The unified approach creates an opportunity for cyber\u2022Bahasa. Since the Internet, we observe a process of rapid language convergence. We use more truly global words across the entire planet independently of our nationality, language and race, Name the Internet. The dream of truly global language is hard to deploy because it is hard to agree on what mean what. But we have tools to make that dream come true. It is not hard to predict that the shorter a word the more it's cyber\u2022rank will be. Global publicly available list of symbols, words and phrases sorted by cyber\u2022rank with corresponding links provided by cyberd can be the foundation for the emergence of truly global language everybody can accept. Recent scientific advances in machine translation [GNMT] are breathtaking but meaningless for those who wish to apply them without Google scale trained model. Proposed cyber\u2022rank offers exactly this. This is sure not the exhaustive list of possible applications but very exciting, though. Extensibility, Upgradability and Governance \u00b6 That is a big question! Performance \u00b6 Proposed blockchain design is based on tendermint consensus algorithm and has fast and predictable near seconds block confirmation time and very fast finality time. Average confirmation timeframe is less than second thus conformations can be asynchronous and nearly invisible for users. A good thing is that users don't need confirmations at all before getting search response as there is no risk associated with that. Scalability \u00b6 Our node implementation theoretically can process about ? predictions per second. This theoretical bound is primarily limited with the possibility to replay a blockchain [https://steemit.com/blockchain/@dantheman/how-to-process-100m-transfers-second-on-a-single-blockchain]. As of now, all blockchains are about 1B immutable documents which size is about 200 GB with average tx 200 kb. We need to store all hashes which are on average 64 bytes long. We estimated that storing in the index all blockchain documents as IPFS hashes and votes are roughly the same as storing all raw blockchain data. Conclusion \u00b6 We describe and implement a motivated blockchain based search engine for the permanent web. A search engine is based on the content-addressable peer-to-peer paradigm and uses IPFS as a foundation. IPFS provide significant benefits in terms of resources consumption. IPFS addresses as a primary objects are robust in its simplicity. For every IPFS hash cyber\u2022rank is computed by a consensus computer with no single point of failure. Cyber\u2022rank is a spring rank enchased with economic based sybil protection. Embedded smart contracts offer fair compensations for those who is able to predict popularity of hashes. The primary goal is indexing of peer-to-peer systems with self-authenticated data either stateless, such as IFSS, DAT, GIT, BitTorent, or stateful such as Bitcoin, Ethereum and other blockchains and tangles. Proposed market of linking offers necessary incentives for outsourcing computing part responsible for finding meaningful relations between objects. A source code of a search engine is open source. Every bit of data accumulated by a consensus computer is available for everybody for free. The performance of proposed software implementation is sufficient for seamless user interactions. Scalability of proposed implementation is enough to index all self-authenticated data that exist today. The blockchain is managed by a decentralized autonomous organization which functions under Tendermint consensus algorithm. Thought a system provide necessary utility to offer an alternative for conventional search engines it is not limited to this use case either. The system is extendable for numerous applications and e.g. makes possible to design economically rational self-owned robots that are able to autonomously understand objects around them. References \u00b6","title":"cyberd: A search consensus computer"},{"location":"cyberd/cyberd/#cyberd-a-search-consensus-computer","text":"@xhipster, @litvintech v 0.2 August 2018, Iceland and Tolyatti","title":"cyberd: A search consensus computer"},{"location":"cyberd/cyberd/#abstract","text":"An incentivized consensus computer would allow to compute provably relevant answers without opinionated blackbox intermediaries such as Google. Stateless content-addressable peer-to-peer communication networks such as IPFS and stateful consensus computers such as Ethereum provide part of the solution but there are at least three problems associated with implementation. Of course, the first problem is subjective nature of relevance. The second problem is that it is hard to scale consensus computer of knowledge graph due to non linear nature of provably working solutions for web search such as PageRank and more than exponentially growing size of a knowledge graph including history of its formation. The third problem is that the quality of such knowledge graph will suffer from different attack surfaces such as sybil and selfish behavior of interacting agents. In this paper we (1) define a protocol for provable consensus computing of relevance between IPFS objects based on some offline observation and theory behind prediction markets, (2) solve a problem of implementation inside consensus computer based on linear SpringRank and propose workaround for pruning historical state and (3) design distribution and incentive scheme based on our experience and known attacks. Also we discuss some considerations on minimalistic architecture of the protocol as we believe that is critical for formation of a network of domain specific search consensus computers. As result of our work some applications never existed before emerge.","title":"Abstract"},{"location":"cyberd/cyberd/#introduction","text":"Existing general purpose search engines are restrictive centralized databases everybody forced to trust. These search engines were designed primarily for client-server architecture based on DNS, HTTP, and IP protocols. The emergence of a distributed protocol stack creates an opportunity for a new kind of Internet. We call it web3. Protocols such as Ethereum create a challenge and opportunity for a search engine based on developing technologies and specifically designed for them. Surprisingly the permission-less blockchain architecture itself allows organizing general purpose search engine in a way inaccessible for previous architectures.","title":"Introduction"},{"location":"cyberd/cyberd/#the-protocol","text":"def state transition take txs a tx format is <peer id> <up to 7 ipfs hashes of links> <signature> <signature> must be valid from <peer id> check that resource consumption of a signer is not exceed limits emit prediction of relevant objects for every valid tx every block calculate spring rank for the whole graph as input for every edge value get signer account's: <CYBER aka non transferable spring rank> plus <CYB aka transferable tokens> - every block distribute 42 CYB based on objects's CYBERs - there are objects with keys and objects without keys: for object with keys distribute payouts based on CYBER weight for objects without keys distribute payouts according to CYBER weight of incoming links with keys - every block apply predictions for links signed with computers threshold - every block write data to key/value store according to storage bound based on size and rank - every N blocks nodes reach consensus around pruned state history via ipfs hash of state blob","title":"The protocol"},{"location":"cyberd/cyberd/#ranking-as-prediction-market-on-links-relevance","text":"(1) maximization of knowledge graph based, aligned with computational, storage and broadband bound and (2) reduction for attack surfaces such as sybil and selfish behavior","title":"Ranking as prediction market on links relevance"},{"location":"cyberd/cyberd/#inductive-reasoning","text":"A useful property of a computer is that it must know nothing about objects except when, who and where some prediction was asked. If we assume that a consensus computer must have some information about linked objects the complexity of such model growth unpredictably, hence a requirements for a computer for memory and computations. That is, deduction of a meaning inside consensus computer is expensive thus our design hardly depend on the blind assumption. Instead of we design incentives around meaning extractions","title":"Inductive reasoning"},{"location":"cyberd/cyberd/#bad-behavior","text":"We design a system under assumption that in terms of search such a thing as bad behavior does not exist as nothing bad can be in the intention of finding answers. Ranks is computed on the only fact that something has been searched, thus linked and as result affected predictive model. Good analogy is observing in quantum mechanics. So no negative voting is implemented. Doing this we remove subjectivity out of the protocol and can define one possible method for proof of relevance. Also this approach significantly reduce attack surface.","title":"Bad behavior"},{"location":"cyberd/cyberd/#about-when","text":"Proof-of-history + Tendermint","title":"About when"},{"location":"cyberd/cyberd/#about-who","text":"Digital signatures and zero knowledge proofs","title":"About who"},{"location":"cyberd/cyberd/#about-where","text":"Mining triangulations and attaching proof of location for every search query can significantly","title":"About where"},{"location":"cyberd/cyberd/#link-chains","text":"Explain the concept of link chains and demonstrate a case of semantic linking based on interpretation of an ERC-20 transfer transaction or something similar","title":"Link chains"},{"location":"cyberd/cyberd/#problem-of-ranking-using-consensus-computer","text":"Consensus computers bring serious resource bounds It's possible to compute a ranks for the whole MerkleDAG. But there are two problems with it: An amount of links in MerkleDAG grows O(n^2) . That is not either conventional web pages with 20-100 links per page. For 1 Mb file can be thousands of links. Once the network starts to take off, complexity inevitably increases. Even if we address some algorithm to extract relevances from these links we should address even more algorithms to extract a meaning. What we need is to find a way to incentivize extraction from this data fog a meaning that is relevant to users queries . We can represent ther knowledge graph as directed acyclic graph where vertices are ipfs hashes and edges are directed links between them. Our knowledge graph include users Need verification: We consider introducing consensus variable, in addition to a block size, in order to target processing capacity of the network. Let's call it a computing target of documents per block or CTD. Any witness will be able to set a number of documents the network should recompute every block. The blockchain takes as input computing target of legitimate witnesses and computes CTD as daily moving average. Based on CTD blockchain can schedule the range of IPFS hashes that should be recomputed by every witness per round.","title":"Problem of ranking using consensus computer"},{"location":"cyberd/cyberd/#cyberrank","text":"Nebulas fail. No rank computed inside consensus computing => no possibility to incentivize network participants to form predictions on relevance. A problem here is that computational complexity of conventional ranks grow sublineary with the growth of the network. So we need to find deterministic algorithm that allow to compute a rank for continuously appended network to scale the consensus computer to orders of magnitude that of Google. Also an algorithm must have good prediction capability for existence of relevant to an object links. Spring ranks cons: linear & better ranking (original work, case proof with Steem)","title":"cyber\u2022Rank"},{"location":"cyberd/cyberd/#state-grow-history-problem","text":"Every N blocks cybernodes prune blockchain/history and calculate IPFS hash for them/publish to IPFS, add hash to block and validate them with consensus. This state/blob economicaly finalized and new node start from them. Cybernodes motivated to store/provide this blob cause this cause network grow. Dynamically recalculate N with economy, newtork size, rank score... ~ vs direct IPFS DAG","title":"State grow history problem"},{"location":"cyberd/cyberd/#motivation-on-requests-processing-problem","text":"Explain an economic difference and censorship impact between read search queries and write search queries Idea: All nodes run payment channels to serve request for their users and take tokens for request processing. Because this is heavy computation (only cybernodes will serve this) nodes will serve this only with payments for them. Cybernodes run protocol and earn tokens for read requests. Solution is payment channels based on HLTC and proof verification which unlocks amount earned for already served request (new signatures post via requester/user to cybernode via whisper/ipfs-pub-sub)","title":"Motivation on request's processing problem"},{"location":"cyberd/cyberd/#prediction-by-a-consensus-computer","text":"A consensus computer is able to continuously build a knowledge graph by itself predicting existence of links and applying this predictions to a state of itself. Idea: Everything that has been earned by a consensus computer can form a validators budget.","title":"Prediction by a consensus computer"},{"location":"cyberd/cyberd/#universal-oracle","text":"A consensus computer is able to store the most relevant data in key value store. She is doing it by making a decision every block about what record he want to prune and what he want to apply. This key-value store can be ...","title":"Universal oracle"},{"location":"cyberd/cyberd/#smart-contracts","text":"","title":"Smart contracts"},{"location":"cyberd/cyberd/#selfish-linking","text":"Manipulating the rank: Sybil Attack: Quadratic voting. Rank squared. Decay from linear in the beginning.","title":"Selfish linking"},{"location":"cyberd/cyberd/#spam-protection","text":"In the center of spam protection system is an assumption that write operations can be executed only by those who have vested interest in the success of a consensus computer. Every 1% of stake in consensus computer gives the ability to use 1% of possible network broadband and computing capabilities. As nobody uses all possessed broadband we use fractional reserves while limiting broadband like ISPs do.","title":"Spam protection"},{"location":"cyberd/cyberd/#distribution-mechanism","text":"Describe drops and other built in incentives","title":"Distribution Mechanism"},{"location":"cyberd/cyberd/#incentive-structure","text":"To make cyber\u2022rank economically resistant to Sybil attack and to incentivize all participant for rational behavior a system uses CYBER token. Since inception, a network prints 42 CYBER every block. Reward pool is defined as 100% of emission and split among the contracts according to (?): Validators Linkers","title":"Incentive Structure"},{"location":"cyberd/cyberd/#applications","text":"Web3 browsers . It easy to imagine the emergence of a full-blown blockchain browser. Currently, there are several efforts for developing browsers around blockchains and distributed tech. Among them are Beaker, Mist and Brave .. All of them suffer from very limited functionality. Our developments can be useful for teams who are developing such tools. Programmable semantic cores . Relevance everywhere means that on any given user input string in any application relevant answer can be computed either globally, in the context of an app or in the context of a user. Actions in search . Proposed design enable native support for blockchain asset related activity. It is trivial to design an applications which are (1) owned by creators, (2) appear right in search results and (3) allow a transact-able call to actions with (4) provable attribution of a conversion to search query. e-Commerce has never been so easy for everybody. Shared mempools . Offline search . IPFS make possible easy retrieval of documents from surroundings without the internet connection. cyberd itself can be distributed using IPFS. That create a possibility for ubiquitous offline search. Smart Command Tools . Command line tools can rely on relevant and structured answers from a search engine. That practically means that the following CLI tool is possible to implement > cyberd earn using 100 gb hdd Enjoy the following predictions: - apt install siad /// 0.0001 btc per month per GB - apt install storjd /// 0.00008 btc per month per GB - apt install filecoind /// 0.00006 btc per month According to the best prediction I made a decision try `apt install siad` Git clone ... Building siad Starting siad Creating wallet using your standard seed You address is .... Placing bids ... Waiting for incoming storage requests ... Search from CLI tools will inevitably create a highly competitive market of a dedicated semantic core for bots. Autonomous robots . Blockchain technology enables creation of devices which are able to earn, store, spend and invest digital assets by themselves. If a robot can earn, store, spend and invest she can do everything you can do What is needed is simple yet powerful API about the state of reality evaluated in transact-able assets. Our solution offers minimalistic but continuously self-improving API that provides necessary tools for programming economically rational robots. Language convergence . A programmer should not care about what language do the user use. We don't need to have knowledge of what language user is searching in. Entire UTF-8 spectrum is at work. A semantic core is open so competition for answering can become distributed across different domain-specific areas, including semantic cores of different languages. The unified approach creates an opportunity for cyber\u2022Bahasa. Since the Internet, we observe a process of rapid language convergence. We use more truly global words across the entire planet independently of our nationality, language and race, Name the Internet. The dream of truly global language is hard to deploy because it is hard to agree on what mean what. But we have tools to make that dream come true. It is not hard to predict that the shorter a word the more it's cyber\u2022rank will be. Global publicly available list of symbols, words and phrases sorted by cyber\u2022rank with corresponding links provided by cyberd can be the foundation for the emergence of truly global language everybody can accept. Recent scientific advances in machine translation [GNMT] are breathtaking but meaningless for those who wish to apply them without Google scale trained model. Proposed cyber\u2022rank offers exactly this. This is sure not the exhaustive list of possible applications but very exciting, though.","title":"Applications"},{"location":"cyberd/cyberd/#extensibility-upgradability-and-governance","text":"That is a big question!","title":"Extensibility, Upgradability and Governance"},{"location":"cyberd/cyberd/#performance","text":"Proposed blockchain design is based on tendermint consensus algorithm and has fast and predictable near seconds block confirmation time and very fast finality time. Average confirmation timeframe is less than second thus conformations can be asynchronous and nearly invisible for users. A good thing is that users don't need confirmations at all before getting search response as there is no risk associated with that.","title":"Performance"},{"location":"cyberd/cyberd/#scalability","text":"Our node implementation theoretically can process about ? predictions per second. This theoretical bound is primarily limited with the possibility to replay a blockchain [https://steemit.com/blockchain/@dantheman/how-to-process-100m-transfers-second-on-a-single-blockchain]. As of now, all blockchains are about 1B immutable documents which size is about 200 GB with average tx 200 kb. We need to store all hashes which are on average 64 bytes long. We estimated that storing in the index all blockchain documents as IPFS hashes and votes are roughly the same as storing all raw blockchain data.","title":"Scalability"},{"location":"cyberd/cyberd/#conclusion","text":"We describe and implement a motivated blockchain based search engine for the permanent web. A search engine is based on the content-addressable peer-to-peer paradigm and uses IPFS as a foundation. IPFS provide significant benefits in terms of resources consumption. IPFS addresses as a primary objects are robust in its simplicity. For every IPFS hash cyber\u2022rank is computed by a consensus computer with no single point of failure. Cyber\u2022rank is a spring rank enchased with economic based sybil protection. Embedded smart contracts offer fair compensations for those who is able to predict popularity of hashes. The primary goal is indexing of peer-to-peer systems with self-authenticated data either stateless, such as IFSS, DAT, GIT, BitTorent, or stateful such as Bitcoin, Ethereum and other blockchains and tangles. Proposed market of linking offers necessary incentives for outsourcing computing part responsible for finding meaningful relations between objects. A source code of a search engine is open source. Every bit of data accumulated by a consensus computer is available for everybody for free. The performance of proposed software implementation is sufficient for seamless user interactions. Scalability of proposed implementation is enough to index all self-authenticated data that exist today. The blockchain is managed by a decentralized autonomous organization which functions under Tendermint consensus algorithm. Thought a system provide necessary utility to offer an alternative for conventional search engines it is not limited to this use case either. The system is extendable for numerous applications and e.g. makes possible to design economically rational self-owned robots that are able to autonomously understand objects around them.","title":"Conclusion"},{"location":"cyberd/cyberd/#references","text":"","title":"References"},{"location":"cybernode/cybernode/","text":"","title":"Cybernode"},{"location":"cybernode/dev-setup/","text":"","title":"Dev setup"},{"location":"cybernode/k8s-cheat-sheet/","text":"K8s cheat sheet \u00b6 k8s dashboard \u00b6 Local dashboard proxy: kubectl proxy Get cluster access token: kubectl config view | grep -A10 \"name: $( kubectl config current-context ) \" | awk '$1==\"access-token:\"{print $2}' Get logs of previously running container (if it failed and then restarts): kubectl logs mypod --previous Reset GKE Node \u00b6 gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko Forward pod port on localhost \u00b6 kubectl port-forward -n monitoring prometheus-kube-prometheus-0 9090 Forward port of first found pod with specific namespace and label: kubectl port-forward -n chains $( kubectl get pod -n chains -l chain = bitcoind-btc -o jsonpath = \"{.items[0].metadata.name}\" ) 8332 Elassandra commands \u00b6 Get nodes status: kubectl exec -n search elassandra-0 -- nodetool status Open cqlsh CLI tool kubectl exec -it -n search elassandra-0 -- cqlsh Dive into elassandra docker container shell(index stats, delete index commands examples) kubectl exec -n search -it elassandra-0 bash curl -XGET 'localhost:9200/_cat/indices?v&pretty' curl -XDELETE 'localhost:9200/twitter?pretty'","title":"K8s cheat sheet"},{"location":"cybernode/k8s-cheat-sheet/#k8s-cheat-sheet","text":"","title":"K8s cheat sheet"},{"location":"cybernode/k8s-cheat-sheet/#k8s-dashboard","text":"Local dashboard proxy: kubectl proxy Get cluster access token: kubectl config view | grep -A10 \"name: $( kubectl config current-context ) \" | awk '$1==\"access-token:\"{print $2}' Get logs of previously running container (if it failed and then restarts): kubectl logs mypod --previous","title":"k8s dashboard"},{"location":"cybernode/k8s-cheat-sheet/#reset-gke-node","text":"gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko","title":"Reset GKE Node"},{"location":"cybernode/k8s-cheat-sheet/#forward-pod-port-on-localhost","text":"kubectl port-forward -n monitoring prometheus-kube-prometheus-0 9090 Forward port of first found pod with specific namespace and label: kubectl port-forward -n chains $( kubectl get pod -n chains -l chain = bitcoind-btc -o jsonpath = \"{.items[0].metadata.name}\" ) 8332","title":"Forward pod port on localhost"},{"location":"cybernode/k8s-cheat-sheet/#elassandra-commands","text":"Get nodes status: kubectl exec -n search elassandra-0 -- nodetool status Open cqlsh CLI tool kubectl exec -it -n search elassandra-0 -- cqlsh Dive into elassandra docker container shell(index stats, delete index commands examples) kubectl exec -n search -it elassandra-0 bash curl -XGET 'localhost:9200/_cat/indices?v&pretty' curl -XDELETE 'localhost:9200/twitter?pretty'","title":"Elassandra commands"},{"location":"cybernode/requirements/","text":"Requirements \u00b6 For near future(1.0 release) cybernode will act as a helping tool around docker images to instantiate p2p systems and app around them. For the first release, we consider to do next: 1 ) cybernode settings cybernode help 2 ) cybernode status 3 ) cybernode chains ethereum | ethereum_kovan | ethereum_classic start ( --light --autostartup ) cybernode chains ethereum stop cybernode chains bitcoin help cybernode chains bitcoin start ( --autostartup ) cybernode chains bitcoin stop cybernode chains status cybernide chains help 4 ) cybernode p2p ipfs start ( --autostartup ) cybernode p2p ipfs stop 5 ) cybernode apps | platforms cyb start cybernode apps | platforms cyb stop First run initialization \u00b6 For first run of cybernode, user should specify where to store data for applications: cybernode.data.location = ~/.cybernode/data By default, all cybernode configs|state will be stored in ~/.cybernode path. Nonchangeble. Research: - [] Path autocompletion? cybernode chains subcomands \u00b6 User shoud be able to run chains with specified options. Research: - [] UI for chains - [] Light clients for chains cybernode p2p subcomands \u00b6 User shoud be able to run p2p systems with specified options. Research: - [] Key pair managment if chain reuqired apps|platforms subcomands \u00b6 User should be able to run DApps. Each DApp also have dependencies on p2p or chains entities. This subcommand should start only missing entities. cybernode status \u00b6 Should display to user cybernode running|stopped entities. Also should be available for all subcommands. cybernode help \u00b6 Should display help message to the user. Also should be available for all subcommands. cybernode settings \u00b6 Should print current settings. Research: - [] Possible solutions for autocomplete for MacOS, Linux systems - [] Language and framework for CLI - [] Package distribution for various platforms(brew, npm, deb etc) - [] Initial Message??","title":"Requirements"},{"location":"cybernode/requirements/#requirements","text":"For near future(1.0 release) cybernode will act as a helping tool around docker images to instantiate p2p systems and app around them. For the first release, we consider to do next: 1 ) cybernode settings cybernode help 2 ) cybernode status 3 ) cybernode chains ethereum | ethereum_kovan | ethereum_classic start ( --light --autostartup ) cybernode chains ethereum stop cybernode chains bitcoin help cybernode chains bitcoin start ( --autostartup ) cybernode chains bitcoin stop cybernode chains status cybernide chains help 4 ) cybernode p2p ipfs start ( --autostartup ) cybernode p2p ipfs stop 5 ) cybernode apps | platforms cyb start cybernode apps | platforms cyb stop","title":"Requirements"},{"location":"cybernode/requirements/#first-run-initialization","text":"For first run of cybernode, user should specify where to store data for applications: cybernode.data.location = ~/.cybernode/data By default, all cybernode configs|state will be stored in ~/.cybernode path. Nonchangeble. Research: - [] Path autocompletion?","title":"First run initialization"},{"location":"cybernode/requirements/#cybernode-chains-subcomands","text":"User shoud be able to run chains with specified options. Research: - [] UI for chains - [] Light clients for chains","title":"cybernode chains subcomands"},{"location":"cybernode/requirements/#cybernode-p2p-subcomands","text":"User shoud be able to run p2p systems with specified options. Research: - [] Key pair managment if chain reuqired","title":"cybernode p2p subcomands"},{"location":"cybernode/requirements/#appsplatforms-subcomands","text":"User should be able to run DApps. Each DApp also have dependencies on p2p or chains entities. This subcommand should start only missing entities.","title":"apps|platforms subcomands"},{"location":"cybernode/requirements/#cybernode-status","text":"Should display to user cybernode running|stopped entities. Also should be available for all subcommands.","title":"cybernode status"},{"location":"cybernode/requirements/#cybernode-help","text":"Should display help message to the user. Also should be available for all subcommands.","title":"cybernode help"},{"location":"cybernode/requirements/#cybernode-settings","text":"Should print current settings. Research: - [] Possible solutions for autocomplete for MacOS, Linux systems - [] Language and framework for CLI - [] Package distribution for various platforms(brew, npm, deb etc) - [] Initial Message??","title":"cybernode settings"},{"location":"cybernode/chains/bitcoin/","text":"","title":"Bitcoin"},{"location":"cybernode/chains/ethereum/","text":"Known issues \u00b6 For 2806264 block there is a transaction with size more than 1mb, general rule, size of item can be >1mb.","title":"Ethereum"},{"location":"cybernode/chains/ethereum/#known-issues","text":"For 2806264 block there is a transaction with size more than 1mb, general rule, size of item can be >1mb.","title":"Known issues"},{"location":"cybernode/components/chain-nodes-components/","text":"Chain Nodes Components \u00b6 Component Cluster Address Parity 1.9.6 eth parity-eth.chains.svc:8545 Parity 1.9.6 etc parity-et\u0441.chains.svc:8545 Bitcoind 0.16.0 bitcoind-btc.chains.svc:8332","title":"Chain Nodes Components"},{"location":"cybernode/components/chain-nodes-components/#chain-nodes-components","text":"Component Cluster Address Parity 1.9.6 eth parity-eth.chains.svc:8545 Parity 1.9.6 etc parity-et\u0441.chains.svc:8545 Bitcoind 0.16.0 bitcoind-btc.chains.svc:8332","title":"Chain Nodes Components"},{"location":"cybernode/components/components-requirments/","text":"Components requirements \u00b6 Resources requirements to run components in gcloud cluster Component Min CPU Max CPU Min RAM Max RAM Kafka Broker 1.5 2 10GB 10GB Kafka Exporter 0.1 0.2 1GB 2GB Kafka Manager 0.1 0.2 1GB 2GB Zoo Keeper 0.1 0.2 1.25GB 1.25GB Elassandra 3.5 3.5 55GB 55GB Grafana 0.1 0.2 100MB 200MB Prometheus 0.25 1 3GB 3GB Prometheus Operator 0.1 0.2 100MB 200MB Parity ETH 3 3 15GB 20GB Parity ETC 1 1.5 10GB 15GB ETH Pump 1.5 3 5GB 6GB ETH Cassandra Dump 1.5 3 3.75GB 3.75GB ETH Contract Summary 1.5 3 3.75GB 3.75GB ETC Pump 1.5 3 3.75GB 3.75GB ETC Cassandra Dump 1.5 3 3.75GB 3.75GB ETC Contract Summary 1.5 3 3.75GB 3.75GB Bitcoind 2 3 20GB 30GB BTC Pump* 2.5 2.5 22GB 25GB BTC Dump 3 3 3.75GB 3.75GB BTC Contract Summary 1.5 3 3.75GB 3.75GB Search Api 0.5 1 3.75GB 3.75GB Search Api Docs 0.1 0.2 100MB 200MB * - With cache size = 10GB","title":"Components requirements"},{"location":"cybernode/components/components-requirments/#components-requirements","text":"Resources requirements to run components in gcloud cluster Component Min CPU Max CPU Min RAM Max RAM Kafka Broker 1.5 2 10GB 10GB Kafka Exporter 0.1 0.2 1GB 2GB Kafka Manager 0.1 0.2 1GB 2GB Zoo Keeper 0.1 0.2 1.25GB 1.25GB Elassandra 3.5 3.5 55GB 55GB Grafana 0.1 0.2 100MB 200MB Prometheus 0.25 1 3GB 3GB Prometheus Operator 0.1 0.2 100MB 200MB Parity ETH 3 3 15GB 20GB Parity ETC 1 1.5 10GB 15GB ETH Pump 1.5 3 5GB 6GB ETH Cassandra Dump 1.5 3 3.75GB 3.75GB ETH Contract Summary 1.5 3 3.75GB 3.75GB ETC Pump 1.5 3 3.75GB 3.75GB ETC Cassandra Dump 1.5 3 3.75GB 3.75GB ETC Contract Summary 1.5 3 3.75GB 3.75GB Bitcoind 2 3 20GB 30GB BTC Pump* 2.5 2.5 22GB 25GB BTC Dump 3 3 3.75GB 3.75GB BTC Contract Summary 1.5 3 3.75GB 3.75GB Search Api 0.5 1 3.75GB 3.75GB Search Api Docs 0.1 0.2 100MB 200MB * - With cache size = 10GB","title":"Components requirements"},{"location":"cybernode/components/monitoring-components/","text":"Monitoring Components \u00b6 During lifetime cybernode collects various metrics using following components: Component Description Cluster Address External Prometheus Operator(PO) Manages Prometheus Configuration Prometheus Metrics Storage prometheus.monitoring.svc:9090 Default Service Monitor Default Service Monitor for PO Grafana Metrics Alerts and Web UI grafana.monitoring.svc:3000 y Prometheus \u00b6 Prometheus is used as a main metrics storage. Components collect metrics and expose them via HTTP endpoint, that Prometheus pulls at configured interval(our is 15s). Prometheus Operator \u00b6 To deploy Prometheus atop Kubernetes, we use Prometheus Operator . It consists of Prometheus Operator itself, that introduced to cluster CRDs : Prometheus, Service Monitor. PO automatically(dynamically) generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language. Default Service Monitor \u00b6 To enable dynamic mapping for new pods to prometheus, service monitor(SM) is used. Each SM defines set of kubernetes label filters. Grafana \u00b6 To visualize metrics collected by Prometheus we use Grafana . In order to configure Grafana, three config maps are used: grafana-datasources . Define Prometheus datasource. grafana-dashboards . Gather predefined dashboards into single folder. grafana-dashboards-providers . Configuration for importing dashboards. Grafana Alerts \u00b6 Also Grafana supports checking metrics satisfied defined criteria. If criteria is violated, than notification is raised to setup alert channels(in our case Telegram Group). To view list of available alerts check \"{grafana_host}/alerting/list\" page.","title":"Monitoring Components"},{"location":"cybernode/components/monitoring-components/#monitoring-components","text":"During lifetime cybernode collects various metrics using following components: Component Description Cluster Address External Prometheus Operator(PO) Manages Prometheus Configuration Prometheus Metrics Storage prometheus.monitoring.svc:9090 Default Service Monitor Default Service Monitor for PO Grafana Metrics Alerts and Web UI grafana.monitoring.svc:3000 y","title":"Monitoring Components"},{"location":"cybernode/components/monitoring-components/#prometheus","text":"Prometheus is used as a main metrics storage. Components collect metrics and expose them via HTTP endpoint, that Prometheus pulls at configured interval(our is 15s).","title":"Prometheus"},{"location":"cybernode/components/monitoring-components/#prometheus-operator","text":"To deploy Prometheus atop Kubernetes, we use Prometheus Operator . It consists of Prometheus Operator itself, that introduced to cluster CRDs : Prometheus, Service Monitor. PO automatically(dynamically) generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language.","title":"Prometheus Operator"},{"location":"cybernode/components/monitoring-components/#default-service-monitor","text":"To enable dynamic mapping for new pods to prometheus, service monitor(SM) is used. Each SM defines set of kubernetes label filters.","title":"Default Service Monitor"},{"location":"cybernode/components/monitoring-components/#grafana","text":"To visualize metrics collected by Prometheus we use Grafana . In order to configure Grafana, three config maps are used: grafana-datasources . Define Prometheus datasource. grafana-dashboards . Gather predefined dashboards into single folder. grafana-dashboards-providers . Configuration for importing dashboards.","title":"Grafana"},{"location":"cybernode/components/monitoring-components/#grafana-alerts","text":"Also Grafana supports checking metrics satisfied defined criteria. If criteria is violated, than notification is raised to setup alert channels(in our case Telegram Group). To view list of available alerts check \"{grafana_host}/alerting/list\" page.","title":"Grafana Alerts"},{"location":"cybernode/staging/chains/","text":"Current chains nodes tables \u00b6 App data path port creds current size bitcoind - btc /cyberdata/ssd05tb/bitcoind 8332 cyber:cyber 184 gb abc - bth /cyberdata/abc 18332 bitcoin:password 156 gb parity - eth /cyberdata/ssd05tb/eth 8545 56 gb parity - eth_c /cyberdata/ssd05tb/eth_c 18545 5.3 gb Local forwarding port for chains \u00b6 ssh -L 18332:localhost:18332 -L 8332:localhost:8332 \\ -L 18545:localhost:18545 -L 8545:localhost:8545 \\ mars@staging.cyber.fund -p 33322 Commands used to run chain and live probe \u00b6 Bitcoind \u00b6 Run: sudo docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v /cyberdata/ssd05tb/bitcoind:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0 Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"getblockhash\",\"params\":[1],\"id\":1}' \\ http://127.0.0.1:8332 -u cyber:cyber BitcoinABC \u00b6 Run: sudo docker run -d -p 18332:8332 --name abc --restart always -v /cyberdata/abc:/data zquestz/bitcoin-abc:0.16.1 bitcoind -server -rest Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"getblockhash\",\"params\":[1],\"id\":1}' \\ http://127.0.0.1:18332 -u cyber:cyber Parity \u00b6 Run: sudo docker run -d -p 8545:8545 --name parity_eth --restart always \\ -v /cyberdata/ssd05tb/eth:/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 10 Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}'\\ http://127.0.0.1:8545 Parity --chain classic \u00b6 Run: sudo docker run -d -p 18545:8545 --name parity_etc --restart always \\ -v /cyberdata/ssd05tb/eth_c:/cyberdata parity/parity:stable --db-path /cyberdata \\ --jsonrpc-hosts all --jsonrpc-interface all --chain classic --jsonrpc-threads 10 Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}' \\ http://127.0.0.1:18545","title":"Chains"},{"location":"cybernode/staging/chains/#current-chains-nodes-tables","text":"App data path port creds current size bitcoind - btc /cyberdata/ssd05tb/bitcoind 8332 cyber:cyber 184 gb abc - bth /cyberdata/abc 18332 bitcoin:password 156 gb parity - eth /cyberdata/ssd05tb/eth 8545 56 gb parity - eth_c /cyberdata/ssd05tb/eth_c 18545 5.3 gb","title":"Current chains nodes tables"},{"location":"cybernode/staging/chains/#local-forwarding-port-for-chains","text":"ssh -L 18332:localhost:18332 -L 8332:localhost:8332 \\ -L 18545:localhost:18545 -L 8545:localhost:8545 \\ mars@staging.cyber.fund -p 33322","title":"Local forwarding port for chains"},{"location":"cybernode/staging/chains/#commands-used-to-run-chain-and-live-probe","text":"","title":"Commands used to run chain and live probe"},{"location":"cybernode/staging/chains/#bitcoind","text":"Run: sudo docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v /cyberdata/ssd05tb/bitcoind:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0 Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"getblockhash\",\"params\":[1],\"id\":1}' \\ http://127.0.0.1:8332 -u cyber:cyber","title":"Bitcoind"},{"location":"cybernode/staging/chains/#bitcoinabc","text":"Run: sudo docker run -d -p 18332:8332 --name abc --restart always -v /cyberdata/abc:/data zquestz/bitcoin-abc:0.16.1 bitcoind -server -rest Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"getblockhash\",\"params\":[1],\"id\":1}' \\ http://127.0.0.1:18332 -u cyber:cyber","title":"BitcoinABC"},{"location":"cybernode/staging/chains/#parity","text":"Run: sudo docker run -d -p 8545:8545 --name parity_eth --restart always \\ -v /cyberdata/ssd05tb/eth:/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 10 Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}'\\ http://127.0.0.1:8545","title":"Parity"},{"location":"cybernode/staging/chains/#parity-chain-classic","text":"Run: sudo docker run -d -p 18545:8545 --name parity_etc --restart always \\ -v /cyberdata/ssd05tb/eth_c:/cyberdata parity/parity:stable --db-path /cyberdata \\ --jsonrpc-hosts all --jsonrpc-interface all --chain classic --jsonrpc-threads 10 Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}' \\ http://127.0.0.1:18545","title":"Parity --chain classic"},{"location":"cybernode/staging/continuous-delivery/","text":"Circle Ci job example \u00b6 To enable auto staging-redeploy for your application you should: Enable staging docker images build. For example, you can use dockerhub tag \"STAGING\". Add to the repo you image running script. Add a CircleCi job, depended on docker build&push job. It's all. Example: build and deploy for all commits in master \u00b6 aliases: - &staging_filter filters: tags: only: /.*/ branches: only: master jobs: build_and_push_image_staging_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - checkout - setup_remote_docker: version: 17.05.0-ce - run: name: Build and upload staging images command: | docker build -t build/cs-search-api -f ./devops/search-api/search-api ./ docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/cs-search-api cybernode/cs-search-api:staging docker push cybernode/cs-search-api:staging deploy_stagin_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - run: name: Rerun image on staging command: >- ssh mars@staging.cyber.fund -p 33322 -o \"StrictHostKeyChecking no\" 'cd /cyberdata/cybernode && git pull && sh /cyberdata/cybernode/up.search.sh' workflows: version: 2 staging_cd: jobs: - build_and_push_image_staging_image: <<: *staging_filter - deploy_stagin_image: <<: *staging_filter requires: - build_and_push_image_staging_image","title":"Continuous delivery"},{"location":"cybernode/staging/continuous-delivery/#circle-ci-job-example","text":"To enable auto staging-redeploy for your application you should: Enable staging docker images build. For example, you can use dockerhub tag \"STAGING\". Add to the repo you image running script. Add a CircleCi job, depended on docker build&push job. It's all.","title":"Circle Ci job example"},{"location":"cybernode/staging/continuous-delivery/#example-build-and-deploy-for-all-commits-in-master","text":"aliases: - &staging_filter filters: tags: only: /.*/ branches: only: master jobs: build_and_push_image_staging_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - checkout - setup_remote_docker: version: 17.05.0-ce - run: name: Build and upload staging images command: | docker build -t build/cs-search-api -f ./devops/search-api/search-api ./ docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/cs-search-api cybernode/cs-search-api:staging docker push cybernode/cs-search-api:staging deploy_stagin_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - run: name: Rerun image on staging command: >- ssh mars@staging.cyber.fund -p 33322 -o \"StrictHostKeyChecking no\" 'cd /cyberdata/cybernode && git pull && sh /cyberdata/cybernode/up.search.sh' workflows: version: 2 staging_cd: jobs: - build_and_push_image_staging_image: <<: *staging_filter - deploy_stagin_image: <<: *staging_filter requires: - build_and_push_image_staging_image","title":"Example: build and deploy for all commits in master"},{"location":"cybernode/staging/kubernetes/","text":"Prerequisites \u00b6 Install kubectl for interacting with Kubernetes. Install minikube if you want to test setup locally. Make sure to install driver for your OS or else slow VirtualBox will be used. Running minikube for local testing \u00b6 $ minikube start --vm-driver kvm2 Starting local Kubernetes v1.9.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. Check status. $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.39.6 Run dashboard. $ minikube dashboard Opening kubernetes dashboard in default browser... Run single container in a cluster \u00b6 $ kubectl run miniecho --image=k8s.gcr.io/echoserver:1.9 --port=8080 This adds deployment called miniecho to cluster, runs Docker echoserver image in it and tells docker to open port 8080. The port is not accessible until we create service . $ kubectl expose deployment miniecho --type=NodePort service \"miniecho\" exposed This makes miniecho service accessible from your machine on random port. $ minikube service list |-------------|----------------------|---------------------------| | NAMESPACE | NAME | URL | |-------------|----------------------|---------------------------| | default | kubernetes | No node port | | default | miniecho | http://192.168.39.6:30426 | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | http://192.168.39.6:30000 | |-------------|----------------------|---------------------------|","title":"Kubernetes"},{"location":"cybernode/staging/kubernetes/#prerequisites","text":"Install kubectl for interacting with Kubernetes. Install minikube if you want to test setup locally. Make sure to install driver for your OS or else slow VirtualBox will be used.","title":"Prerequisites"},{"location":"cybernode/staging/kubernetes/#running-minikube-for-local-testing","text":"$ minikube start --vm-driver kvm2 Starting local Kubernetes v1.9.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. Check status. $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.39.6 Run dashboard. $ minikube dashboard Opening kubernetes dashboard in default browser...","title":"Running minikube for local testing"},{"location":"cybernode/staging/kubernetes/#run-single-container-in-a-cluster","text":"$ kubectl run miniecho --image=k8s.gcr.io/echoserver:1.9 --port=8080 This adds deployment called miniecho to cluster, runs Docker echoserver image in it and tells docker to open port 8080. The port is not accessible until we create service . $ kubectl expose deployment miniecho --type=NodePort service \"miniecho\" exposed This makes miniecho service accessible from your machine on random port. $ minikube service list |-------------|----------------------|---------------------------| | NAMESPACE | NAME | URL | |-------------|----------------------|---------------------------| | default | kubernetes | No node port | | default | miniecho | http://192.168.39.6:30426 | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | http://192.168.39.6:30000 | |-------------|----------------------|---------------------------|","title":"Run single container in a cluster"},{"location":"cybernode/staging/run/","text":"Port mapping \u00b6 Ports that are not accessible from internet. Use ssh forwarding. 8332 - chain btc 18332 - chain bth 8545 - chain eth 18545 - chain etc 2181 - zookeper 9092 - kafka 9042 - elassandra search 9043 - elassandra markets 9200 - elastic rest search 9201 - elastic rest markets 9300 - elastic transport search 9301 - elastic transport markets Ports open for internet. 32001 - portainer 32002 - grafana (monitoring) 32500 - browser-ui 32600 - chaingear 1.0 api 32800 - markets rest api 32801 - markets stream api 32901 - search api Docker data \u00b6 Run chains \u00b6 Chains should be run manually once and ideally should not require addition work. To run chains, execute: sudo bash /cyberdata/cybernode/chains/up.sh Run portainer.io \u00b6 Run once docker run -d -p 32001:9000 --name portainer --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /cyberdata/portainer:/data portainer/portainer Run components \u00b6 sudo bash /cyberdata/cybernode/up.browser.sh sudo bash /cyberdata/cybernode/up.chaingear.sh sudo bash /cyberdata/cybernode/up.ethereum.index.sh sudo bash /cyberdata/cybernode/up.search.api.sh Then go into each kafka , elassandra and run docker-compose up -d To access monitoring dashboard, run docker-compose up -d in monitoring as well.","title":"Run"},{"location":"cybernode/staging/run/#port-mapping","text":"Ports that are not accessible from internet. Use ssh forwarding. 8332 - chain btc 18332 - chain bth 8545 - chain eth 18545 - chain etc 2181 - zookeper 9092 - kafka 9042 - elassandra search 9043 - elassandra markets 9200 - elastic rest search 9201 - elastic rest markets 9300 - elastic transport search 9301 - elastic transport markets Ports open for internet. 32001 - portainer 32002 - grafana (monitoring) 32500 - browser-ui 32600 - chaingear 1.0 api 32800 - markets rest api 32801 - markets stream api 32901 - search api","title":"Port mapping"},{"location":"cybernode/staging/run/#docker-data","text":"","title":"Docker data"},{"location":"cybernode/staging/run/#run-chains","text":"Chains should be run manually once and ideally should not require addition work. To run chains, execute: sudo bash /cyberdata/cybernode/chains/up.sh","title":"Run chains"},{"location":"cybernode/staging/run/#run-portainerio","text":"Run once docker run -d -p 32001:9000 --name portainer --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /cyberdata/portainer:/data portainer/portainer","title":"Run portainer.io"},{"location":"cybernode/staging/run/#run-components","text":"sudo bash /cyberdata/cybernode/up.browser.sh sudo bash /cyberdata/cybernode/up.chaingear.sh sudo bash /cyberdata/cybernode/up.ethereum.index.sh sudo bash /cyberdata/cybernode/up.search.api.sh Then go into each kafka , elassandra and run docker-compose up -d To access monitoring dashboard, run docker-compose up -d in monitoring as well.","title":"Run components"},{"location":"cybernode/staging/setup/","text":"Mars is our staging server. You may reuse its config for your own. Current Mars storage setup \u00b6 Name Type Size LABEL Mapping nvme0n1p1 ssd 1 tb SDD1 /cyberdata/ssd1tb nvme2n1p1 ssd 500 gb SDD2 /cyberdata/ssd05tb sdc1 hdd 3.5 tb HDD3 /cyberdata/elassandra-markets sdb1 hdd 3.5 tb HDD2 /cyberdata/elassandra-search sda1 hdd 3.5 tb HDD1 /backupsData sdd1 hdd 3.5 tb cyberdata /cyberdata /cyberdata contents \u00b6 /cyberdata/portainer/ - portainer data directory /cyberdata/cybernode/ - clone of repo https://github.com/cyberFund/cybernode Useful commands \u00b6 Format partition: sudo mkfs -t ext4 -L SSD1 /dev/nvme0n1p1 sudo mkfs -t ext4 -L SSD2 /dev/nvme2n1p1 /etc/fstab addition lines: LABEL=cyberdata /cyberdata ext4 defaults 0 0 LABEL=SSD2 /cyberdata/ssd05tb ext4 nofail 0 0 LABEL=SSD1 /cyberdata/ssd1tb ext4 nofail 0 0 LABEL=HDD1 /backupsData ext4 defaults 0 0 LABEL=HDD2 /cyberdata/elassandra-search ext4 defaults 0 0 LABEL=HDD3 /cyberdata/elassandra-markets ext4 defaults 0 0 Copy data: sudo cp -R --verbose /backupsData/chain/eth/eth /cyberdata/ssd05tb/eth/ sudo cp -R --verbose /cyberdata/bitcoind /cyberdata/ssd05tb/bitcoind/","title":"Setup"},{"location":"cybernode/staging/setup/#current-mars-storage-setup","text":"Name Type Size LABEL Mapping nvme0n1p1 ssd 1 tb SDD1 /cyberdata/ssd1tb nvme2n1p1 ssd 500 gb SDD2 /cyberdata/ssd05tb sdc1 hdd 3.5 tb HDD3 /cyberdata/elassandra-markets sdb1 hdd 3.5 tb HDD2 /cyberdata/elassandra-search sda1 hdd 3.5 tb HDD1 /backupsData sdd1 hdd 3.5 tb cyberdata /cyberdata","title":"Current Mars storage setup"},{"location":"cybernode/staging/setup/#cyberdata-contents","text":"/cyberdata/portainer/ - portainer data directory /cyberdata/cybernode/ - clone of repo https://github.com/cyberFund/cybernode","title":"/cyberdata contents"},{"location":"cybernode/staging/setup/#useful-commands","text":"Format partition: sudo mkfs -t ext4 -L SSD1 /dev/nvme0n1p1 sudo mkfs -t ext4 -L SSD2 /dev/nvme2n1p1 /etc/fstab addition lines: LABEL=cyberdata /cyberdata ext4 defaults 0 0 LABEL=SSD2 /cyberdata/ssd05tb ext4 nofail 0 0 LABEL=SSD1 /cyberdata/ssd1tb ext4 nofail 0 0 LABEL=HDD1 /backupsData ext4 defaults 0 0 LABEL=HDD2 /cyberdata/elassandra-search ext4 defaults 0 0 LABEL=HDD3 /cyberdata/elassandra-markets ext4 defaults 0 0 Copy data: sudo cp -R --verbose /backupsData/chain/eth/eth /cyberdata/ssd05tb/eth/ sudo cp -R --verbose /cyberdata/bitcoind /cyberdata/ssd05tb/bitcoind/","title":"Useful commands"},{"location":"\u0441haingear/contracts/","text":"Contracts Overview \u00b6 /chaingear \u00b6 Chaingear allows any user to create his own registry. Building fee is collecting by new registry creation. All builded registries are tokenized with ERC721 NFT token standard and saved in Chaingear metaregistry with registry metainformation. Creator of registry may transfer tokenized ownership of registry and destroy registry with token burning. Chaingear supports multiple benefitiaries witch have access to collected fees. ###### depends on: - ERC721Token - SplitPaymentChangeable - ChaingearCore - Registry (int) ChaingearCore holds general logic of Chaingear. Allows change chaingear's metainformation, amount of registration fee, set registry creator contract. ###### depends on: - RegistryBase - IPFSeable - Destructible - Pausable - RegistryCreator (int) RegistryBase holds struct of data with describes registry metainformation which associated with token, views function for registry metainformation. RegistryCreator contains the bytecode of current version of Registry. This bytecode used by Chaingear for Registry Creation. Registry Creator address can be changed in Chaingear by it's owner. Creation function can be only called by setted out chaingear address. ###### depends on: - Registry - Ownable /common \u00b6 Seriality is a library for serializing and de-serializing all the Solidity types in a very efficient way which mostly written in solidity-assembly. IPFSeable contains logic which allows view and save links to CID in IPFS with ABI, source code and contract metainformation. Inherited by Chaingear and Registry. ###### depends on: - Ownable RegistySafe allows inherited contract transfer ETHs to safe and client claim from safe, accounting logic holded by inherited contract. Inherited by Chaingear and Registry. ###### depends on: - Ownable - Destructible SplitPaymentChangeable allows add beneficiaries to contract (addresses and shares amount) and change payee address. Beneficiaries can claim ETHs from contract proportionally to their shares. Inherited by Chaingear and Registry. ###### depends on: - Ownable - SplitPayment /registry \u00b6 Chaingeareable holds basic logic of Registry as registry basic information, balance and fees amount. Contains getters and setters for registry name, desciption, tags, entry base address. ###### depends on: - IPFSeable - RegistryAccessControl EntryBase base for EntryCore . Holds entry metainformation and interfaces of functions ( C R UD ) which should be implemented in EntryCore . EntryCore partilly codegenerated contract where registry creator setup their custom entry structure and setters/getters. EntryCore then goes to Registry constructor as bytecode, where EntryCore contract creates. Registry goes as owner of contract (and acts as proxy) with entries creating, transfering and deleting. ###### depends on: - EntryBase - Ownable - Seriality Registry contract witch tokenize entries as NFT tokens via ERC721 standard. Users can create entries with structure described in EntryCore sent serialized params according to entry access policy. Also users can fund entries with ETHs which send to RegistrySafe where owner of entry can claim funds. ###### depends on: - Chaingeareable - ERC721Token - SplitPaymentChangeable - EntryBase (int) RegistryAccessControl holds logic of controlling registry and accessing to entries creation. Policy options to entries creation are OnlyCreator, Whitelist, AllUsers. Chaingear acts as owner of registry and creator of registry acts of creator with separated policies to Registry functions. ###### depends on: - Ownable - Destructible","title":"Contracts Overview"},{"location":"\u0441haingear/contracts/#contracts-overview","text":"","title":"Contracts Overview"},{"location":"\u0441haingear/contracts/#chaingear","text":"Chaingear allows any user to create his own registry. Building fee is collecting by new registry creation. All builded registries are tokenized with ERC721 NFT token standard and saved in Chaingear metaregistry with registry metainformation. Creator of registry may transfer tokenized ownership of registry and destroy registry with token burning. Chaingear supports multiple benefitiaries witch have access to collected fees. ###### depends on: - ERC721Token - SplitPaymentChangeable - ChaingearCore - Registry (int) ChaingearCore holds general logic of Chaingear. Allows change chaingear's metainformation, amount of registration fee, set registry creator contract. ###### depends on: - RegistryBase - IPFSeable - Destructible - Pausable - RegistryCreator (int) RegistryBase holds struct of data with describes registry metainformation which associated with token, views function for registry metainformation. RegistryCreator contains the bytecode of current version of Registry. This bytecode used by Chaingear for Registry Creation. Registry Creator address can be changed in Chaingear by it's owner. Creation function can be only called by setted out chaingear address. ###### depends on: - Registry - Ownable","title":"/chaingear"},{"location":"\u0441haingear/contracts/#common","text":"Seriality is a library for serializing and de-serializing all the Solidity types in a very efficient way which mostly written in solidity-assembly. IPFSeable contains logic which allows view and save links to CID in IPFS with ABI, source code and contract metainformation. Inherited by Chaingear and Registry. ###### depends on: - Ownable RegistySafe allows inherited contract transfer ETHs to safe and client claim from safe, accounting logic holded by inherited contract. Inherited by Chaingear and Registry. ###### depends on: - Ownable - Destructible SplitPaymentChangeable allows add beneficiaries to contract (addresses and shares amount) and change payee address. Beneficiaries can claim ETHs from contract proportionally to their shares. Inherited by Chaingear and Registry. ###### depends on: - Ownable - SplitPayment","title":"/common"},{"location":"\u0441haingear/contracts/#registry","text":"Chaingeareable holds basic logic of Registry as registry basic information, balance and fees amount. Contains getters and setters for registry name, desciption, tags, entry base address. ###### depends on: - IPFSeable - RegistryAccessControl EntryBase base for EntryCore . Holds entry metainformation and interfaces of functions ( C R UD ) which should be implemented in EntryCore . EntryCore partilly codegenerated contract where registry creator setup their custom entry structure and setters/getters. EntryCore then goes to Registry constructor as bytecode, where EntryCore contract creates. Registry goes as owner of contract (and acts as proxy) with entries creating, transfering and deleting. ###### depends on: - EntryBase - Ownable - Seriality Registry contract witch tokenize entries as NFT tokens via ERC721 standard. Users can create entries with structure described in EntryCore sent serialized params according to entry access policy. Also users can fund entries with ETHs which send to RegistrySafe where owner of entry can claim funds. ###### depends on: - Chaingeareable - ERC721Token - SplitPaymentChangeable - EntryBase (int) RegistryAccessControl holds logic of controlling registry and accessing to entries creation. Policy options to entries creation are OnlyCreator, Whitelist, AllUsers. Chaingear acts as owner of registry and creator of registry acts of creator with separated policies to Registry functions. ###### depends on: - Ownable - Destructible","title":"/registry"},{"location":"\u0441haingear/development/","text":"Configuring, development and deploying \u00b6 Deploy contract: \u00b6 parity ui --chain=kovan truffle migrate --network=kovan PS: approve transaction in parity ui (http://127.0.0.1:8180/) Build contract in file: truffle-flattener contracts/common/Chaingeareable.sol >> app/src/Chaingeareable.sol Linting: \u00b6 npm install -g solium solium -d contracts Development environment \u00b6 Recommending to use Remix Ethereum Online IDE or desktop electron-based Remix IDE PS: to import to IDE open-zeppelin contacts follow this: import \"github.com/openZeppelin/zeppelin-solidity/contracts/ownership/Ownable.sol\"; Truffle + Ganache workflow \u00b6 Install Ganache from latest release , then => npm install -g ganache-cli Configure development config in truffle.js and launch Ganache (configure them too if needed) and: ganache-cli -p 7545 (in first tab) truffle migrate --network development --reset (in second tab) truffle console --network development (in second tab) Create new registry \u00b6 var chaingear = Chaingear.at(Chaingear.address) var beneficiaries = [] var shares = [] var buildingFee = 1000000 var gas = 10000000 chaingear.registerRegistry(beneficiaries, shares, \"BlockchainRegistry\", \"BLR\", \"\", EntryCore.bytecode, {value: buildingFee, gas: 10000000})","title":"Configuring, development and deploying"},{"location":"\u0441haingear/development/#configuring-development-and-deploying","text":"","title":"Configuring, development and deploying"},{"location":"\u0441haingear/development/#deploy-contract","text":"parity ui --chain=kovan truffle migrate --network=kovan PS: approve transaction in parity ui (http://127.0.0.1:8180/) Build contract in file: truffle-flattener contracts/common/Chaingeareable.sol >> app/src/Chaingeareable.sol","title":"Deploy contract:"},{"location":"\u0441haingear/development/#linting","text":"npm install -g solium solium -d contracts","title":"Linting:"},{"location":"\u0441haingear/development/#development-environment","text":"Recommending to use Remix Ethereum Online IDE or desktop electron-based Remix IDE PS: to import to IDE open-zeppelin contacts follow this: import \"github.com/openZeppelin/zeppelin-solidity/contracts/ownership/Ownable.sol\";","title":"Development environment"},{"location":"\u0441haingear/development/#truffle-ganache-workflow","text":"Install Ganache from latest release , then => npm install -g ganache-cli Configure development config in truffle.js and launch Ganache (configure them too if needed) and: ganache-cli -p 7545 (in first tab) truffle migrate --network development --reset (in second tab) truffle console --network development (in second tab)","title":"Truffle + Ganache workflow"},{"location":"\u0441haingear/development/#create-new-registry","text":"var chaingear = Chaingear.at(Chaingear.address) var beneficiaries = [] var shares = [] var buildingFee = 1000000 var gas = 10000000 chaingear.registerRegistry(beneficiaries, shares, \"BlockchainRegistry\", \"BLR\", \"\", EntryCore.bytecode, {value: buildingFee, gas: 10000000})","title":"Create new registry"},{"location":"\u0441haingear/overview/","text":"Most expensive Registry Chaingear is an Ethereum ERC721-based registries framework. Built by cyber\u2022Search and contributors Overview \u00b6 This project allows you to create your own Registry of general purpose entries on Ethereum blockchain. Entry type is defined during creation, so you can put into entry any custom logic you want (validation, entry-level permission control). Entries are tokenized as NFT. Your creating your registry in Chaingear - metaregistry, which are one point of access to all registries. Registries on chaingear level are tokenized as NFT. Chaingear is most expensive registry, so you should pay for your registry creation. Features \u00b6 Chaingear \u00b6 1 2 3 Custom registry \u00b6 1 2 3 Chaingear in browser \u00b6 1 2 3","title":"Overview"},{"location":"\u0441haingear/overview/#overview","text":"This project allows you to create your own Registry of general purpose entries on Ethereum blockchain. Entry type is defined during creation, so you can put into entry any custom logic you want (validation, entry-level permission control). Entries are tokenized as NFT. Your creating your registry in Chaingear - metaregistry, which are one point of access to all registries. Registries on chaingear level are tokenized as NFT. Chaingear is most expensive registry, so you should pay for your registry creation.","title":"Overview"},{"location":"\u0441haingear/overview/#features","text":"","title":"Features"},{"location":"\u0441haingear/overview/#chaingear","text":"1 2 3","title":"Chaingear"},{"location":"\u0441haingear/overview/#custom-registry","text":"1 2 3","title":"Custom registry"},{"location":"\u0441haingear/overview/#chaingear-in-browser","text":"1 2 3","title":"Chaingear in browser"}]}