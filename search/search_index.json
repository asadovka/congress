{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Cyber Development Knowledge Base.\n\n\nRight now these docs cover two \ncyber-\n open source products:\n\n\n\n\nblockchain browser - https://github.com/cybercongress/cyber-browser - this service is responsible\n  for indexing and searching information stored in different chains\n\n\nmarkets analysis - https://github.com/cybercongress/cyber-markets - this service collects information\n  from crypto exchanges and does calculations of Cybernomics parameters", 
            "title": "Home"
        }, 
        {
            "location": "/contribute/", 
            "text": "Current wiki is built on top of \nmkdocs.org\n engine with\n\nMaterial for MkDocs\n extensions pack.\n\n\nRequired Installations\n\n\n\n\nhttps://hub.docker.com/r/squidfunk/mkdocs-material/\n\n\n\n\nCommands Cheat Sheet\n\n\n\n\ndocker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material\n\n\ndocker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build\n\n\ndocker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs squidfunk/mkdocs-material gh-deploy\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Contributing"
        }, 
        {
            "location": "/contribute/#required-installations", 
            "text": "https://hub.docker.com/r/squidfunk/mkdocs-material/", 
            "title": "Required Installations"
        }, 
        {
            "location": "/contribute/#commands-cheat-sheet", 
            "text": "docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material  docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build  docker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs squidfunk/mkdocs-material gh-deploy", 
            "title": "Commands Cheat Sheet"
        }, 
        {
            "location": "/contribute/#project-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/staging/setup/", 
            "text": "Mars is our \nstaging\n server. You may reuse its config for your own.\n\n\nCurrent Mars storage setup\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nSize\n\n\nLABEL\n\n\nMapping\n\n\n\n\n\n\n\n\n\n\nnvme0n1p1\n\n\nssd\n\n\n1 tb\n\n\nSDD1\n\n\n/cyberdata/ssd1tb\n\n\n\n\n\n\nnvme2n1p1\n\n\nssd\n\n\n500 gb\n\n\nSDD2\n\n\n/cyberdata/ssd05tb\n\n\n\n\n\n\nsdc1\n\n\nhdd\n\n\n3.5 tb\n\n\nHDD3\n\n\n/cyberdata/elassandra-markets\n\n\n\n\n\n\nsdb1\n\n\nhdd\n\n\n3.5 tb\n\n\nHDD2\n\n\n/cyberdata/elassandra-search\n\n\n\n\n\n\nsda1\n\n\nhdd\n\n\n3.5 tb\n\n\nHDD1\n\n\n/backupsData\n\n\n\n\n\n\nsdd1\n\n\nhdd\n\n\n3.5 tb\n\n\ncyberdata\n\n\n/cyberdata\n\n\n\n\n\n\n\n\n/cyberdata\n contents\n\n\n\n\n/cyberdata/portainer/\n - portainer data directory\n\n\n/cyberdata/cybernode/\n - clone of repo https://github.com/cyberFund/cybernode\n\n\n\n\nUseful commands\n\n\nFormat partition:\n\n\nsudo mkfs -t ext4 -L SSD1 /dev/nvme0n1p1\nsudo mkfs -t ext4 -L SSD2 /dev/nvme2n1p1\n\n\n\n\n\n/etc/fstab addition lines:\n\n\nLABEL=cyberdata /cyberdata ext4 defaults 0 0\nLABEL=SSD2 /cyberdata/ssd05tb ext4 nofail 0 0\nLABEL=SSD1 /cyberdata/ssd1tb ext4 nofail 0 0\nLABEL=HDD1 /backupsData ext4 defaults 0 0\nLABEL=HDD2 /cyberdata/elassandra-search ext4 defaults 0 0 \nLABEL=HDD3 /cyberdata/elassandra-markets ext4 defaults 0 0 \n\n\n\n\n\nCopy data:\n\n\nsudo cp -R --verbose /backupsData/chain/eth/eth /cyberdata/ssd05tb/eth/\nsudo cp -R --verbose /cyberdata/bitcoind /cyberdata/ssd05tb/bitcoind/", 
            "title": "Mars Setup"
        }, 
        {
            "location": "/staging/setup/#current-mars-storage-setup", 
            "text": "Name  Type  Size  LABEL  Mapping      nvme0n1p1  ssd  1 tb  SDD1  /cyberdata/ssd1tb    nvme2n1p1  ssd  500 gb  SDD2  /cyberdata/ssd05tb    sdc1  hdd  3.5 tb  HDD3  /cyberdata/elassandra-markets    sdb1  hdd  3.5 tb  HDD2  /cyberdata/elassandra-search    sda1  hdd  3.5 tb  HDD1  /backupsData    sdd1  hdd  3.5 tb  cyberdata  /cyberdata", 
            "title": "Current Mars storage setup"
        }, 
        {
            "location": "/staging/setup/#cyberdata-contents", 
            "text": "/cyberdata/portainer/  - portainer data directory  /cyberdata/cybernode/  - clone of repo https://github.com/cyberFund/cybernode", 
            "title": "/cyberdata contents"
        }, 
        {
            "location": "/staging/setup/#useful-commands", 
            "text": "Format partition:  sudo mkfs -t ext4 -L SSD1 /dev/nvme0n1p1\nsudo mkfs -t ext4 -L SSD2 /dev/nvme2n1p1  /etc/fstab addition lines:  LABEL=cyberdata /cyberdata ext4 defaults 0 0\nLABEL=SSD2 /cyberdata/ssd05tb ext4 nofail 0 0\nLABEL=SSD1 /cyberdata/ssd1tb ext4 nofail 0 0\nLABEL=HDD1 /backupsData ext4 defaults 0 0\nLABEL=HDD2 /cyberdata/elassandra-search ext4 defaults 0 0 \nLABEL=HDD3 /cyberdata/elassandra-markets ext4 defaults 0 0   Copy data:  sudo cp -R --verbose /backupsData/chain/eth/eth /cyberdata/ssd05tb/eth/\nsudo cp -R --verbose /cyberdata/bitcoind /cyberdata/ssd05tb/bitcoind/", 
            "title": "Useful commands"
        }, 
        {
            "location": "/staging/continuous-delivery/", 
            "text": "Circle Ci job example\n\n\nTo enable auto staging-redeploy for your application you should:\n\n\n\n\n\n\nEnable \nstaging\n docker images build. For example, you can use dockerhub tag \"STAGING\". \n\n\n\n\n\n\nAdd to \nthe repo\n you image running script.\n\n\n\n\n\n\nAdd a CircleCi job, depended on docker build\npush job. It's all.\n\n\n\n\n\n\nExample: build and deploy for all commits in master\n\n\naliases:\n - \nstaging_filter\n    filters:\n      tags:\n        only: /.*/\n      branches:\n        only: master\n\n\njobs:        \n  build_and_push_image_staging_image:\n    working_directory: ~/build\n    docker:\n      - image: docker:17.05.0-ce-git\n    steps:\n      - checkout\n      - setup_remote_docker:\n          version: 17.05.0-ce\n      - run:\n          name: Build and upload staging images\n          command: |\n            docker build -t build/cs-search-api -f ./devops/search-api/search-api ./\n            docker login -u $DOCKER_USER -p $DOCKER_PASS\n            docker tag build/cs-search-api cybernode/cs-search-api:staging\n            docker push cybernode/cs-search-api:staging\n\n    deploy_stagin_image:\n      working_directory: ~/build\n      docker:\n        - image: docker:17.05.0-ce-git\n      steps:\n        - run:\n            name: Rerun image on staging\n            command: \n-\n              ssh mars@staging.cyber.fund -p 33322 -o \nStrictHostKeyChecking no\n\n              \ncd /cyberdata/cybernode \n git pull \n sh /cyberdata/cybernode/up.search.sh\n\n\n\nworkflows:\n  version: 2\n  staging_cd:\n    jobs:\n      - build_and_push_image_staging_image:\n          \n: *staging_filter\n      - deploy_stagin_image:\n          \n: *staging_filter\n          requires:\n            - build_and_push_image_staging_image", 
            "title": "Continuous Delivery"
        }, 
        {
            "location": "/staging/continuous-delivery/#circle-ci-job-example", 
            "text": "To enable auto staging-redeploy for your application you should:    Enable  staging  docker images build. For example, you can use dockerhub tag \"STAGING\".     Add to  the repo  you image running script.    Add a CircleCi job, depended on docker build push job. It's all.", 
            "title": "Circle Ci job example"
        }, 
        {
            "location": "/staging/continuous-delivery/#example-build-and-deploy-for-all-commits-in-master", 
            "text": "aliases:\n -  staging_filter\n    filters:\n      tags:\n        only: /.*/\n      branches:\n        only: master\n\n\njobs:        \n  build_and_push_image_staging_image:\n    working_directory: ~/build\n    docker:\n      - image: docker:17.05.0-ce-git\n    steps:\n      - checkout\n      - setup_remote_docker:\n          version: 17.05.0-ce\n      - run:\n          name: Build and upload staging images\n          command: |\n            docker build -t build/cs-search-api -f ./devops/search-api/search-api ./\n            docker login -u $DOCKER_USER -p $DOCKER_PASS\n            docker tag build/cs-search-api cybernode/cs-search-api:staging\n            docker push cybernode/cs-search-api:staging\n\n    deploy_stagin_image:\n      working_directory: ~/build\n      docker:\n        - image: docker:17.05.0-ce-git\n      steps:\n        - run:\n            name: Rerun image on staging\n            command:  -\n              ssh mars@staging.cyber.fund -p 33322 -o  StrictHostKeyChecking no \n               cd /cyberdata/cybernode   git pull   sh /cyberdata/cybernode/up.search.sh \n\n\nworkflows:\n  version: 2\n  staging_cd:\n    jobs:\n      - build_and_push_image_staging_image:\n           : *staging_filter\n      - deploy_stagin_image:\n           : *staging_filter\n          requires:\n            - build_and_push_image_staging_image", 
            "title": "Example: build and deploy for all commits in master"
        }, 
        {
            "location": "/staging/run/", 
            "text": "Port mapping\n\n\nPorts that are not accessible from internet. Use ssh forwarding.\n\n\n\n\n8332 - chain btc\n\n\n18332 - chain bth\n\n\n8545 - chain eth\n\n\n\n\n18545 - chain etc\n\n\n\n\n\n\n2181 - zookeper\n\n\n\n\n9092 - kafka\n\n\n9042 - elassandra search\n\n\n9043 - elassandra markets\n\n\n9200 - elastic rest search\n\n\n9201 - elastic rest markets\n\n\n9300 - elastic transport search\n\n\n9301 - elastic transport markets\n\n\n\n\nPorts open for internet.\n\n\n\n\n32001 - portainer\n\n\n32002 - grafana (monitoring)\n\n\n32500 - browser-ui\n\n\n32600 - chaingear 1.0 api\n\n\n32800 - markets rest api\n\n\n32801 - markets stream api\n\n\n32901 - search api\n\n\n\n\nDocker data\n\n\nRun chains\n\n\nChains should be run manually once and ideally should not require addition work. To run chains, execute:\n\n\nsudo bash /cyberdata/cybernode/chains/up.sh\n\n\n\n\n\nRun portainer.io\n\n\nRun once\n\n\ndocker run -d -p 32001:9000 --name portainer --restart always \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /cyberdata/portainer:/data portainer/portainer\n\n\n\n\n\nRun components\n\n\nsudo bash /cyberdata/cybernode/up.browser.sh\nsudo bash /cyberdata/cybernode/up.chaingear.sh\nsudo bash /cyberdata/cybernode/up.ethereum.index.sh\nsudo bash /cyberdata/cybernode/up.search.api.sh\n\n\n\n\n\nThen go into each \nkafka\n, \nelassandra\n and run\n\n\ndocker-compose up -d\n\n\n\n\n\nTo access \nmonitoring\n dashboard, run \ndocker-compose up -d\n\nin \nmonitoring\n as well.", 
            "title": "Running Application"
        }, 
        {
            "location": "/staging/run/#port-mapping", 
            "text": "Ports that are not accessible from internet. Use ssh forwarding.   8332 - chain btc  18332 - chain bth  8545 - chain eth   18545 - chain etc    2181 - zookeper   9092 - kafka  9042 - elassandra search  9043 - elassandra markets  9200 - elastic rest search  9201 - elastic rest markets  9300 - elastic transport search  9301 - elastic transport markets   Ports open for internet.   32001 - portainer  32002 - grafana (monitoring)  32500 - browser-ui  32600 - chaingear 1.0 api  32800 - markets rest api  32801 - markets stream api  32901 - search api", 
            "title": "Port mapping"
        }, 
        {
            "location": "/staging/run/#docker-data", 
            "text": "", 
            "title": "Docker data"
        }, 
        {
            "location": "/staging/run/#run-chains", 
            "text": "Chains should be run manually once and ideally should not require addition work. To run chains, execute:  sudo bash /cyberdata/cybernode/chains/up.sh", 
            "title": "Run chains"
        }, 
        {
            "location": "/staging/run/#run-portainerio", 
            "text": "Run once  docker run -d -p 32001:9000 --name portainer --restart always \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /cyberdata/portainer:/data portainer/portainer", 
            "title": "Run portainer.io"
        }, 
        {
            "location": "/staging/run/#run-components", 
            "text": "sudo bash /cyberdata/cybernode/up.browser.sh\nsudo bash /cyberdata/cybernode/up.chaingear.sh\nsudo bash /cyberdata/cybernode/up.ethereum.index.sh\nsudo bash /cyberdata/cybernode/up.search.api.sh  Then go into each  kafka ,  elassandra  and run  docker-compose up -d  To access  monitoring  dashboard, run  docker-compose up -d \nin  monitoring  as well.", 
            "title": "Run components"
        }, 
        {
            "location": "/staging/chains/", 
            "text": "Current chains nodes tables\n\n\n\n\n\n\n\n\nApp\n\n\ndata path\n\n\nport\n\n\ncreds\n\n\ncurrent size\n\n\n\n\n\n\n\n\n\n\nbitcoind - btc\n\n\n/cyberdata/ssd05tb/bitcoind\n\n\n8332\n\n\ncyber:cyber\n\n\n184 gb\n\n\n\n\n\n\nabc    -   bth\n\n\n/cyberdata/abc\n\n\n18332\n\n\nbitcoin:password\n\n\n156 gb\n\n\n\n\n\n\nparity -   eth\n\n\n/cyberdata/ssd05tb/eth\n\n\n8545\n\n\n\n\n56  gb\n\n\n\n\n\n\nparity -   eth_c\n\n\n/cyberdata/ssd05tb/eth_c\n\n\n18545\n\n\n\n\n5.3 gb\n\n\n\n\n\n\n\n\nLocal forwarding port for chains\n\n\nssh -L 18332:localhost:18332 -L 8332:localhost:8332 \\\n-L 18545:localhost:18545 -L 8545:localhost:8545 \\\nmars@staging.cyber.fund  -p 33322\n\n\n\n\n\nCommands used to run chain and live probe\n\n\nBitcoind\n\n\nRun:\n\n\nsudo  docker run -d -p 8332:8332 --name bitcoind  --restart always \\\n-v /cyberdata/ssd05tb/bitcoind:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\\n-server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0\n\n\n\n\n\nProbe:\n\n\ncurl -X POST -H \nContent-Type: application/json\n \\\n--data \n{\njsonrpc\n:\n2.0\n,\nmethod\n:\ngetblockhash\n,\nparams\n:[1],\nid\n:1}\n \\\nhttp://127.0.0.1:8332 -u cyber:cyber\n\n\n\n\n\nBitcoinABC\n\n\nRun:\n\n\nsudo  docker run -d -p 18332:8332 --name abc  --restart always \n-v /cyberdata/abc:/data zquestz/bitcoin-abc:0.16.1 bitcoind -server -rest\n\n\n\n\n\nProbe:\n\n\ncurl -X POST -H \nContent-Type: application/json\n \\\n--data \n{\njsonrpc\n:\n2.0\n,\nmethod\n:\ngetblockhash\n,\nparams\n:[1],\nid\n:1}\n \\\nhttp://127.0.0.1:18332 -u cyber:cyber\n\n\n\n\n\nParity\n\n\nRun:\n\n\nsudo  docker run -d -p 8545:8545 --name parity_eth  --restart always \\\n-v /cyberdata/ssd05tb/eth:/cyberdata parity/parity:stable \\\n--db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 10\n\n\n\n\n\nProbe:\n\n\ncurl -X POST -H \nContent-Type: application/json\n \\\n--data \n{\njsonrpc\n:\n2.0\n,\nmethod\n:\neth_getBlockByNumber\n,\nparams\n:[\n0x1\n, true],\nid\n:1}\n\\\n http://127.0.0.1:8545\n\n\n\n\n\nParity --chain classic\n\n\nRun:\n\n\nsudo  docker run -d -p 18545:8545 --name parity_etc  --restart always \\\n-v /cyberdata/ssd05tb/eth_c:/cyberdata parity/parity:stable --db-path /cyberdata \\\n--jsonrpc-hosts all --jsonrpc-interface all --chain classic --jsonrpc-threads 10\n\n\n\n\n\nProbe:\n\n\ncurl -X POST -H \nContent-Type: application/json\n \\\n--data \n{\njsonrpc\n:\n2.0\n,\nmethod\n:\neth_getBlockByNumber\n,\nparams\n:[\n0x1\n, true],\nid\n:1}\n \\\nhttp://127.0.0.1:18545", 
            "title": "Local Sync Chains"
        }, 
        {
            "location": "/staging/chains/#current-chains-nodes-tables", 
            "text": "App  data path  port  creds  current size      bitcoind - btc  /cyberdata/ssd05tb/bitcoind  8332  cyber:cyber  184 gb    abc    -   bth  /cyberdata/abc  18332  bitcoin:password  156 gb    parity -   eth  /cyberdata/ssd05tb/eth  8545   56  gb    parity -   eth_c  /cyberdata/ssd05tb/eth_c  18545   5.3 gb", 
            "title": "Current chains nodes tables"
        }, 
        {
            "location": "/staging/chains/#local-forwarding-port-for-chains", 
            "text": "ssh -L 18332:localhost:18332 -L 8332:localhost:8332 \\\n-L 18545:localhost:18545 -L 8545:localhost:8545 \\\nmars@staging.cyber.fund  -p 33322", 
            "title": "Local forwarding port for chains"
        }, 
        {
            "location": "/staging/chains/#commands-used-to-run-chain-and-live-probe", 
            "text": "", 
            "title": "Commands used to run chain and live probe"
        }, 
        {
            "location": "/staging/chains/#bitcoind", 
            "text": "Run:  sudo  docker run -d -p 8332:8332 --name bitcoind  --restart always \\\n-v /cyberdata/ssd05tb/bitcoind:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\\n-server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0  Probe:  curl -X POST -H  Content-Type: application/json  \\\n--data  { jsonrpc : 2.0 , method : getblockhash , params :[1], id :1}  \\\nhttp://127.0.0.1:8332 -u cyber:cyber", 
            "title": "Bitcoind"
        }, 
        {
            "location": "/staging/chains/#bitcoinabc", 
            "text": "Run:  sudo  docker run -d -p 18332:8332 --name abc  --restart always \n-v /cyberdata/abc:/data zquestz/bitcoin-abc:0.16.1 bitcoind -server -rest  Probe:  curl -X POST -H  Content-Type: application/json  \\\n--data  { jsonrpc : 2.0 , method : getblockhash , params :[1], id :1}  \\\nhttp://127.0.0.1:18332 -u cyber:cyber", 
            "title": "BitcoinABC"
        }, 
        {
            "location": "/staging/chains/#parity", 
            "text": "Run:  sudo  docker run -d -p 8545:8545 --name parity_eth  --restart always \\\n-v /cyberdata/ssd05tb/eth:/cyberdata parity/parity:stable \\\n--db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 10  Probe:  curl -X POST -H  Content-Type: application/json  \\\n--data  { jsonrpc : 2.0 , method : eth_getBlockByNumber , params :[ 0x1 , true], id :1} \\\n http://127.0.0.1:8545", 
            "title": "Parity"
        }, 
        {
            "location": "/staging/chains/#parity-chain-classic", 
            "text": "Run:  sudo  docker run -d -p 18545:8545 --name parity_etc  --restart always \\\n-v /cyberdata/ssd05tb/eth_c:/cyberdata parity/parity:stable --db-path /cyberdata \\\n--jsonrpc-hosts all --jsonrpc-interface all --chain classic --jsonrpc-threads 10  Probe:  curl -X POST -H  Content-Type: application/json  \\\n--data  { jsonrpc : 2.0 , method : eth_getBlockByNumber , params :[ 0x1 , true], id :1}  \\\nhttp://127.0.0.1:18545", 
            "title": "Parity --chain classic"
        }, 
        {
            "location": "/staging/kubernetes/", 
            "text": "Prerequisites\n\n\n\n\n\n\nInstall \nkubectl\n\nfor interacting with Kubernetes.\n\n\n\n\n\n\nInstall \nminikube\n if you want to\ntest setup locally. Make sure to install driver for your OS or else slow\nVirtualBox will be used.\n\n\n\n\n\n\nRunning \nminikube\n for local testing\n\n\n$ minikube start --vm-driver kvm2\nStarting local Kubernetes v1.9.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.\n\n\n\n\n\nCheck status.\n\n\n$ minikube status\nminikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.39.6\n\n\n\n\n\nRun dashboard.\n\n\n$ minikube dashboard\nOpening kubernetes dashboard in default browser...\n\n\n\n\n\nRun single container in a cluster\n\n\n$ kubectl run miniecho --image=k8s.gcr.io/echoserver:1.9 --port=8080\n\n\n\n\n\nThis adds \ndeployment\n called \nminiecho\n to cluster, runs Docker \nechoserver\n\nimage in it and tells docker to open port 8080. The port is not accessible\nuntil we create \nservice\n.\n\n\n$ kubectl expose deployment miniecho --type=NodePort\nservice \nminiecho\n exposed\n\n\n\n\n\nThis makes \nminiecho\n service accessible from your machine on random port.\n\n\n$ minikube service list\n|-------------|----------------------|---------------------------|\n|  NAMESPACE  |         NAME         |            URL            |\n|-------------|----------------------|---------------------------|\n| default     | kubernetes           | No node port              |\n| default     | miniecho             | http://192.168.39.6:30426 |\n| kube-system | kube-dns             | No node port              |\n| kube-system | kubernetes-dashboard | http://192.168.39.6:30000 |\n|-------------|----------------------|---------------------------|", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/staging/kubernetes/#prerequisites", 
            "text": "Install  kubectl \nfor interacting with Kubernetes.    Install  minikube  if you want to\ntest setup locally. Make sure to install driver for your OS or else slow\nVirtualBox will be used.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/staging/kubernetes/#running-minikube-for-local-testing", 
            "text": "$ minikube start --vm-driver kvm2\nStarting local Kubernetes v1.9.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.  Check status.  $ minikube status\nminikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.39.6  Run dashboard.  $ minikube dashboard\nOpening kubernetes dashboard in default browser...", 
            "title": "Running minikube for local testing"
        }, 
        {
            "location": "/staging/kubernetes/#run-single-container-in-a-cluster", 
            "text": "$ kubectl run miniecho --image=k8s.gcr.io/echoserver:1.9 --port=8080  This adds  deployment  called  miniecho  to cluster, runs Docker  echoserver \nimage in it and tells docker to open port 8080. The port is not accessible\nuntil we create  service .  $ kubectl expose deployment miniecho --type=NodePort\nservice  miniecho  exposed  This makes  miniecho  service accessible from your machine on random port.  $ minikube service list\n|-------------|----------------------|---------------------------|\n|  NAMESPACE  |         NAME         |            URL            |\n|-------------|----------------------|---------------------------|\n| default     | kubernetes           | No node port              |\n| default     | miniecho             | http://192.168.39.6:30426 |\n| kube-system | kube-dns             | No node port              |\n| kube-system | kubernetes-dashboard | http://192.168.39.6:30000 |\n|-------------|----------------------|---------------------------|", 
            "title": "Run single container in a cluster"
        }, 
        {
            "location": "/cybernode/about/", 
            "text": "", 
            "title": "About Node"
        }, 
        {
            "location": "/cybernode/dev-setup/", 
            "text": "", 
            "title": "Development Setup"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/", 
            "text": "K8s cheat sheet\n\n\nk8s dashboard\n\n\nLocal dashboard proxy:\n\n\nkubectl proxy\n\n\n\n\n\nGet cluster access token:\n\n\nkubectl config view \n|\n grep -A10 \nname: \n$(\nkubectl config current-context\n)\n \n|\n awk \n$1==\naccess-token:\n{print $2}\n\n\n\n\n\n\nForward pod port on localhost\n\n\nkubectl port-forward -n monitoring prometheus-kube-prometheus-0 \n9090\n\n\n\n\n\n\nElassandra commands\n\n\nGet nodes status:\n\n\nkubectl \nexec\n -n search elassandra-0 -- nodetool status\n\n\n\n\n\nOpen cqlsh CLI tool\n\n\nkubectl \nexec\n -it -n search elassandra-0 -- cqlsh\n\n\n\n\n\nDive into elassandra docker container shell(index stats, delete index commands examples)\n\n\nkubectl \nexec\n -n search -it elassandra-0 bash\ncurl -XGET \nlocalhost:9200/_cat/indices?v\npretty\n\ncurl -XDELETE \nlocalhost:9200/twitter?pretty", 
            "title": "K8s Cheat Sheet"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/#k8s-cheat-sheet", 
            "text": "", 
            "title": "K8s cheat sheet"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/#k8s-dashboard", 
            "text": "Local dashboard proxy:  kubectl proxy  Get cluster access token:  kubectl config view  |  grep -A10  name:  $( kubectl config current-context )   |  awk  $1== access-token: {print $2}", 
            "title": "k8s dashboard"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/#forward-pod-port-on-localhost", 
            "text": "kubectl port-forward -n monitoring prometheus-kube-prometheus-0  9090", 
            "title": "Forward pod port on localhost"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/#elassandra-commands", 
            "text": "Get nodes status:  kubectl  exec  -n search elassandra-0 -- nodetool status  Open cqlsh CLI tool  kubectl  exec  -it -n search elassandra-0 -- cqlsh  Dive into elassandra docker container shell(index stats, delete index commands examples)  kubectl  exec  -n search -it elassandra-0 bash\ncurl -XGET  localhost:9200/_cat/indices?v pretty \ncurl -XDELETE  localhost:9200/twitter?pretty", 
            "title": "Elassandra commands"
        }, 
        {
            "location": "/cybernode/components/monitoring/", 
            "text": "Monitoring Components\n\n\nDuring lifetime cybernode collects various metrics using following components:\n\n\n\n\n\n\n\n\nComponent\n\n\nDescription\n\n\nCluster Address\n\n\nExternal\n\n\n\n\n\n\n\n\n\n\nPrometheus Operator(PO)\n\n\nManages Prometheus Configuration\n\n\n\n\n\n\n\n\n\n\nPrometheus\n\n\nMetrics Storage\n\n\nprometheus.monitoring.svc:9090\n\n\n\n\n\n\n\n\nDefault Service Monitor\n\n\nDefault Service Monitor for PO\n\n\n\n\n\n\n\n\n\n\nGrafana\n\n\nMetrics Alerts and Web UI\n\n\ngrafana.monitoring.svc:3000\n\n\ny\n\n\n\n\n\n\n\n\nPrometheus\n\n\nPrometheus\n is used as a main metrics storage. Components collect metrics and expose them via \n HTTP endpoint, that Prometheus pulls at configured interval(our is 15s).\n\n\nPrometheus Operator\n\n\nTo deploy Prometheus atop Kubernetes, we use \nPrometheus Operator\n.\n It consists of Prometheus Operator itself, that introduced to cluster \n \nCRDs\n: Prometheus,\n Service Monitor. PO automatically(dynamically) generate monitoring target configurations based on familiar Kubernetes label queries; \n no need to learn a Prometheus specific configuration language.\n\n\nDefault Service Monitor\n\n\nTo enable dynamic mapping for new pods to prometheus, service monitor(SM) is used. Each SM defines set of kubernetes \n label filters.\n\n\nGrafana\n\n\nTo visualize metrics collected by Prometheus we use \nGrafana\n. In order to configure Grafana, three\n \nconfig maps\n are used:\n\n\n\n\ngrafana-datasources\n. Define Prometheus datasource. \n\n\ngrafana-dashboards\n. Gather predefined dashboards into single folder. \n\n\ngrafana-dashboards-providers\n. Configuration for importing dashboards. \n\n\n\n\nGrafana Alerts\n\n\nAlso Grafana supports checking metrics satisfied defined criteria. If criteria is violated, than notification is raised \n to setup alert channels(in our case Telegram Group). To view list of available alerts check \n \"{grafana_host}/alerting/list\" page.", 
            "title": "Monitoring"
        }, 
        {
            "location": "/cybernode/components/monitoring/#monitoring-components", 
            "text": "During lifetime cybernode collects various metrics using following components:     Component  Description  Cluster Address  External      Prometheus Operator(PO)  Manages Prometheus Configuration      Prometheus  Metrics Storage  prometheus.monitoring.svc:9090     Default Service Monitor  Default Service Monitor for PO      Grafana  Metrics Alerts and Web UI  grafana.monitoring.svc:3000  y", 
            "title": "Monitoring Components"
        }, 
        {
            "location": "/cybernode/components/monitoring/#prometheus", 
            "text": "Prometheus  is used as a main metrics storage. Components collect metrics and expose them via \n HTTP endpoint, that Prometheus pulls at configured interval(our is 15s).", 
            "title": "Prometheus"
        }, 
        {
            "location": "/cybernode/components/monitoring/#prometheus-operator", 
            "text": "To deploy Prometheus atop Kubernetes, we use  Prometheus Operator .\n It consists of Prometheus Operator itself, that introduced to cluster \n  CRDs : Prometheus,\n Service Monitor. PO automatically(dynamically) generate monitoring target configurations based on familiar Kubernetes label queries; \n no need to learn a Prometheus specific configuration language.", 
            "title": "Prometheus Operator"
        }, 
        {
            "location": "/cybernode/components/monitoring/#default-service-monitor", 
            "text": "To enable dynamic mapping for new pods to prometheus, service monitor(SM) is used. Each SM defines set of kubernetes \n label filters.", 
            "title": "Default Service Monitor"
        }, 
        {
            "location": "/cybernode/components/monitoring/#grafana", 
            "text": "To visualize metrics collected by Prometheus we use  Grafana . In order to configure Grafana, three\n  config maps  are used:   grafana-datasources . Define Prometheus datasource.   grafana-dashboards . Gather predefined dashboards into single folder.   grafana-dashboards-providers . Configuration for importing dashboards.", 
            "title": "Grafana"
        }, 
        {
            "location": "/cybernode/components/monitoring/#grafana-alerts", 
            "text": "Also Grafana supports checking metrics satisfied defined criteria. If criteria is violated, than notification is raised \n to setup alert channels(in our case Telegram Group). To view list of available alerts check \n \"{grafana_host}/alerting/list\" page.", 
            "title": "Grafana Alerts"
        }, 
        {
            "location": "/cybernode/components/search/", 
            "text": "Search Components\n\n\n\n\n\n\n\n\nComponent\n\n\nScale\n\n\nDescription\n\n\nCluster Address\n\n\nMetrics\n\n\nExternal\n\n\n\n\n\n\n\n\n\n\nKafka\n\n\n1\n\n\nData Entry\n\n\n\n\n\n\n\n\n\n\n\n\nKafka Manager\n\n\n1\n\n\nKafka Explorer\n\n\n\n\n\n\n\n\n\n\n\n\nElassandra\n\n\n1 - N\n\n\nData Back And Search\n\n\nelassandra.search.svc:9042\n\n\n\n\n\n\n\n\n\n\nEthereum Pump\n\n\n1\n\n\nEthereum Chain Data Kafka Pump\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\nEthereum Cassandra Dump\n\n\n1\n\n\nDump Kafka Topics Into Cassandra\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\nEthereum Contract Summary\n\n\n1 - N\n\n\nCalculates Contract Summaries (balances and etc)\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\nSearch Api\n\n\n1 - N\n\n\nApi To Search Chain Entities\n\n\nsearch-api.search.svc:80\n\n\n8080:/actuator/metrics\n\n\ny\n\n\n\n\n\n\nSearch Api Docs\n\n\n1 - N\n\n\nSearch Api Docs Based On Swagger\n\n\nsearch-api-docs.search.svc:80\n\n\n\n\ny\n\n\n\n\n\n\n\n\n\n\nKafka\n\n\nKafka\n is message queue used as main data entry. All raw data firstly goes into kafka, than \n various services consume it to provide new functionality.\n\n\nElassandra (Elastic + Cassandra)\n\n\nElassandra\n is used as main backend storage with linear growth fulltext-search\n index on source cassandra data.\n\n\nEthereum Pump\n\n\nPumps Ethereum raw data(block,tx,uncles) into Kafka.\n\n\nEthereum Cassandra Dump\n\n\nConsumes Ethereum raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.\n\n\nEthereum Contract Summary\n\n\nCollects summary for each contract stored in Ethereum chain. Consumes Kafka topics and update data in Elassandra.", 
            "title": "Search"
        }, 
        {
            "location": "/cybernode/components/search/#search-components", 
            "text": "Component  Scale  Description  Cluster Address  Metrics  External      Kafka  1  Data Entry       Kafka Manager  1  Kafka Explorer       Elassandra  1 - N  Data Back And Search  elassandra.search.svc:9042      Ethereum Pump  1  Ethereum Chain Data Kafka Pump   8080:/actuator/metrics     Ethereum Cassandra Dump  1  Dump Kafka Topics Into Cassandra   8080:/actuator/metrics     Ethereum Contract Summary  1 - N  Calculates Contract Summaries (balances and etc)   8080:/actuator/metrics     Search Api  1 - N  Api To Search Chain Entities  search-api.search.svc:80  8080:/actuator/metrics  y    Search Api Docs  1 - N  Search Api Docs Based On Swagger  search-api-docs.search.svc:80   y", 
            "title": "Search Components"
        }, 
        {
            "location": "/cybernode/components/search/#kafka", 
            "text": "Kafka  is message queue used as main data entry. All raw data firstly goes into kafka, than \n various services consume it to provide new functionality.", 
            "title": "Kafka"
        }, 
        {
            "location": "/cybernode/components/search/#elassandra-elastic-cassandra", 
            "text": "Elassandra  is used as main backend storage with linear growth fulltext-search\n index on source cassandra data.", 
            "title": "Elassandra (Elastic + Cassandra)"
        }, 
        {
            "location": "/cybernode/components/search/#ethereum-pump", 
            "text": "Pumps Ethereum raw data(block,tx,uncles) into Kafka.", 
            "title": "Ethereum Pump"
        }, 
        {
            "location": "/cybernode/components/search/#ethereum-cassandra-dump", 
            "text": "Consumes Ethereum raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.", 
            "title": "Ethereum Cassandra Dump"
        }, 
        {
            "location": "/cybernode/components/search/#ethereum-contract-summary", 
            "text": "Collects summary for each contract stored in Ethereum chain. Consumes Kafka topics and update data in Elassandra.", 
            "title": "Ethereum Contract Summary"
        }, 
        {
            "location": "/cybernode/components/markets/", 
            "text": "Markets Components\n\n\n\n\n\n\n\n\nComponent\n\n\nScale\n\n\nDescription\n\n\nCluster Address\n\n\nMetrics\n\n\nExternal\n\n\n\n\n\n\n\n\n\n\nKafka\n\n\n1\n\n\nData Entry\n\n\n\n\n\n\n\n\n\n\n\n\nKafka Manager\n\n\n1\n\n\nKafka Explorer\n\n\n\n\n\n\n\n\n\n\n\n\nElassandra\n\n\n1 - N\n\n\nData Back\n\n\n\n\n\n\n\n\n\n\n\n\nExchanges Connector\n\n\n1 - N\n\n\nConnect to CEXs/DEXs for raw data\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\nTickers\n\n\n1 - N\n\n\nCalculate Tickers From Raw Data\n\n\n\n\n\n\n\n\n\n\n\n\nMarkets REST Api\n\n\n1 - N\n\n\nRest Api To Markets Entities\n\n\n\n\n\n\ny\n\n\n\n\n\n\nMarkets Stream\n\n\n1 - N\n\n\nWeb Socket Api To Markets Entities\n\n\n\n\n\n\ny\n\n\n\n\n\n\nMarkets Api Docs\n\n\n1 - N\n\n\nMarkets Api Docs Based On Swagger\n\n\n\n\n\n\ny\n\n\n\n\n\n\n\n\n\n\nKafka\n\n\nKafka\n is message queue used as main data entry. All raw data firstly goes into kafka, than \n various services consume it to provide new functionality.\n\n\nElassandra (Elastic + Cassandra)\n\n\nElassandra\n is used as main backend storage with linear growth fulltext-search\n index on source cassandra data.\n\n\nExchanges Connector\n\n\nCollect raw data from centralized and decentalized exchanges such as trades, orderbooks and put it to kafka.\n\n\nTickers\n\n\nCalculate tickers, prices, etc and aggregate raw data which collecting exchanges connectors module", 
            "title": "Markets"
        }, 
        {
            "location": "/cybernode/components/markets/#markets-components", 
            "text": "Component  Scale  Description  Cluster Address  Metrics  External      Kafka  1  Data Entry       Kafka Manager  1  Kafka Explorer       Elassandra  1 - N  Data Back       Exchanges Connector  1 - N  Connect to CEXs/DEXs for raw data   8080:/actuator/metrics     Tickers  1 - N  Calculate Tickers From Raw Data       Markets REST Api  1 - N  Rest Api To Markets Entities    y    Markets Stream  1 - N  Web Socket Api To Markets Entities    y    Markets Api Docs  1 - N  Markets Api Docs Based On Swagger    y", 
            "title": "Markets Components"
        }, 
        {
            "location": "/cybernode/components/markets/#kafka", 
            "text": "Kafka  is message queue used as main data entry. All raw data firstly goes into kafka, than \n various services consume it to provide new functionality.", 
            "title": "Kafka"
        }, 
        {
            "location": "/cybernode/components/markets/#elassandra-elastic-cassandra", 
            "text": "Elassandra  is used as main backend storage with linear growth fulltext-search\n index on source cassandra data.", 
            "title": "Elassandra (Elastic + Cassandra)"
        }, 
        {
            "location": "/cybernode/components/markets/#exchanges-connector", 
            "text": "Collect raw data from centralized and decentalized exchanges such as trades, orderbooks and put it to kafka.", 
            "title": "Exchanges Connector"
        }, 
        {
            "location": "/cybernode/components/markets/#tickers", 
            "text": "Calculate tickers, prices, etc and aggregate raw data which collecting exchanges connectors module", 
            "title": "Tickers"
        }, 
        {
            "location": "/cybernode/components/chain-nodes/", 
            "text": "Chain Nodes Components\n\n\n\n\n\n\n\n\nComponent\n\n\nCluster Address\n\n\n\n\n\n\n\n\n\n\nParity 1.9.6 eth\n\n\nparity-eth.chains.svc:8545", 
            "title": "Chains Nodes"
        }, 
        {
            "location": "/cybernode/components/chain-nodes/#chain-nodes-components", 
            "text": "Component  Cluster Address      Parity 1.9.6 eth  parity-eth.chains.svc:8545", 
            "title": "Chain Nodes Components"
        }, 
        {
            "location": "/cybernode/chains/bitcoin/", 
            "text": "", 
            "title": "Bitcoin"
        }, 
        {
            "location": "/cybernode/chains/ethereum/", 
            "text": "Known issues\n\n\n\n\nFor \n2806264 block\n there is a transaction with size more than 1mb, general rule, size of item can be \n1mb.", 
            "title": "Ethereum"
        }, 
        {
            "location": "/cybernode/chains/ethereum/#known-issues", 
            "text": "For  2806264 block  there is a transaction with size more than 1mb, general rule, size of item can be  1mb.", 
            "title": "Known issues"
        }
    ]
}