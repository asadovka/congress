{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to cyber\u2022Search knowledge base.\n\n\nRight now these docs cover eight open source projects:\n\n\n\n\ncybernode\n - smart node manager and transaction crawler\n\n\ncyber-search\n - transaction parser for cybernode\n\n\ncyber-markets\n - toolchain for parsing of orders and trades\n\n\ncyb-js\n - cyber-search javascript library\n\n\ncyb\n - indexing and searching information stored in different chains\n\n\nchaingear\n - create your own Registry of general purpose entries on Ethereum blockchain\n\n\ncyberd\n - research on the cyber protocol\n\n\ncybercongress\n - community of of scientists, developers, engineers and craftsmen", 
            "title": "Home"
        }, 
        {
            "location": "/contribute/", 
            "text": "Current wiki is built on top of \nmkdocs.org\n engine with\n\nMaterial for MkDocs\n extensions pack.\n\n\nRequired Installations\n\n\n\n\nhttps://hub.docker.com/r/squidfunk/mkdocs-material/\n\n\n\n\nCommands Cheat Sheet\n\n\n\n\ndocker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material\n\n\ndocker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build\n\n\ndocker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs squidfunk/mkdocs-material gh-deploy\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Contribute"
        }, 
        {
            "location": "/contribute/#required-installations", 
            "text": "https://hub.docker.com/r/squidfunk/mkdocs-material/", 
            "title": "Required Installations"
        }, 
        {
            "location": "/contribute/#commands-cheat-sheet", 
            "text": "docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material  docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build  docker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs squidfunk/mkdocs-material gh-deploy", 
            "title": "Commands Cheat Sheet"
        }, 
        {
            "location": "/contribute/#project-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/cyb/requirements/", 
            "text": "Tha main purpose of this document is to describe requirements and the structure of cyb and apps for MVP.\n\n\nProject description\n\n\nCyber browser - an entrance point to start working with blockchains.\n\n\nRequirements\n\n\n1. Common functional requirements\n\n\n\n\nIntegration with Metamask\n\n\nIPFS integration\n\n\nPrefered type of data structure - tables\n\n\n\n\n2. Common non-functional requirements\n\n\n\n\nSimple and attractive design\n\n\nIntuitive UI\n\n\nAdaptive design for mobile devices\n\n\nDisplay IPFS hash for all objects\n\n\nAll hashes should be easy to copy\n\n\n\n\n3. System requirements\n\n\n\n\nless than 1 second for loading page\n\n\nless than 3 seconds for loading all data\n\n\nWeb version of browser (React)\n\n\nDesktop version (Electron + React)\n\n\nMobile web version\n\n\n\n\nBrowser structure:\n\n\n0 Shell.\n\n\nSearch bar, App bar, State bar, Context bar.\n\n\n0.1 Search (Path) bar\n\n\nTop of the page\n\n\n\n\nsearch in existing list of apps\n\n\nsearch in blockchain explorer app\n\n\nsearch in token monitor app\n\n\nsearch path\n\n\n\n\n0.2 App bar:\n\n\nLeft menu\n\n\n\n\nlist of installed apps (4 blockchain explorers, chaingear, token monitor)\n\n\nadd/delete app  \n\n\n\n\n0.3 State bar:\n\n\n\n\ndesync state\n\n\nmetamask instelled state (Y/N)\n\n\ntx processing state\n\n\n\n\n0.4 Context bar:\n\n\n\n\naccount logo\n\n\naccount address\n\n\naccount balance\n\n\n\n\nsettings:\n\n\n\n\nlanguage\n\n\nlocation\n\n\ncurrency (USD/ETH/BTC)\n\n\nactivity type (private/anonymous/public)\n\n\n\n\nUSE CASES:\n\n\n\n\nno metamask - install metamask\n\n\nnon auth user- log in metamask\n\n\nauth user - account logo\n\n\n\n\n1. Main Page\n\n\nPurpose:\n accent the user's attention on all browser apps. \n\n\nDesign \n UI features:\n simple and attractive design, hints to start usage of cyber products.\n\n\nThere is a status text below search panel which describes technical information about cyber.Search products:\n\n\nSearch in 134 M transactions in 2 blockchain systems with 135 parsed tokens. Database size : 369 GBs\n \n\n\nWhere:\n\n\n\n\nTransactions [number] - number of all indexed transactions from all blockchains connected to Cybernode. \n\n\nBlockchain systems [number] - all blockchains processed by Cybernode.\n\n\nTokens [numger] - all unique tokens from all blockchains indexed. \n\n\nDatabase size [number + Gb] - size of Cassandra (index) database.\n\n\n\n\nThere are 3 main widgets below the status string describing the cryptoeconomy, registers and portfolio. Their apperance depends on user type:\n\n\n\n\n\n\nUser without Metamask\n\n\n\n\n\n\nTotal market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD. \n\n\nCall to action:\n install Metamask and transit to tokens page\n.\n\n\n\n\n\n\nChaingear registers [number] - number of created registers in Chaingear.\n\n\nCall to action:\n install Metamask and transit to Chaingear page\n.\n\n\n\n\n\n\nApps [number] - number of browser apps.\n\n\nCall to action:\n deploy app\n.\n\n\n\n\n\n\nUser with Metamask (no activities in system)\n\n\n\n\n\n\nTotal market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD. \n\n\nCall to action:\n transit to tokens page\n.\n\n\n\n\n\n\nChaingear registers [number] - number of created registers in Chaingear.\n\n\nCall to action:\n transit to Chaingear page, hint to create register or record\n.\n\n\n\n\n\n\nApps [number] - number of browser apps.\n\n\nCall to action:\n deploy app\n.\n\n\n\n\n\n\nUser with Metamask (no activities in system)\n\n\n\n\n\n\nTotal market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD. \n\n\nCall to action:\n transit to tokens page\n.\n\n\n\n\n\n\nChaingear registers [number] - number of created registers/records in Chaingear.\n\n\nCall to action:\n Chaingear page\n.\n\n\n\n\n\n\nApps [number] - number of browser apps.\n\n\nCall to action:\n deploy app\n.\n\n\n\n\n\n\nAt the bottom of the page 5 project links are placed:\n1. GitHub - GitHub repository of cyber.Congress [https://github.com/cybercongress]\n2. Roadmap - roadmap for cyber.Search project [https://github.com/orgs/cybercongress/projects/1]\n3. Cybernode - Cybernode stats page [cybersearch.live]\n4. Dashboard - a link user's custom dashboard []\n5. Knowledge - a link to knowledge database of cyber.Search project [cybersearch.io]\n\n\n2. App list\n\n\nPurpose:\n accent the user's attention on all browser apps. \n\n\nDesign \n UI features:\n simple and attractive design, hints to start usage of apps.\n\n\nThere is list of browser apps with categories (with brief overview):\n\n\n\n\n\n\nBlockchain explorers - Search in 135 million transactions in 4 blockchains\n\n\n\n\n\n\nEthereum Explorer\n\n\n\n\nEthereum Classic explorer\n\n\nBitcoin Explorer\n\n\n\n\nBitcoin Cash explorer\n\n\n\n\n\n\nCybernomics - Watch and anayze 135 parsed tokens in 7 exchanges \n\n\n\n\n\n\nToken Monitor\n\n\n\n\nExchange Monitor\n\n\n\n\nBlockchain Monitor\n\n\n\n\n\n\nRegisters - Store your records in blockchains\n\n\n\n\n\n\nChaingear\n\n\n\n\n\n\nAccounting - Manage your cryptoassets\n\n\n\n\n\n\nPortfolio\n\n\n\n\n\n\nThere are 2 versions of apps:\n\n\n\n\nstable released apps\n\n\ndeveloping apps\n\n\n\n\nProposals:\n\n\n\n\ndonation button for developing apps\n\n\ninstall to App bar button (in app is not installed)\n\n\n\n\nApplications\n\n\n\n\n\n\nBlockchain explorer\n\n\n\n\n\n\nBitcoin\n\n\n\n\nBitcoin Cash\n\n\nEthereum\n\n\nEthereum Classic\n\n\n\n\n2. Search Results Page\n\n\nPurpose:\n provide easy and quick functionality for working with blockchain search.\n\n\nDesign \n UI  features:\n strictly logical UI, adaptive preferenses of filtration and sorting.\n\n\nObjects of search.\n\n\nThere are 2 types of search provided by browser:\n1. Global search (searching in whole ecosystem of indexed objects)\n2. Context search (searching the data in certain pages)\n\n\nThere are 4 systems (blockchains) in which you can find data:\n\n\n\n\nBitcoin\n\n\nBitcoin Cash\n\n\nEthereum\n\n\nEthereum Classic\n\n\n\n\nThere are 4 types of objects that can be foung in listed systems:\n\n\n\n\nContracts\n\n\nTransactions\n\n\nBlocks\n\n\nUncle blocks\n\n\n\n\nObjects can be found by entering next types of queries:\n\n\n\n\nFull hash (address, block, uncle, transaction)\n\n\nNumber (block/uncle)\n\n\n\n\nThe search pannel in general should include next functions:\n1. Global and local search (GitHub style)\n2. Autocomplete function\n\n\nSearch results.\n\n\nLeft menu includes next hardcoded functions:\n\n\n\n\nDisplay listed systems (blockchains)\n\n\nDisplay listed objects\n\n\n\n\nSearch results in general are shown as a list of object preview. Each object preview its own structure:\n\n\n\n\n\n\nTransaction:\n\n\n\n\n\n\nTime (finalization, or time of confirmation or \"Mempool\" status)\n\n\n\n\nHash\n\n\n\n\nValue\n\n\n\n\n\n\nBlock + number\n\n\n\n\n\n\nTime of creation\n\n\n\n\nHash\n\n\n\n\nNumber of transactions\n\n\n\n\n\n\nUncle + number\n\n\n\n\n\n\nTime  of creation\n\n\n\n\nHash\n\n\n\n\nUncle position\n\n\n\n\n\n\nContract\n\n\n\n\n\n\nTime of creation\n\n\n\n\nHash\n\n\nValue\n\n\n\n\nEach preview has clickable hash string, that leads to block/uncle, contract or transaction page.\n\n\nThere is a infinite page scroll function on results page.\n\n\n2.1 Contract Page\n\n\nCurrently browser shows 2 types of contract pages:\n\n\n\n\nEthereum (Ethereum Classic) contract page\n\n\nBitcoin (Bitcoin Cash) contract page\n\n\n\n\n2.1.1 Bitcoin contract\n\n\nDisplayed data:\n- Robohash logo\n- QR code of address hash\n\n\nGeneral\n\n\n\n\nUTC Time [date] - time of contract getting into blockchain\n\n\nBalance [number + currency] - BTC available to withdraw\n\n\nHash [string] - hash of address\n\n\n\n\nCashflow\n\n\n\n\nTransactions [number] - number of transactions in contract\n\n\nUnconfirmed transactions [number] - number of transactions in mempool\n\n\nAccumulated income [number + currency] - received BTC\n\n\nPending income [number + currency] - BTC in mempool transactions\n\n\n\n\nCharts:\n\n\n\n\nValuation tab (Regular graph, all above zero):\n\n\n\n\nValuation / Time - balance of contract on each period of time\n\n\n\n\n\n\nTransactions tab (incoming tx - above zero, outcoming - below):\n\n\n\n\nTransactions / Time - activity of transactions by contract on each period of time\n\n\n\n\nTransactions and blocks;\n\n\n\n\n\n\nTransactions tab:\n\n\n\n\n\n\nUTC Time [date] - time of getting transaction to mempool\n\n\n\n\nHash [string] - hash of transaction\n\n\nBlock [number] - number of block\n\n\nSender[number]- number of inputs\n\n\nValue[number + currency] - total input balance in BTC\n\n\nReceiver [number]- number of outputs\n\n\nValue [number + currency] - total output balance in BTC\n\n\nFee [number + currency] - accumulated fees in BTC\n\n\n\n\nState [string] - \"Confirmed\", \"Mempool\", \"Finalized\"\n\n\n\n\n\n\nMined blocks (for miner address only):\n\n\n\n\nUTC Time [date] - time of block generation\n\n\nBlock [number] - number of mined block\n\n\nTransactions [number] - number of transactions in mined block\n\n\nReward [number + currency] - rewards for block in BTC\n\n\n\n\nCode:\n- bitcoin scripts\n\n\nActions:\n\n\n\n\nInfo by pointing:\n\n\nTime (UTC) - show age of transaction (current time minus mempool)\n\n\n\n\nAddress - show label of address\n\n\n\n\n\n\nClicking\n\n\n\n\nHash string - copy string in buffer\n\n\n\n\nTransaction row - expand transaction details (inputs and outputs)\n\n\n\n\n\n\nLabeling\n\n\n\n\nlabeling via button \"label it\"\n\n\n\n\n2.1.2 Ethereum contract\n\n\nDisplayed data:\n- Robohash logo\n- QR code of address hash\n\n\nGeneral:\n\n\n\n\nUTC Time [date] - time of contract getting into blockchain\n\n\nBalance [number + currency] - ETH available to withdraw\n\n\nHash [string] - hash of address\n\n\n\n\nCashflow:\n\n\n\n\nTransactions [number] - number of transactions in contract\n\n\nUnconfirmed transactions [number] - number of transactions in mempool\n\n\nAccumulated income [number + currency] - received ETH\n\n\nPending income [number + currency] - ETH in mempool transactions\n\n\n\n\nCharts:\n\n\n\n\nValuation tab (Regular graph, all above zero):\n\n\n\n\nValuation / Time - balance of contract on each period of time\n\n\n\n\n\n\nTransactions tab (incoming tx - above zero, outcoming - below):\n\n\n\n\nTransactions / Time - activity of transactions by contract on each period of time\n\n\n\n\nTransactions and blocks;\n\n\n\n\nTransactions tab:\n\n\nUTC Time [date] - time of getting transaction to mempool\n\n\nHash [string] - hash of transaction\n\n\nBlock [number] - number of block\n\n\nSender[hash]- hash of \"from\" address\n\n\nReceiver [hash]- hash of \"to\" address\n\n\nValue [number + currency] - tx value in ETH\n\n\nFee [number + currency] - accumulated fees in ETH\n\n\n\n\nState [string] - \"Confirmed\", \"Mempool\", \"Finalized\"\n\n\n\n\n\n\nOperations tab (if available):\n\n\n\n\nUTC Time [date] - time of getting transaction to mempool\n\n\nType [string] - type of internal tx (call, delegate call, destroy, create)\n\n\nSender[hash]- hash of \"from\" address\n\n\nReceiver [hash]- hash of \"to\" address\n\n\nValue [number + currency] - total input balance in ETH\n\n\nGas used [number] - gas used\n\n\nGas limit [number] - gas limit\n\n\n\n\nState [string] - \"Failed\", \"Reverted\", \"Successful\"\n\n\n\n\n\n\nTokens tab:\n\n\n\n\nToken [string] - token name\n\n\nHash [string] - transaction hash\n\n\nSender [number + currency] - sent tokens\n\n\nReceiver [number + currency] - received tokens\n\n\n\n\nValue [number + currency] - sent minus received tokens\n\n\n\n\n\n\nMined blocks tab (for miner address only):\n\n\n\n\nUTC Time [date] - time of block generation\n\n\nBlock [number] - number of mined block\n\n\nTransactions [number] - number of transactions in mined block\n\n\n\n\nReward [number + currency] - rewards for block in ETH\n\n\n\n\n\n\nMined uncles tab (if available):\n\n\n\n\n\n\nUTC Time [date] - time of block generation\n\n\n\n\nHash [string] - uncle hash\n\n\nBlock [number] - number of block with uncle\n\n\n\n\nUncle [number] - number of mined uncle\n\n\n\n\n\n\nReward [number + currency] - rewards for uncle in ETH\n\n\n\n\n\n\nCode:\n\n\nContract code:\n- Contract name [string] - name of contract\n- Compiler version [string] - version of compiler\nSource code - code of contract\nABI - contract ABI\nSwarm code - link in ethereum swarm\n\n\nActions:\n\n\n\n\nInfo by pointing:\n\n\nTime  - show age of transaction (current time minus mempool)\n\n\n\n\nAddress - show label of address\n\n\n\n\n\n\nClicking\n\n\n\n\n\n\nHash string - copy string in buffer\n\n\n\n\n\n\nLabeling\n\n\n\n\nlabeling via button \"label it\"\n\n\n\n\n2.2 Transaction Page\n\n\nCurrently browser shows 2 types of transaction pages:\n\n\n\n\nEthereum (Ethereum Classic) transaction page\n\n\nBitcoin (Bitcoin Cash) transaction page\n\n\n\n\n2.2.1 Bitcoin transaction\n\n\nDisplayed data:\n\n\nGeneral\n\n\n\n\nUTC Time [date] - time of getting transaction to mempool\n\n\nHash [string] - hash of transaction\n\n\nValue [number + currency] - total transaction value in BTC\n\n\nState [string] - \"Confirmed\", \"Mempool\", \"Finalized\"\n\n\n\n\nBlockchain specific\n\n\n\n\nBlock [number] - number of block\n\n\nSize - [number + bytes] - size of transaction in bytes\n\n\nConfirmations [number] - number of confirmations (for confirmed or finalized transactions)\n\n\nInputs [number] - number of input addresses\n\n\nOutputs [number] - number of output addresses\n\n\n\n\nFees\n\n\n\n\nFee [number + currency] - accumulated fees in BTC\n\n\nFee per byte [number + satoshi/Byte] - fee/size\n\n\nFee per weight unit [number + satoshi/WU] - fee/weight unit\n\n\n\n\nAddress table. Headers:\n\n\n\n\nSenders [string]- input hashes:\n\n\nValue [number + currency] - input value in BTC\n\n\nReceivers [string]- output hashes:\n\n\nValue [number + currency] - output value in BTC\n\n\n\n\nTransaction data:\n\n\n\n\nInput data [string] - input scripts\n\n\nOutput data [string] - output scripts\n\n\n\n\nActions:\n\n\n\n\nInfo by pointing:\n\n\nTime (UTC) - show age of transaction (current time minus mempool)\n\n\n\n\nConfirmations - first confirmation time minus mempool time\n\n\n\n\n\n\nClicking\n\n\n\n\n\n\nHash string - copy string in buffer\n\n\n\n\n\n\nLabeling\n\n\n\n\nlabeling via button \"label it\"\n\n\n\n\n2.2.2 Ethereum transaction\n\n\nDisplayed data:\n\n\nGeneral\n\n\n\n\nUTC Time [date] - time of getting transaction to mempool\n\n\nHash [string] - hash of transaction\n\n\nValue [number + currency] - total transaction value in ETH\n\n\nState [string] - \"Confirmed\", \"Mempool\", \"Finalized\"\n\n\n\n\nBlockchain specific\n\n\n\n\nBlock [number] - number of block\n\n\nNonce [number] - nonce of transaction\n\n\nSize - [number + bytes] - size of transaction in bytes\n\n\nConfirmations [number] - number of confirmations (for confirmed or finalized transactions)\n\n\n\n\nFees\n\n\n\n\nFee [number + currency] - accumulated fees in ETH\n\n\nGas price [number + wei] - price of gas\n\n\nGas used [number] - used gas\n\n\nGas limit [number] - limit of gas\n\n\n\n\nAddress table. Headers:\n\n\n\n\nSender [string]- input hashes\n\n\nReceiver [string]- output hashes:\n\n\nValue [number + currency] - tx value in ETH\n\n\n\n\nTransaction data:\n\n\n\n\nInput data [string] - hash\n\n\nLogs [string] - logs of transaction\n\n\n\n\nActions:\n\n\n\n\nInfo by pointing:\n\n\nTime - show age of transaction (current time minus mempool)\n\n\n\n\nConfirmations - first confirmation time minus mempool time\n\n\n\n\n\n\nClicking\n\n\n\n\n\n\nHash string - copy string in buffer\n\n\n\n\n\n\nLabeling\n\n\n\n\nlabeling via button \"label it\"\n\n\n\n\n2.3 Block Page\n\n\nCurrently browser shows 3 types of block pages:\n\n\n\n\nEthereum (Ethereum Classic) block page\n\n\nEthereum (Ethereum Classic) uncle block page\n\n\nBitcoin (Bitcoin Cash) block page\n\n\n\n\n2.3.1 Bitcoin block\n\n\nDisplayed data:\n\n\n\n\nBlock number [number] (header of page)\n\n\n\n\nGeneral\n\n\n\n\nUTC Time [date] - time of block generation\n\n\nHash [string] - hash of block\n\n\nSize [number + bytes] - size of block in bytes\n\n\nNonce [string] - answer to PoW\n\n\nTransactions[number] - number of transactions in block\n\n\n\n\nBlockchain specific\n\n\n\n\nMerkle root [string] - hash of merkle tree\n\n\nVersion [number] - number of block\n\n\n\n\nMining\n\n\n\n\nMiner [string] - miner hash\n\n\nDifficulty [number] - mining difficulty\n\n\n\n\nRewards\n\n\n\n\nStatic block reward [number + currency] - static reward for block mining in BTC\n\n\nFees [number + currency] - accumulated fees in BTC\n\n\nTotal blobk reward [number + currency] - sum of static reward and fees in BTC\n\n\n\n\nTransaction table. Headers:\n\n\n\n\nUTC Time [date] - time of getting into block or confirmation minus time of getting into mempool\n\n\nHash [string] - transaction hash\n\n\nSenders [number] - number in inputs\n\n\nValue [number + currency] - summ of all input values in BTC\n\n\nReceivers [number] - number in outputs\n\n\nValue [number + currency] - summ of all output values in BTC\n\n\nFee [number + currency] - fees per transaction in BTC\n\n\n\n\nActions:\n\n\n\n\nInfo by pointing:\n\n\nTime (UTC) - show age of block (current time minus block generation time)\n\n\nMiner - show label of miner\n\n\n\n\nTotal block reward - string \"Static block reward + Fees\"\n\n\n\n\n\n\nClicking\n\n\n\n\nTransaction table - expanding tx inputs and outputs by clicking on transaction row (like https://tradeblock.com/bitcoin/block/400000)\n\n\nNext \n Previous buttons (top of the page)\n\n\nHash string - copy string in buffer\n\n\n\n\n2.3.2 Ethereum block\n\n\nDisplayed data:\n\n\n\n\nEthereum block number [number] (header of page)\n\n\n\n\nGeneral\n\n\n\n\nTime [date] - time of block generation\n\n\nHash [string] - hash of block\n\n\nSize [number + bytes] - size of block in bytes\n\n\nNonce [string] - answer to PoW\n\n\nTransactions [number] - number of transactions in block\n\n\n\n\nBlockchain specific\n\n\n\n\nSha3Uncles [string] - hash of uncles\n\n\nExtra Data [string] - extra mining date\n\n\nUncles [number] - number of uncle blocks\n\n\nGas used [number] - used gas\n\n\nGas limit [number] - limit of gas\n\n\n\n\nMining\n\n\n\n\nMiner [string] - miner hash\n\n\nDifficulty [number] - mining difficulty\n\n\n\n\nRewards\n\n\n\n\nStatic block reward [number + currency] - static reward for block mining in ETH\n\n\nFees [number + currency] - accumulated fees in ETH\n\n\nUncle inclusion rewards [number + currency] - rewards in ETH for uncle inclusion\n\n\nTotal blobk reward [number + currency] - sum of static reward, fees and uncle inclusion\n\n\n\n\nTransaction table. Headers:\n\n\n\n\nUTC Time [number + seconds] - time of getting into block or confirmation minus time of getting into mempool\n\n\nHash [string] - transaction hash\n\n\nSender [hash] - hash of input\n\n\nReceiver [hash] - hash of output\n\n\nValue [number + currency] - transaction value in ETH\n\n\nFee [number + currency] - fees per transaction in ETH\n\n\n\n\nUncle table. Headers:\n\n\n\n\nHash [string] - uncle hash\n\n\nLevel [number] - uncle position\n\n\nMiner [hash] - hash of miner\n\n\nReward [number + currency] - rewards of uncle\n\n\n\n\nActions:\n\n\n\n\nInfo by pointing:\n\n\nTime (UTC) - show age of block (current time minus block generation time)\n\n\nExtra - show converted hash\n\n\nMiner - show label of miner\n\n\nGas used - 100% * (gas used / gas limit )\n\n\n\n\nTotal block reward - string \"Static block reward + Uncle block reward + Fees\"\n\n\n\n\n\n\nClicking\n\n\n\n\nNext \n Previous buttons (top of the page)\n\n\nHash string - copy string in buffer\n\n\n\n\n2.3.3 Ethereum uncle block\n\n\nDisplayed data:\n\n\n\n\nEthereum uncle block number [number] (header of page)\n\n\nTime [date] - time of block generation\n\n\nHash [string] - hash of uncle block\n\n\nParent block [number] - number of parent block\n\n\nParent hash [string] - hash of parent block\n\n\n\n\nLevel [number] - uncle position\n\n\n\n\n\n\nMiner [hash] - hash of miner\n\n\n\n\n\n\nUncle inclusion rewards [number + currency] - rewards in ETH for uncle inclusion\n\n\n\n\n\n\nActions:\n\n\n\n\nInfo by pointing:\n\n\nTime - show age of block (current time minus block generation time)\n\n\n\n\nMiner - show label of miner\n\n\n\n\n\n\nClicking\n\n\n\n\nNext \n Previous buttons (top of the page)\n\n\nHash string - copy string in buffer\n\n\n\n\n3. Blockchain Monitor\n\n\n4. Token Monitor\n\n\nToken Table:\n\n\nHeaders:\n\n\n\n\nName [string] - token name\n\n\nMarket cap [number + currency] - market cap of token (price * supply)\n\n\nPrice [number + currency] - weightrd price of token\n\n\nVolume (24h) [number + currency] - 24h volume of token trades\n\n\nSupply [number + token ticker] - number of tokens\n\n\nChange (24h) [number + percent] - 24h chdnge of price\n\n\nPrice graph (7d) [img] - weekly price change\n\n\n\n\n5. Exchange Monitor\n\n\n6. Chaingear\n\n\nPurpose:\n provide easy integration with Chaingear.\n\nDesign \n UI  features:\n simple UI, autoupdate register data, preview of changes.\n\n\nAll functionality is available after Metamask authorization.\n\n\nMain functions of the page:\n\n\n\n\nWatch and label created registers.\n\n\nCreate register\n\n\nEdit register (entry)\n\n\nDelete register\n\n\nTransfer the rights of usage to another account\n\n\nUpload content to register via IPFS\n\n\nJSON import of custom fields\n\n\nReal time calculation of registry creation costs\n\n\nData import from smart contract\n\n\n\n\n8. Labels\n\n\n9. Portfolio", 
            "title": "Requirements"
        }, 
        {
            "location": "/cyb/requirements/#project-description", 
            "text": "Cyber browser - an entrance point to start working with blockchains.", 
            "title": "Project description"
        }, 
        {
            "location": "/cyb/requirements/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/cyb/requirements/#1-common-functional-requirements", 
            "text": "Integration with Metamask  IPFS integration  Prefered type of data structure - tables", 
            "title": "1. Common functional requirements"
        }, 
        {
            "location": "/cyb/requirements/#2-common-non-functional-requirements", 
            "text": "Simple and attractive design  Intuitive UI  Adaptive design for mobile devices  Display IPFS hash for all objects  All hashes should be easy to copy", 
            "title": "2. Common non-functional requirements"
        }, 
        {
            "location": "/cyb/requirements/#3-system-requirements", 
            "text": "less than 1 second for loading page  less than 3 seconds for loading all data  Web version of browser (React)  Desktop version (Electron + React)  Mobile web version", 
            "title": "3. System requirements"
        }, 
        {
            "location": "/cyb/requirements/#browser-structure", 
            "text": "", 
            "title": "Browser structure:"
        }, 
        {
            "location": "/cyb/requirements/#0-shell", 
            "text": "Search bar, App bar, State bar, Context bar.", 
            "title": "0 Shell."
        }, 
        {
            "location": "/cyb/requirements/#01-search-path-bar", 
            "text": "Top of the page   search in existing list of apps  search in blockchain explorer app  search in token monitor app  search path", 
            "title": "0.1 Search (Path) bar"
        }, 
        {
            "location": "/cyb/requirements/#02-app-bar", 
            "text": "Left menu   list of installed apps (4 blockchain explorers, chaingear, token monitor)  add/delete app", 
            "title": "0.2 App bar:"
        }, 
        {
            "location": "/cyb/requirements/#03-state-bar", 
            "text": "desync state  metamask instelled state (Y/N)  tx processing state", 
            "title": "0.3 State bar:"
        }, 
        {
            "location": "/cyb/requirements/#04-context-bar", 
            "text": "account logo  account address  account balance   settings:   language  location  currency (USD/ETH/BTC)  activity type (private/anonymous/public)   USE CASES:   no metamask - install metamask  non auth user- log in metamask  auth user - account logo", 
            "title": "0.4 Context bar:"
        }, 
        {
            "location": "/cyb/requirements/#1-main-page", 
            "text": "Purpose:  accent the user's attention on all browser apps.   Design   UI features:  simple and attractive design, hints to start usage of cyber products.  There is a status text below search panel which describes technical information about cyber.Search products:  Search in 134 M transactions in 2 blockchain systems with 135 parsed tokens. Database size : 369 GBs    Where:   Transactions [number] - number of all indexed transactions from all blockchains connected to Cybernode.   Blockchain systems [number] - all blockchains processed by Cybernode.  Tokens [numger] - all unique tokens from all blockchains indexed.   Database size [number + Gb] - size of Cassandra (index) database.   There are 3 main widgets below the status string describing the cryptoeconomy, registers and portfolio. Their apperance depends on user type:    User without Metamask    Total market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD.   Call to action:  install Metamask and transit to tokens page .    Chaingear registers [number] - number of created registers in Chaingear.  Call to action:  install Metamask and transit to Chaingear page .    Apps [number] - number of browser apps.  Call to action:  deploy app .    User with Metamask (no activities in system)    Total market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD.   Call to action:  transit to tokens page .    Chaingear registers [number] - number of created registers in Chaingear.  Call to action:  transit to Chaingear page, hint to create register or record .    Apps [number] - number of browser apps.  Call to action:  deploy app .    User with Metamask (no activities in system)    Total market cap [billions of USD] - is a summ of all token capitalizations, valuated in USD.   Call to action:  transit to tokens page .    Chaingear registers [number] - number of created registers/records in Chaingear.  Call to action:  Chaingear page .    Apps [number] - number of browser apps.  Call to action:  deploy app .    At the bottom of the page 5 project links are placed:\n1. GitHub - GitHub repository of cyber.Congress [https://github.com/cybercongress]\n2. Roadmap - roadmap for cyber.Search project [https://github.com/orgs/cybercongress/projects/1]\n3. Cybernode - Cybernode stats page [cybersearch.live]\n4. Dashboard - a link user's custom dashboard []\n5. Knowledge - a link to knowledge database of cyber.Search project [cybersearch.io]", 
            "title": "1. Main Page"
        }, 
        {
            "location": "/cyb/requirements/#2-app-list", 
            "text": "Purpose:  accent the user's attention on all browser apps.   Design   UI features:  simple and attractive design, hints to start usage of apps.  There is list of browser apps with categories (with brief overview):    Blockchain explorers - Search in 135 million transactions in 4 blockchains    Ethereum Explorer   Ethereum Classic explorer  Bitcoin Explorer   Bitcoin Cash explorer    Cybernomics - Watch and anayze 135 parsed tokens in 7 exchanges     Token Monitor   Exchange Monitor   Blockchain Monitor    Registers - Store your records in blockchains    Chaingear    Accounting - Manage your cryptoassets    Portfolio    There are 2 versions of apps:   stable released apps  developing apps   Proposals:   donation button for developing apps  install to App bar button (in app is not installed)", 
            "title": "2. App list"
        }, 
        {
            "location": "/cyb/requirements/#applications", 
            "text": "Blockchain explorer    Bitcoin   Bitcoin Cash  Ethereum  Ethereum Classic", 
            "title": "Applications"
        }, 
        {
            "location": "/cyb/requirements/#2-search-results-page", 
            "text": "Purpose:  provide easy and quick functionality for working with blockchain search.  Design   UI  features:  strictly logical UI, adaptive preferenses of filtration and sorting.", 
            "title": "2. Search Results Page"
        }, 
        {
            "location": "/cyb/requirements/#objects-of-search", 
            "text": "There are 2 types of search provided by browser:\n1. Global search (searching in whole ecosystem of indexed objects)\n2. Context search (searching the data in certain pages)  There are 4 systems (blockchains) in which you can find data:   Bitcoin  Bitcoin Cash  Ethereum  Ethereum Classic   There are 4 types of objects that can be foung in listed systems:   Contracts  Transactions  Blocks  Uncle blocks   Objects can be found by entering next types of queries:   Full hash (address, block, uncle, transaction)  Number (block/uncle)   The search pannel in general should include next functions:\n1. Global and local search (GitHub style)\n2. Autocomplete function", 
            "title": "Objects of search."
        }, 
        {
            "location": "/cyb/requirements/#search-results", 
            "text": "Left menu includes next hardcoded functions:   Display listed systems (blockchains)  Display listed objects   Search results in general are shown as a list of object preview. Each object preview its own structure:    Transaction:    Time (finalization, or time of confirmation or \"Mempool\" status)   Hash   Value    Block + number    Time of creation   Hash   Number of transactions    Uncle + number    Time  of creation   Hash   Uncle position    Contract    Time of creation   Hash  Value   Each preview has clickable hash string, that leads to block/uncle, contract or transaction page.  There is a infinite page scroll function on results page.", 
            "title": "Search results."
        }, 
        {
            "location": "/cyb/requirements/#21-contract-page", 
            "text": "Currently browser shows 2 types of contract pages:   Ethereum (Ethereum Classic) contract page  Bitcoin (Bitcoin Cash) contract page", 
            "title": "2.1 Contract Page"
        }, 
        {
            "location": "/cyb/requirements/#211-bitcoin-contract", 
            "text": "Displayed data:\n- Robohash logo\n- QR code of address hash  General   UTC Time [date] - time of contract getting into blockchain  Balance [number + currency] - BTC available to withdraw  Hash [string] - hash of address   Cashflow   Transactions [number] - number of transactions in contract  Unconfirmed transactions [number] - number of transactions in mempool  Accumulated income [number + currency] - received BTC  Pending income [number + currency] - BTC in mempool transactions   Charts:   Valuation tab (Regular graph, all above zero):   Valuation / Time - balance of contract on each period of time    Transactions tab (incoming tx - above zero, outcoming - below):   Transactions / Time - activity of transactions by contract on each period of time   Transactions and blocks;    Transactions tab:    UTC Time [date] - time of getting transaction to mempool   Hash [string] - hash of transaction  Block [number] - number of block  Sender[number]- number of inputs  Value[number + currency] - total input balance in BTC  Receiver [number]- number of outputs  Value [number + currency] - total output balance in BTC  Fee [number + currency] - accumulated fees in BTC   State [string] - \"Confirmed\", \"Mempool\", \"Finalized\"    Mined blocks (for miner address only):   UTC Time [date] - time of block generation  Block [number] - number of mined block  Transactions [number] - number of transactions in mined block  Reward [number + currency] - rewards for block in BTC   Code:\n- bitcoin scripts  Actions:   Info by pointing:  Time (UTC) - show age of transaction (current time minus mempool)   Address - show label of address    Clicking   Hash string - copy string in buffer   Transaction row - expand transaction details (inputs and outputs)    Labeling   labeling via button \"label it\"", 
            "title": "2.1.1 Bitcoin contract"
        }, 
        {
            "location": "/cyb/requirements/#212-ethereum-contract", 
            "text": "Displayed data:\n- Robohash logo\n- QR code of address hash  General:   UTC Time [date] - time of contract getting into blockchain  Balance [number + currency] - ETH available to withdraw  Hash [string] - hash of address   Cashflow:   Transactions [number] - number of transactions in contract  Unconfirmed transactions [number] - number of transactions in mempool  Accumulated income [number + currency] - received ETH  Pending income [number + currency] - ETH in mempool transactions   Charts:   Valuation tab (Regular graph, all above zero):   Valuation / Time - balance of contract on each period of time    Transactions tab (incoming tx - above zero, outcoming - below):   Transactions / Time - activity of transactions by contract on each period of time   Transactions and blocks;   Transactions tab:  UTC Time [date] - time of getting transaction to mempool  Hash [string] - hash of transaction  Block [number] - number of block  Sender[hash]- hash of \"from\" address  Receiver [hash]- hash of \"to\" address  Value [number + currency] - tx value in ETH  Fee [number + currency] - accumulated fees in ETH   State [string] - \"Confirmed\", \"Mempool\", \"Finalized\"    Operations tab (if available):   UTC Time [date] - time of getting transaction to mempool  Type [string] - type of internal tx (call, delegate call, destroy, create)  Sender[hash]- hash of \"from\" address  Receiver [hash]- hash of \"to\" address  Value [number + currency] - total input balance in ETH  Gas used [number] - gas used  Gas limit [number] - gas limit   State [string] - \"Failed\", \"Reverted\", \"Successful\"    Tokens tab:   Token [string] - token name  Hash [string] - transaction hash  Sender [number + currency] - sent tokens  Receiver [number + currency] - received tokens   Value [number + currency] - sent minus received tokens    Mined blocks tab (for miner address only):   UTC Time [date] - time of block generation  Block [number] - number of mined block  Transactions [number] - number of transactions in mined block   Reward [number + currency] - rewards for block in ETH    Mined uncles tab (if available):    UTC Time [date] - time of block generation   Hash [string] - uncle hash  Block [number] - number of block with uncle   Uncle [number] - number of mined uncle    Reward [number + currency] - rewards for uncle in ETH    Code:  Contract code:\n- Contract name [string] - name of contract\n- Compiler version [string] - version of compiler\nSource code - code of contract\nABI - contract ABI\nSwarm code - link in ethereum swarm  Actions:   Info by pointing:  Time  - show age of transaction (current time minus mempool)   Address - show label of address    Clicking    Hash string - copy string in buffer    Labeling   labeling via button \"label it\"", 
            "title": "2.1.2 Ethereum contract"
        }, 
        {
            "location": "/cyb/requirements/#22-transaction-page", 
            "text": "Currently browser shows 2 types of transaction pages:   Ethereum (Ethereum Classic) transaction page  Bitcoin (Bitcoin Cash) transaction page", 
            "title": "2.2 Transaction Page"
        }, 
        {
            "location": "/cyb/requirements/#221-bitcoin-transaction", 
            "text": "Displayed data:  General   UTC Time [date] - time of getting transaction to mempool  Hash [string] - hash of transaction  Value [number + currency] - total transaction value in BTC  State [string] - \"Confirmed\", \"Mempool\", \"Finalized\"   Blockchain specific   Block [number] - number of block  Size - [number + bytes] - size of transaction in bytes  Confirmations [number] - number of confirmations (for confirmed or finalized transactions)  Inputs [number] - number of input addresses  Outputs [number] - number of output addresses   Fees   Fee [number + currency] - accumulated fees in BTC  Fee per byte [number + satoshi/Byte] - fee/size  Fee per weight unit [number + satoshi/WU] - fee/weight unit   Address table. Headers:   Senders [string]- input hashes:  Value [number + currency] - input value in BTC  Receivers [string]- output hashes:  Value [number + currency] - output value in BTC   Transaction data:   Input data [string] - input scripts  Output data [string] - output scripts   Actions:   Info by pointing:  Time (UTC) - show age of transaction (current time minus mempool)   Confirmations - first confirmation time minus mempool time    Clicking    Hash string - copy string in buffer    Labeling   labeling via button \"label it\"", 
            "title": "2.2.1 Bitcoin transaction"
        }, 
        {
            "location": "/cyb/requirements/#222-ethereum-transaction", 
            "text": "Displayed data:  General   UTC Time [date] - time of getting transaction to mempool  Hash [string] - hash of transaction  Value [number + currency] - total transaction value in ETH  State [string] - \"Confirmed\", \"Mempool\", \"Finalized\"   Blockchain specific   Block [number] - number of block  Nonce [number] - nonce of transaction  Size - [number + bytes] - size of transaction in bytes  Confirmations [number] - number of confirmations (for confirmed or finalized transactions)   Fees   Fee [number + currency] - accumulated fees in ETH  Gas price [number + wei] - price of gas  Gas used [number] - used gas  Gas limit [number] - limit of gas   Address table. Headers:   Sender [string]- input hashes  Receiver [string]- output hashes:  Value [number + currency] - tx value in ETH   Transaction data:   Input data [string] - hash  Logs [string] - logs of transaction   Actions:   Info by pointing:  Time - show age of transaction (current time minus mempool)   Confirmations - first confirmation time minus mempool time    Clicking    Hash string - copy string in buffer    Labeling   labeling via button \"label it\"", 
            "title": "2.2.2 Ethereum transaction"
        }, 
        {
            "location": "/cyb/requirements/#23-block-page", 
            "text": "Currently browser shows 3 types of block pages:   Ethereum (Ethereum Classic) block page  Ethereum (Ethereum Classic) uncle block page  Bitcoin (Bitcoin Cash) block page", 
            "title": "2.3 Block Page"
        }, 
        {
            "location": "/cyb/requirements/#231-bitcoin-block", 
            "text": "Displayed data:   Block number [number] (header of page)   General   UTC Time [date] - time of block generation  Hash [string] - hash of block  Size [number + bytes] - size of block in bytes  Nonce [string] - answer to PoW  Transactions[number] - number of transactions in block   Blockchain specific   Merkle root [string] - hash of merkle tree  Version [number] - number of block   Mining   Miner [string] - miner hash  Difficulty [number] - mining difficulty   Rewards   Static block reward [number + currency] - static reward for block mining in BTC  Fees [number + currency] - accumulated fees in BTC  Total blobk reward [number + currency] - sum of static reward and fees in BTC   Transaction table. Headers:   UTC Time [date] - time of getting into block or confirmation minus time of getting into mempool  Hash [string] - transaction hash  Senders [number] - number in inputs  Value [number + currency] - summ of all input values in BTC  Receivers [number] - number in outputs  Value [number + currency] - summ of all output values in BTC  Fee [number + currency] - fees per transaction in BTC   Actions:   Info by pointing:  Time (UTC) - show age of block (current time minus block generation time)  Miner - show label of miner   Total block reward - string \"Static block reward + Fees\"    Clicking   Transaction table - expanding tx inputs and outputs by clicking on transaction row (like https://tradeblock.com/bitcoin/block/400000)  Next   Previous buttons (top of the page)  Hash string - copy string in buffer", 
            "title": "2.3.1 Bitcoin block"
        }, 
        {
            "location": "/cyb/requirements/#232-ethereum-block", 
            "text": "Displayed data:   Ethereum block number [number] (header of page)   General   Time [date] - time of block generation  Hash [string] - hash of block  Size [number + bytes] - size of block in bytes  Nonce [string] - answer to PoW  Transactions [number] - number of transactions in block   Blockchain specific   Sha3Uncles [string] - hash of uncles  Extra Data [string] - extra mining date  Uncles [number] - number of uncle blocks  Gas used [number] - used gas  Gas limit [number] - limit of gas   Mining   Miner [string] - miner hash  Difficulty [number] - mining difficulty   Rewards   Static block reward [number + currency] - static reward for block mining in ETH  Fees [number + currency] - accumulated fees in ETH  Uncle inclusion rewards [number + currency] - rewards in ETH for uncle inclusion  Total blobk reward [number + currency] - sum of static reward, fees and uncle inclusion   Transaction table. Headers:   UTC Time [number + seconds] - time of getting into block or confirmation minus time of getting into mempool  Hash [string] - transaction hash  Sender [hash] - hash of input  Receiver [hash] - hash of output  Value [number + currency] - transaction value in ETH  Fee [number + currency] - fees per transaction in ETH   Uncle table. Headers:   Hash [string] - uncle hash  Level [number] - uncle position  Miner [hash] - hash of miner  Reward [number + currency] - rewards of uncle   Actions:   Info by pointing:  Time (UTC) - show age of block (current time minus block generation time)  Extra - show converted hash  Miner - show label of miner  Gas used - 100% * (gas used / gas limit )   Total block reward - string \"Static block reward + Uncle block reward + Fees\"    Clicking   Next   Previous buttons (top of the page)  Hash string - copy string in buffer", 
            "title": "2.3.2 Ethereum block"
        }, 
        {
            "location": "/cyb/requirements/#233-ethereum-uncle-block", 
            "text": "Displayed data:   Ethereum uncle block number [number] (header of page)  Time [date] - time of block generation  Hash [string] - hash of uncle block  Parent block [number] - number of parent block  Parent hash [string] - hash of parent block   Level [number] - uncle position    Miner [hash] - hash of miner    Uncle inclusion rewards [number + currency] - rewards in ETH for uncle inclusion    Actions:   Info by pointing:  Time - show age of block (current time minus block generation time)   Miner - show label of miner    Clicking   Next   Previous buttons (top of the page)  Hash string - copy string in buffer", 
            "title": "2.3.3 Ethereum uncle block"
        }, 
        {
            "location": "/cyb/requirements/#3-blockchain-monitor", 
            "text": "", 
            "title": "3. Blockchain Monitor"
        }, 
        {
            "location": "/cyb/requirements/#4-token-monitor", 
            "text": "Token Table:  Headers:   Name [string] - token name  Market cap [number + currency] - market cap of token (price * supply)  Price [number + currency] - weightrd price of token  Volume (24h) [number + currency] - 24h volume of token trades  Supply [number + token ticker] - number of tokens  Change (24h) [number + percent] - 24h chdnge of price  Price graph (7d) [img] - weekly price change", 
            "title": "4. Token Monitor"
        }, 
        {
            "location": "/cyb/requirements/#5-exchange-monitor", 
            "text": "", 
            "title": "5. Exchange Monitor"
        }, 
        {
            "location": "/cyb/requirements/#6-chaingear", 
            "text": "Purpose:  provide easy integration with Chaingear. Design   UI  features:  simple UI, autoupdate register data, preview of changes.  All functionality is available after Metamask authorization.  Main functions of the page:   Watch and label created registers.  Create register  Edit register (entry)  Delete register  Transfer the rights of usage to another account  Upload content to register via IPFS  JSON import of custom fields  Real time calculation of registry creation costs  Data import from smart contract", 
            "title": "6. Chaingear"
        }, 
        {
            "location": "/cyb/requirements/#8-labels", 
            "text": "", 
            "title": "8. Labels"
        }, 
        {
            "location": "/cyb/requirements/#9-portfolio", 
            "text": "", 
            "title": "9. Portfolio"
        }, 
        {
            "location": "/cyb/staging/", 
            "text": "Staging server\n\n\nTo avoid conflicts with other services on the same ports, we run our services on the following ports:\n\n\ncyber-ui :32500\nchaingear-api :32600\ncyber-search-api :32700\ncyber-markets-api :32800\n\n\n\n\n\nStaging runs 4 \ncomponents\n:\n\n\n\n\ncyber-ui\n\n\ncyber-search\n\n\ncyber-markets\n\n\nchaingear\n\n\n\n\nEach component is composed from different \ncontainers\n. Some containers are shared to save resources.\n\n\nServer setup\n\n\nThis is only needed to be done once.\n\n\n\n\n\n\nSSH to staging server. Make sure you're in \ndocker\n group:\n\n\n$ groups\n   anatoli docker wheel cyber\n\n\n\n\n\n\nCheckout \ncyber-ui\n repo with staging setup:\n\n\n$ git clone https://github.com/cyberFund/cyber-ui\n\n\n\n\n\n\nUpdate running containers\n\n\nUpdate running containers:\n\n\n   $ cd cyber-ui \n git pull\n   $ ./devops/staging/up.sh\n\n\n\n\n\nThis uses \ndocker-compose\n to start containers from DockerHub. It doesn't rebuild them.\n\n\nRebuilding\n\n\nTODO: Move stuff out of CircleCI configs into build scripts + conventions.", 
            "title": "Staging"
        }, 
        {
            "location": "/cyb/staging/#staging-server", 
            "text": "To avoid conflicts with other services on the same ports, we run our services on the following ports:  cyber-ui :32500\nchaingear-api :32600\ncyber-search-api :32700\ncyber-markets-api :32800  Staging runs 4  components :   cyber-ui  cyber-search  cyber-markets  chaingear   Each component is composed from different  containers . Some containers are shared to save resources.", 
            "title": "Staging server"
        }, 
        {
            "location": "/cyb/staging/#server-setup", 
            "text": "This is only needed to be done once.    SSH to staging server. Make sure you're in  docker  group:  $ groups\n   anatoli docker wheel cyber    Checkout  cyber-ui  repo with staging setup:  $ git clone https://github.com/cyberFund/cyber-ui", 
            "title": "Server setup"
        }, 
        {
            "location": "/cyb/staging/#update-running-containers", 
            "text": "Update running containers:     $ cd cyber-ui   git pull\n   $ ./devops/staging/up.sh  This uses  docker-compose  to start containers from DockerHub. It doesn't rebuild them.", 
            "title": "Update running containers"
        }, 
        {
            "location": "/cyb/staging/#rebuilding", 
            "text": "TODO: Move stuff out of CircleCI configs into build scripts + conventions.", 
            "title": "Rebuilding"
        }, 
        {
            "location": "/cyb/whitepaper/", 
            "text": "cyb\n\n\n\n\n\nFront door to blockchain universe\n\n\n\n\nOverview\n\n\nCyb is a blockchain browser with integrated DApp platform. Browser consists of two main parts:\n\n\n\n\n\n\nShell, which alows:\n\n\n\n\n\n\nsearch in blockchains and Dapps, and IPFS content\n\n\n\n\ndeploy and manage Dapps for users\n\n\nshow desync state of user and blockchain\n\n\n\n\nmanage user's account data (sign transactions, create custom feed)\n\n\n\n\n\n\nDApps, which:\n\n\n\n\n\n\nuse cyber.Search services for data obtaining (cyber.Search, cyber.Markets, cybernode)\n\n\n\n\nare stored in IPFS\n\n\ncan interract with user and each other\n\n\n\n\nPrinciples\n\n\nThere are sereral key principles of cyb behavior, design and development.\n\n\n1. Users\n\n\nWe focus on developers and advanced blockchain users (Metamask users). But cyb is also friendly for everyone who wants to interract with blockchains and decentralized systems.\n\n\n1.1. User activity types\n\n\nWe respect user's attitude and principles. So we offer 3 clear custom types of user behavior:\n\n\n\n\npublic activity [address + events tracking]\n\n\nprivate activity [abstract ID + events tracking ]\n\n\nanonymous activity [no tracking at all]\n\n\n\n\nFor successful browser development we need to collect at least public and private data. Thus users of these 2 groups well be incentivized by giving nice perks.\n\n\n1.2 User activity incentivization\n\n\nWe use full tokenization of processes (data obtain and transfer, content generation and adding).\n\n\n1.3 Contribution\n\n\n1.4 Donation mechanism\n\n\n1.5 Feeedback and bug collection\n\n\nWe use user's feedback to make products better. So we provide options for bug reporting and feedback leaving on every page.\n\n\n2. Design\n\n\nCyb design process follows Web3 design principles\n\n\n2.1. State rules\n\n\nWe use simple colored states for states of transactions or operations:\n\n\n\n\nmempool/failed - red\n\n\nconfirmed/reverted - yellow\n\n\nfinalized/succesful - green\n\n\n\n\n2.2 Data visualisation\n\n\nData should be visualised in a simple and attractive way. No overloaded plots an diagrams, we use animation instead.\n\n\n2.3 Blockchain objects presenting\n\n\nBlockchain data is too complicated and sometimes not obvious for people. Thus we use adaptive tricks to make work process more convenient:\n\n\n\n\nLogical grouping for objects. Every app page has common groups of data (general, blockchain specific) for inheritance of views and better navigation or data observing.\n\n\nClassical accounting terms used for balance and cashflow operations. Blockahains use econimic principles for interaction between subjects thus we can describe such processes in established terms.\n\n\nRobohash logo for contracts entities. Contracts can act by themself, have and algorithms, so it's more natural to perceive them like robots instead of pieces of code.\n\n\n\n\n3. Development\n\n\n3.1 Shell development\n\n\n3.2 DApps development\n\n\n3.3 DApps deployment\n\n\n3.4 DApps interaction", 
            "title": "Whitepaper"
        }, 
        {
            "location": "/cyb/whitepaper/#overview", 
            "text": "Cyb is a blockchain browser with integrated DApp platform. Browser consists of two main parts:    Shell, which alows:    search in blockchains and Dapps, and IPFS content   deploy and manage Dapps for users  show desync state of user and blockchain   manage user's account data (sign transactions, create custom feed)    DApps, which:    use cyber.Search services for data obtaining (cyber.Search, cyber.Markets, cybernode)   are stored in IPFS  can interract with user and each other", 
            "title": "Overview"
        }, 
        {
            "location": "/cyb/whitepaper/#principles", 
            "text": "There are sereral key principles of cyb behavior, design and development.", 
            "title": "Principles"
        }, 
        {
            "location": "/cyb/whitepaper/#1-users", 
            "text": "We focus on developers and advanced blockchain users (Metamask users). But cyb is also friendly for everyone who wants to interract with blockchains and decentralized systems.", 
            "title": "1. Users"
        }, 
        {
            "location": "/cyb/whitepaper/#11-user-activity-types", 
            "text": "We respect user's attitude and principles. So we offer 3 clear custom types of user behavior:   public activity [address + events tracking]  private activity [abstract ID + events tracking ]  anonymous activity [no tracking at all]   For successful browser development we need to collect at least public and private data. Thus users of these 2 groups well be incentivized by giving nice perks.", 
            "title": "1.1. User activity types"
        }, 
        {
            "location": "/cyb/whitepaper/#12-user-activity-incentivization", 
            "text": "We use full tokenization of processes (data obtain and transfer, content generation and adding).", 
            "title": "1.2 User activity incentivization"
        }, 
        {
            "location": "/cyb/whitepaper/#13-contribution", 
            "text": "", 
            "title": "1.3 Contribution"
        }, 
        {
            "location": "/cyb/whitepaper/#14-donation-mechanism", 
            "text": "", 
            "title": "1.4 Donation mechanism"
        }, 
        {
            "location": "/cyb/whitepaper/#15-feeedback-and-bug-collection", 
            "text": "We use user's feedback to make products better. So we provide options for bug reporting and feedback leaving on every page.", 
            "title": "1.5 Feeedback and bug collection"
        }, 
        {
            "location": "/cyb/whitepaper/#2-design", 
            "text": "Cyb design process follows Web3 design principles", 
            "title": "2. Design"
        }, 
        {
            "location": "/cyb/whitepaper/#21-state-rules", 
            "text": "We use simple colored states for states of transactions or operations:   mempool/failed - red  confirmed/reverted - yellow  finalized/succesful - green", 
            "title": "2.1. State rules"
        }, 
        {
            "location": "/cyb/whitepaper/#22-data-visualisation", 
            "text": "Data should be visualised in a simple and attractive way. No overloaded plots an diagrams, we use animation instead.", 
            "title": "2.2 Data visualisation"
        }, 
        {
            "location": "/cyb/whitepaper/#23-blockchain-objects-presenting", 
            "text": "Blockchain data is too complicated and sometimes not obvious for people. Thus we use adaptive tricks to make work process more convenient:   Logical grouping for objects. Every app page has common groups of data (general, blockchain specific) for inheritance of views and better navigation or data observing.  Classical accounting terms used for balance and cashflow operations. Blockahains use econimic principles for interaction between subjects thus we can describe such processes in established terms.  Robohash logo for contracts entities. Contracts can act by themself, have and algorithms, so it's more natural to perceive them like robots instead of pieces of code.", 
            "title": "2.3 Blockchain objects presenting"
        }, 
        {
            "location": "/cyb/whitepaper/#3-development", 
            "text": "", 
            "title": "3. Development"
        }, 
        {
            "location": "/cyb/whitepaper/#31-shell-development", 
            "text": "", 
            "title": "3.1 Shell development"
        }, 
        {
            "location": "/cyb/whitepaper/#32-dapps-development", 
            "text": "", 
            "title": "3.2 DApps development"
        }, 
        {
            "location": "/cyb/whitepaper/#33-dapps-deployment", 
            "text": "", 
            "title": "3.3 DApps deployment"
        }, 
        {
            "location": "/cyb/whitepaper/#34-dapps-interaction", 
            "text": "", 
            "title": "3.4 DApps interaction"
        }, 
        {
            "location": "/cyb-js/build/", 
            "text": "How to write a NPM module using TypeScript\n\n\nimport {DefaultSearchApi, SearchApi} from \ncyber-search-js\n;\n\nconst searchApi: SearchApi = new DefaultSearchApi(\nhttp://api.search.cyber.fund\n);\n\n\n\n\n\nHow to run\n\n\nnpm run build\n\n\n\n\n\npublish\n\n\nnpm publish", 
            "title": "How to write a NPM module using TypeScript"
        }, 
        {
            "location": "/cyb-js/build/#how-to-write-a-npm-module-using-typescript", 
            "text": "import {DefaultSearchApi, SearchApi} from  cyber-search-js ;\n\nconst searchApi: SearchApi = new DefaultSearchApi( http://api.search.cyber.fund );", 
            "title": "How to write a NPM module using TypeScript"
        }, 
        {
            "location": "/cyb-js/build/#how-to-run", 
            "text": "npm run build", 
            "title": "How to run"
        }, 
        {
            "location": "/cyb-js/build/#publish", 
            "text": "npm publish", 
            "title": "publish"
        }, 
        {
            "location": "/cyb-js/overwiev/", 
            "text": "mission \nsimplify development of distribution and decentralized application for javascript development. Fast cheap and scalable.\n\n\nIntegration between blockchain and client side.\ncli utils?\n\n\nreadmap\n    cyber seach integration\n    payment chanel integration\n        bitcoin\n        ethereum\n    buling with cybernode\n    light clents\n\n\ncyber.js is sample wrapper around cyber infrastructure - search, markets and changing also it provide method for work with ipfs and ethereum smart contracts.", 
            "title": "Overwiev"
        }, 
        {
            "location": "/cyber-markets/README/", 
            "text": "Build and Run locally\n\n\ndocker build -t build/raw-api -f ./docs/Dockerfile ./ \n docker run -p \n8080\n:8080 build/raw-api", 
            "title": "README"
        }, 
        {
            "location": "/cyber-markets/README/#build-and-run-locally", 
            "text": "docker build -t build/raw-api -f ./docs/Dockerfile ./   docker run -p  8080 :8080 build/raw-api", 
            "title": "Build and Run locally"
        }, 
        {
            "location": "/cyber-markets/about-markets/", 
            "text": "About Markets", 
            "title": "About Markets"
        }, 
        {
            "location": "/cyber-markets/about-markets/#about-markets", 
            "text": "", 
            "title": "About Markets"
        }, 
        {
            "location": "/cyber-markets/api/README/", 
            "text": "Build and Run locally\n\n\ndocker build -t build/raw-api -f ./docs/Dockerfile ./ \n docker run -p \n8080\n:8080 build/raw-api", 
            "title": "README"
        }, 
        {
            "location": "/cyber-markets/api/README/#build-and-run-locally", 
            "text": "docker build -t build/raw-api -f ./docs/Dockerfile ./   docker run -p  8080 :8080 build/raw-api", 
            "title": "Build and Run locally"
        }, 
        {
            "location": "/cyber-markets/components/overview/", 
            "text": "Markets Components\n\n\n\n\n\n\n\n\nComponent\n\n\nScale\n\n\nDescription\n\n\nCluster Address\n\n\nMetrics\n\n\nExternal\n\n\n\n\n\n\n\n\n\n\nKafka\n\n\n1\n\n\nData Entry\n\n\n\n\n\n\n\n\n\n\n\n\nKafka Manager\n\n\n1\n\n\nKafka Explorer\n\n\n\n\n\n\n\n\n\n\n\n\nElassandra\n\n\n1 - N\n\n\nData Back\n\n\n\n\n\n\n\n\n\n\n\n\nExchanges Connector\n\n\n1 - N\n\n\nConnect to CEXs/DEXs for raw data\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\nTickers\n\n\n1 - N\n\n\nCalculate Tickers From Raw Data\n\n\n\n\n\n\n\n\n\n\n\n\nStorer\n\n\n1\n\n\nWrite data to database\n\n\n\n\n\n\n\n\n\n\n\n\nMarkets REST API\n\n\n1 - N\n\n\nRest Api To Markets Entities\n\n\n\n\n\n\ny\n\n\n\n\n\n\nMarkets Stream API\n\n\n1 - N\n\n\nWeb Socket Api To Markets Entities\n\n\n\n\n\n\ny\n\n\n\n\n\n\nMarkets Api Docs\n\n\n1 - N\n\n\nMarkets Api Docs Based On Swagger\n\n\n\n\n\n\ny\n\n\n\n\n\n\n\n\n\n\nKafka\n\n\nKafka\n is message queue used as main data entry. All raw data firstly goes into kafka, than \n various services consume it to provide new functionality.\n\n\nElassandra (Elastic + Cassandra)\n\n\nElassandra\n is used as main backend storage with linear growth fulltext-search\n index on source cassandra data.\n\n\nExchanges Connector\n\n\nCollect raw data from centralized and decentralized exchanges such as trades, order books and put it to kafka.\n\n\nStorer\n\n\nWrites data from kafka topics or directly from exchanges-connector to cassandra cluster\n\n\nTickers\n\n\nCalculate tickers, prices, etc and aggregate raw data which collecting exchanges connectors module", 
            "title": "Markets Components"
        }, 
        {
            "location": "/cyber-markets/components/overview/#markets-components", 
            "text": "Component  Scale  Description  Cluster Address  Metrics  External      Kafka  1  Data Entry       Kafka Manager  1  Kafka Explorer       Elassandra  1 - N  Data Back       Exchanges Connector  1 - N  Connect to CEXs/DEXs for raw data   8080:/actuator/metrics     Tickers  1 - N  Calculate Tickers From Raw Data       Storer  1  Write data to database       Markets REST API  1 - N  Rest Api To Markets Entities    y    Markets Stream API  1 - N  Web Socket Api To Markets Entities    y    Markets Api Docs  1 - N  Markets Api Docs Based On Swagger    y", 
            "title": "Markets Components"
        }, 
        {
            "location": "/cyber-markets/components/overview/#kafka", 
            "text": "Kafka  is message queue used as main data entry. All raw data firstly goes into kafka, than \n various services consume it to provide new functionality.", 
            "title": "Kafka"
        }, 
        {
            "location": "/cyber-markets/components/overview/#elassandra-elastic-cassandra", 
            "text": "Elassandra  is used as main backend storage with linear growth fulltext-search\n index on source cassandra data.", 
            "title": "Elassandra (Elastic + Cassandra)"
        }, 
        {
            "location": "/cyber-markets/components/overview/#exchanges-connector", 
            "text": "Collect raw data from centralized and decentralized exchanges such as trades, order books and put it to kafka.", 
            "title": "Exchanges Connector"
        }, 
        {
            "location": "/cyber-markets/components/overview/#storer", 
            "text": "Writes data from kafka topics or directly from exchanges-connector to cassandra cluster", 
            "title": "Storer"
        }, 
        {
            "location": "/cyber-markets/components/overview/#tickers", 
            "text": "Calculate tickers, prices, etc and aggregate raw data which collecting exchanges connectors module", 
            "title": "Tickers"
        }, 
        {
            "location": "/cyber-markets/contributing/cheat-sheet/", 
            "text": "Kafka\n\n\nStop kafka and delete kafka data(cheat sheet)\n\n\ndocker stop fast-data-dev-markets\ndocker rm fast-data-dev-markets\n\n\n\n\n\nElassandra\n\n\nStop elassandra and delete elassandra data(cheat sheet)\n\n\ndocker stop elassandra-markets\ndocker rm elassandra-markets \n\n\n\n\n\nChains\n\n\nRun parity node(cheat sheet)\n\n\nsudo  docker run -d -p \n8545\n:8545 --name parity_eth \n\\\n\n-v \n${\nREPLACE_IT_BY_HOST_FOLDER\n}\n:/cyberdata parity/parity:stable \n\\\n\n--db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads \n4", 
            "title": "Cheat sheet"
        }, 
        {
            "location": "/cyber-markets/contributing/cheat-sheet/#kafka", 
            "text": "", 
            "title": "Kafka"
        }, 
        {
            "location": "/cyber-markets/contributing/cheat-sheet/#stop-kafka-and-delete-kafka-datacheat-sheet", 
            "text": "docker stop fast-data-dev-markets\ndocker rm fast-data-dev-markets", 
            "title": "Stop kafka and delete kafka data(cheat sheet)"
        }, 
        {
            "location": "/cyber-markets/contributing/cheat-sheet/#elassandra", 
            "text": "", 
            "title": "Elassandra"
        }, 
        {
            "location": "/cyber-markets/contributing/cheat-sheet/#stop-elassandra-and-delete-elassandra-datacheat-sheet", 
            "text": "docker stop elassandra-markets\ndocker rm elassandra-markets", 
            "title": "Stop elassandra and delete elassandra data(cheat sheet)"
        }, 
        {
            "location": "/cyber-markets/contributing/cheat-sheet/#chains", 
            "text": "", 
            "title": "Chains"
        }, 
        {
            "location": "/cyber-markets/contributing/cheat-sheet/#run-parity-nodecheat-sheet", 
            "text": "sudo  docker run -d -p  8545 :8545 --name parity_eth  \\ \n-v  ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable  \\ \n--db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads  4", 
            "title": "Run parity node(cheat sheet)"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/", 
            "text": "Contributing to Cyber Markets \n\n\nThank you for considering a contribution to Cyber Markets! This guide explains how to:\n\n Get started\n\n Development workflow\n* Get help if you encounter trouble\n\n\nGet in touch\n\n\nBefore starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can \nsave both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining:\n\n\n\n\nWhy is this change done? What's the use case?\n\n\nWhat will the API look like? (For new features)\n\n\nWhat test cases should it have? What could go wrong?\n\n\nHow will it roughly be implemented? (We'll happily provide code pointers to save you time)\n\n\n\n\nDevelopment Workflow\n\n\nDevelopment Setup\n\n\nPlease, use \ndevelopment environment setup guide\n.\n\n\nMake Changes\n\n\nUse this \nArchitecture Overview\n as a start point for making changes.\n\n\nLocal Check\n\n\nSeveral checks should passed to succeed build.\n\n Detekt code analyze tool should not report any issues\n\n \nJUnit\n tests should pass\n\n\nBefore committing you changes, please, run local project check by:\n\n\n./gradlew build    //linux, mac\ngradlew.bat build  //windows\n\n\n\n\n\nCreating Commits And Writing Commit Messages\n\n\nThe commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages:\n\n\n\n\nKeep commits discrete: avoid including multiple unrelated changes in a single commit\n\n\nKeep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation\n\n\nAdd GitHub issue to \nCHANGELOG.md\n\n\nInclude GitHub issue in the commit message on a first line at the beginning. Example:\n\n\n\n\n#123 Refactor CONTRIBUTING.md\n\n--Add Creating Commits And Writing Commit Messages Section\n\n\n\n\n\nSubmitting Your Change\n\n\nAfter you submit your pull request, a core developer will review it. It is normal that this takes several \niterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy.\n\n\nGetting Help\n\n\nIf you run into any trouble, please reach out to us on the issue you are working on.\n\n\nOur Thanks\n\n\nWe deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized\n in the release notes for the version you've contributed to.", 
            "title": "Contributing to Cyber Markets [![GitHub contributors](https://img.shields.io/github/contributors/cybercongress/cyber-markets.svg?style=flat-square&longCache=true)](https://github.com/cybercongress/cyber-markets)"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/#contributing-to-cyber-markets", 
            "text": "Thank you for considering a contribution to Cyber Markets! This guide explains how to:  Get started  Development workflow\n* Get help if you encounter trouble", 
            "title": "Contributing to Cyber Markets"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/#get-in-touch", 
            "text": "Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can \nsave both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining:   Why is this change done? What's the use case?  What will the API look like? (For new features)  What test cases should it have? What could go wrong?  How will it roughly be implemented? (We'll happily provide code pointers to save you time)", 
            "title": "Get in touch"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/#development-workflow", 
            "text": "", 
            "title": "Development Workflow"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/#development-setup", 
            "text": "Please, use  development environment setup guide .", 
            "title": "Development Setup"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/#make-changes", 
            "text": "Use this  Architecture Overview  as a start point for making changes.", 
            "title": "Make Changes"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/#local-check", 
            "text": "Several checks should passed to succeed build.  Detekt code analyze tool should not report any issues   JUnit  tests should pass  Before committing you changes, please, run local project check by:  ./gradlew build    //linux, mac\ngradlew.bat build  //windows", 
            "title": "Local Check"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/#creating-commits-and-writing-commit-messages", 
            "text": "The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages:   Keep commits discrete: avoid including multiple unrelated changes in a single commit  Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation  Add GitHub issue to  CHANGELOG.md  Include GitHub issue in the commit message on a first line at the beginning. Example:   #123 Refactor CONTRIBUTING.md\n\n--Add Creating Commits And Writing Commit Messages Section", 
            "title": "Creating Commits And Writing Commit Messages"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/#submitting-your-change", 
            "text": "After you submit your pull request, a core developer will review it. It is normal that this takes several \niterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy.", 
            "title": "Submitting Your Change"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/#getting-help", 
            "text": "If you run into any trouble, please reach out to us on the issue you are working on.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/cyber-markets/contributing/contributing/#our-thanks", 
            "text": "We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized\n in the release notes for the version you've contributed to.", 
            "title": "Our Thanks"
        }, 
        {
            "location": "/cyber-markets/contributing/dev-environment/", 
            "text": "Development environment\n\n\nUseful Links\n\n\n\n\ncheat sheet\n\n\n\n\nPrestart\n\n\n\n\nInstal Java 8 JDK\n\n\nInstall Docker and Docker Compose\n\n\nInstall Intellij Idea\n\n\n\n\nRun Kafka, Elassandra, Prometheus and Grafana\n\n\nStart containers(required)\n\n\nFor mac:\n\n\ndocker-compose -f dev-environment/env-mac.yml up -d\n\n\n\n\n\nFor linux family:\n\n\ndocker-compose -f dev-environment/env.yml up -d\n\n\n\n\n\nBootstrap Elassandra with keyspaces(required)\n\n\ndocker cp dev-environment/elassandra-bootstrap.cql  elassandra-markets:/elassandra-bootstrap.cql\ndocker \nexec\n -it elassandra-markets bash\ncqlsh -f elassandra-bootstrap.cql\n\n\n\n\n\nImport project to Intellij Idea\n\n\nOpen Project in idea by selecting: Import Project -\n selecting build.gradle file from the repository root\n\n\n\n\n\n\nWait for dependency downloading and indexation\n\n\nRun Exchanges Connector, Tickers, or API from intellij Idea\n\n\nGo to ExchangesConnectorApplication.kt and press green triangle on left to the code (on example line 16):\n\n\n\nIf, you use parity endpoint different rather that localhost:8545, than Etherdelta connector will fail due to lack of environment property PARITY_URL.\nLet's define it: Select \"Edit Configurations\"\n\n\n\n\nAdd next properties:\n\n\n\n\nNow, run exchanges connector one more time then etherdelta connector should start.\nYou can add environment variables in the same way for Tickers, APIs and etc.", 
            "title": "Development environment"
        }, 
        {
            "location": "/cyber-markets/contributing/dev-environment/#development-environment", 
            "text": "", 
            "title": "Development environment"
        }, 
        {
            "location": "/cyber-markets/contributing/dev-environment/#useful-links", 
            "text": "cheat sheet", 
            "title": "Useful Links"
        }, 
        {
            "location": "/cyber-markets/contributing/dev-environment/#prestart", 
            "text": "Instal Java 8 JDK  Install Docker and Docker Compose  Install Intellij Idea", 
            "title": "Prestart"
        }, 
        {
            "location": "/cyber-markets/contributing/dev-environment/#run-kafka-elassandra-prometheus-and-grafana", 
            "text": "", 
            "title": "Run Kafka, Elassandra, Prometheus and Grafana"
        }, 
        {
            "location": "/cyber-markets/contributing/dev-environment/#start-containersrequired", 
            "text": "For mac:  docker-compose -f dev-environment/env-mac.yml up -d  For linux family:  docker-compose -f dev-environment/env.yml up -d", 
            "title": "Start containers(required)"
        }, 
        {
            "location": "/cyber-markets/contributing/dev-environment/#bootstrap-elassandra-with-keyspacesrequired", 
            "text": "docker cp dev-environment/elassandra-bootstrap.cql  elassandra-markets:/elassandra-bootstrap.cql\ndocker  exec  -it elassandra-markets bash\ncqlsh -f elassandra-bootstrap.cql", 
            "title": "Bootstrap Elassandra with keyspaces(required)"
        }, 
        {
            "location": "/cyber-markets/contributing/dev-environment/#import-project-to-intellij-idea", 
            "text": "Open Project in idea by selecting: Import Project -  selecting build.gradle file from the repository root   \nWait for dependency downloading and indexation", 
            "title": "Import project to Intellij Idea"
        }, 
        {
            "location": "/cyber-markets/contributing/dev-environment/#run-exchanges-connector-tickers-or-api-from-intellij-idea", 
            "text": "Go to ExchangesConnectorApplication.kt and press green triangle on left to the code (on example line 16):  If, you use parity endpoint different rather that localhost:8545, than Etherdelta connector will fail due to lack of environment property PARITY_URL.\nLet's define it: Select \"Edit Configurations\"   Add next properties:   Now, run exchanges connector one more time then etherdelta connector should start.\nYou can add environment variables in the same way for Tickers, APIs and etc.", 
            "title": "Run Exchanges Connector, Tickers, or API from intellij Idea"
        }, 
        {
            "location": "/cyber-search/about-search/", 
            "text": "About Search", 
            "title": "About Search"
        }, 
        {
            "location": "/cyber-search/about-search/#about-search", 
            "text": "", 
            "title": "About Search"
        }, 
        {
            "location": "/cyber-search/api/README/", 
            "text": "Build and Run locally\n\n\ndocker build -t build/raw-api -f ./docs/api/Dockerfile ./ \n docker run -p \n8080\n:8080 build/raw-api", 
            "title": "README"
        }, 
        {
            "location": "/cyber-search/api/README/#build-and-run-locally", 
            "text": "docker build -t build/raw-api -f ./docs/api/Dockerfile ./   docker run -p  8080 :8080 build/raw-api", 
            "title": "Build and Run locally"
        }, 
        {
            "location": "/cyber-search/components/bitcoin-components/", 
            "text": "Bitcoin Components\n\n\n\n\n\n\n\n\nComponent\n\n\nScale\n\n\nDescription\n\n\nCluster Address\n\n\nMetrics\n\n\nExternal\n\n\n\n\n\n\n\n\n\n\nBitcoin Pump\n\n\n1\n\n\nBitcoin Chain Data Kafka Pump\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\nBitcoin Cassandra Dump\n\n\n1\n\n\nDump Kafka Topics Into Cassandra\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\nBitcoin Contract Summary\n\n\n1 - N\n\n\nCalculates Contract Summaries (balances and etc)\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\n\n\nBitcoin Pump\n\n\nPumps Bitcoin raw data(block,tx,uncles) into Kafka.\n\n\nBitcoin Cassandra Dump\n\n\nConsumes Bitcoin raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.\n\n\nBitcoin Contract Summary\n\n\nCollects summary for each contract stored in Bitcoin chain. Consumes Kafka topics and update data in Elassandra.", 
            "title": "Bitcoin Components"
        }, 
        {
            "location": "/cyber-search/components/bitcoin-components/#bitcoin-components", 
            "text": "Component  Scale  Description  Cluster Address  Metrics  External      Bitcoin Pump  1  Bitcoin Chain Data Kafka Pump   8080:/actuator/metrics     Bitcoin Cassandra Dump  1  Dump Kafka Topics Into Cassandra   8080:/actuator/metrics     Bitcoin Contract Summary  1 - N  Calculates Contract Summaries (balances and etc)   8080:/actuator/metrics", 
            "title": "Bitcoin Components"
        }, 
        {
            "location": "/cyber-search/components/bitcoin-components/#bitcoin-pump", 
            "text": "Pumps Bitcoin raw data(block,tx,uncles) into Kafka.", 
            "title": "Bitcoin Pump"
        }, 
        {
            "location": "/cyber-search/components/bitcoin-components/#bitcoin-cassandra-dump", 
            "text": "Consumes Bitcoin raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.", 
            "title": "Bitcoin Cassandra Dump"
        }, 
        {
            "location": "/cyber-search/components/bitcoin-components/#bitcoin-contract-summary", 
            "text": "Collects summary for each contract stored in Bitcoin chain. Consumes Kafka topics and update data in Elassandra.", 
            "title": "Bitcoin Contract Summary"
        }, 
        {
            "location": "/cyber-search/components/cassandra-service/", 
            "text": "Cassandra Service\n\n\nUsage of cassandra-service Module\n\n\n\u0421assandra service module could be used in two ways:\n\n\n\n\nWith specifying \nCHAIN_FAMILY\n environment variable\n\n\nWithout specifying \nCHAIN_FAMILY\n environment variable\n\n\n\n\nSpecifying \nCHAIN_FAMILY\n\n\nWhen you're specifying \nCHAIN_FAMILY\n environment variable cassandra-service module will work in the context of one keyspace based on this family.\nFor example if \nCHAIN_FAMILY=BITCOIN\n it will create \nbitcoin\n keyspace if it doesn't exists and do all migrations for this keyspace if they're not applied.\nIn spring context only repositories beans related to this keyspace will be available \n\n\nAll of the cassandra work handled by \nCassandraRepositoriesConfiguration\n and it specific inheritor.\n\n\nThis kind of interaction with cassandra-service used in dumps and contract summaries modules.\n\n\nWithout \nCHAIN_FAMILY\n\n\nWithout specifying \nCHAIN_FAMILY\n environment variable cassandra-service module will act like this:\n\n\n\n\nIt will run no migrations and creations of keyspaces\n\n\nIt will look on available keyspaces in current cassandra database based on \nSearchRepositoriesConfiguration\n inheritors\n and if they're exists it will initialize all repositories beans for this keyspaces and put them into spring context. \n\n\n\n\nThis is useful when you want to interact with all of available keyspaces and indicies.\nFor example in search-api module we need all of repositories beans to build requests.", 
            "title": "Cassandra Service"
        }, 
        {
            "location": "/cyber-search/components/cassandra-service/#cassandra-service", 
            "text": "", 
            "title": "Cassandra Service"
        }, 
        {
            "location": "/cyber-search/components/cassandra-service/#usage-of-cassandra-service-module", 
            "text": "\u0421assandra service module could be used in two ways:   With specifying  CHAIN_FAMILY  environment variable  Without specifying  CHAIN_FAMILY  environment variable", 
            "title": "Usage of cassandra-service Module"
        }, 
        {
            "location": "/cyber-search/components/cassandra-service/#specifying-chain_family", 
            "text": "When you're specifying  CHAIN_FAMILY  environment variable cassandra-service module will work in the context of one keyspace based on this family.\nFor example if  CHAIN_FAMILY=BITCOIN  it will create  bitcoin  keyspace if it doesn't exists and do all migrations for this keyspace if they're not applied.\nIn spring context only repositories beans related to this keyspace will be available   All of the cassandra work handled by  CassandraRepositoriesConfiguration  and it specific inheritor.  This kind of interaction with cassandra-service used in dumps and contract summaries modules.", 
            "title": "Specifying CHAIN_FAMILY"
        }, 
        {
            "location": "/cyber-search/components/cassandra-service/#without-chain_family", 
            "text": "Without specifying  CHAIN_FAMILY  environment variable cassandra-service module will act like this:   It will run no migrations and creations of keyspaces  It will look on available keyspaces in current cassandra database based on  SearchRepositoriesConfiguration  inheritors\n and if they're exists it will initialize all repositories beans for this keyspaces and put them into spring context.    This is useful when you want to interact with all of available keyspaces and indicies.\nFor example in search-api module we need all of repositories beans to build requests.", 
            "title": "Without CHAIN_FAMILY"
        }, 
        {
            "location": "/cyber-search/components/custom-chain-name/", 
            "text": "Custom Chain Name\n\n\nYou have an ability to run our services with custom chain name based on supported chain families.\nIt may be needed if you have your own custom chain and you want API endpoints, Kafka topic names and Cassandra keyspaces to use your chain name.\nCurrently we're supporting \nBITCOIN\n and \nETHEREUM\n chain families.\n\n\nTo make it work you need to define \nCHAIN_NAME\n environment variable along with \nCHAIN_FAMILY\n.\nAlso you may need to define your chain node url in PUMP by setting \nCHAIN_NODE_URL\n environment variable.\n\n\nFor example if you want to index \nETHEREUM\n based chain called \nMY_PRECIOUS\n.\nYou have to use following config in all services (\nCHAIN_NODE_URL\n is necessary only for PUMP service):\n\n\nCHAIN_FAMILY\n \n=\n \nETHEREUM\n\n\nCHAIN_NAME\n \n=\n \nMY_PRECIOUS\n\n\nCHAIN_NODE_URL\n \n=\n \nhttps:\n//my_precious_node_url:port\n\n\n\n\n\n\nThen all Kafka topics will be named as \nMY_PRECIOUS_TX_PUMP\n,\nMY_PRECIOUS_BLOCK_PUMP\n,\nMY_PRECIOUS_UNCLE_PUMP\n. Cassandra keyspace will be named \nmy_precious\n.\nAnd all API url will be build like this \nhttp://localhost:8080/my_precious/block/42\n and etc.", 
            "title": "Custom Chain Name"
        }, 
        {
            "location": "/cyber-search/components/custom-chain-name/#custom-chain-name", 
            "text": "You have an ability to run our services with custom chain name based on supported chain families.\nIt may be needed if you have your own custom chain and you want API endpoints, Kafka topic names and Cassandra keyspaces to use your chain name.\nCurrently we're supporting  BITCOIN  and  ETHEREUM  chain families.  To make it work you need to define  CHAIN_NAME  environment variable along with  CHAIN_FAMILY .\nAlso you may need to define your chain node url in PUMP by setting  CHAIN_NODE_URL  environment variable.  For example if you want to index  ETHEREUM  based chain called  MY_PRECIOUS .\nYou have to use following config in all services ( CHAIN_NODE_URL  is necessary only for PUMP service):  CHAIN_FAMILY   =   ETHEREUM  CHAIN_NAME   =   MY_PRECIOUS  CHAIN_NODE_URL   =   https: //my_precious_node_url:port   Then all Kafka topics will be named as  MY_PRECIOUS_TX_PUMP , MY_PRECIOUS_BLOCK_PUMP , MY_PRECIOUS_UNCLE_PUMP . Cassandra keyspace will be named  my_precious .\nAnd all API url will be build like this  http://localhost:8080/my_precious/block/42  and etc.", 
            "title": "Custom Chain Name"
        }, 
        {
            "location": "/cyber-search/components/ethereum-components/", 
            "text": "Ethereum Components\n\n\n\n\n\n\n\n\nComponent\n\n\nScale\n\n\nDescription\n\n\nCluster Address\n\n\nMetrics\n\n\nExternal\n\n\n\n\n\n\n\n\n\n\nEthereum Pump\n\n\n1\n\n\nEthereum Chain Data Kafka Pump\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\nEthereum Cassandra Dump\n\n\n1\n\n\nDump Kafka Topics Into Cassandra\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\nEthereum Contract Summary\n\n\n1 - N\n\n\nCalculates Contract Summaries (balances and etc)\n\n\n\n\n8080:/actuator/metrics\n\n\n\n\n\n\n\n\n\n\nEthereum Pump\n\n\nPumps Ethereum raw data(block,tx,uncles) into Kafka.\n\n\nEthereum Cassandra Dump\n\n\nConsumes Ethereum raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.\n\n\nEthereum Contract Summary\n\n\nCollects summary for each contract stored in Ethereum chain. Consumes Kafka topics and update data in Elassandra.", 
            "title": "Ethereum Components"
        }, 
        {
            "location": "/cyber-search/components/ethereum-components/#ethereum-components", 
            "text": "Component  Scale  Description  Cluster Address  Metrics  External      Ethereum Pump  1  Ethereum Chain Data Kafka Pump   8080:/actuator/metrics     Ethereum Cassandra Dump  1  Dump Kafka Topics Into Cassandra   8080:/actuator/metrics     Ethereum Contract Summary  1 - N  Calculates Contract Summaries (balances and etc)   8080:/actuator/metrics", 
            "title": "Ethereum Components"
        }, 
        {
            "location": "/cyber-search/components/ethereum-components/#ethereum-pump", 
            "text": "Pumps Ethereum raw data(block,tx,uncles) into Kafka.", 
            "title": "Ethereum Pump"
        }, 
        {
            "location": "/cyber-search/components/ethereum-components/#ethereum-cassandra-dump", 
            "text": "Consumes Ethereum raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.", 
            "title": "Ethereum Cassandra Dump"
        }, 
        {
            "location": "/cyber-search/components/ethereum-components/#ethereum-contract-summary", 
            "text": "Collects summary for each contract stored in Ethereum chain. Consumes Kafka topics and update data in Elassandra.", 
            "title": "Ethereum Contract Summary"
        }, 
        {
            "location": "/cyber-search/components/search-common-components/", 
            "text": "Search Common Components\n\n\n\n\n\n\n\n\nComponent\n\n\nScale\n\n\nDescription\n\n\nCluster Address\n\n\nMetrics\n\n\nExternal\n\n\n\n\n\n\n\n\n\n\nKafka\n\n\n1\n\n\nData Entry\n\n\n\n\n\n\n\n\n\n\n\n\nKafka Manager\n\n\n1\n\n\nKafka Explorer\n\n\n\n\n\n\n\n\n\n\n\n\nElassandra\n\n\n1 - N\n\n\nData Back And Search\n\n\nelassandra.search.svc:9042\n\n\n\n\n\n\n\n\n\n\nSearch Api\n\n\n1 - N\n\n\nApi To Search Chain Entities\n\n\nsearch-api.search.svc:80\n\n\n8080:/actuator/metrics\n\n\ny\n\n\n\n\n\n\nSearch Api Docs\n\n\n1 - N\n\n\nSearch Api Docs Based On Swagger\n\n\nsearch-api-docs.search.svc:80\n\n\n\n\ny\n\n\n\n\n\n\n\n\n\n\nChains Components\n\n\n\n\nEthereum\n\n\nBitcoin\n\n\n\n\nKafka\n\n\nKafka\n is message queue used as main data entry. All raw data firstly goes into kafka, than \n various services consume it to provide new functionality.\n\n\nElassandra (Elastic + Cassandra)\n\n\nElassandra\n is used as main backend storage with linear growth fulltext-search\n index on source cassandra data.\n\n\nSearch Api\n\n\nMain search access endpoint. See \napi calls documentations\n.", 
            "title": "Search Common Components"
        }, 
        {
            "location": "/cyber-search/components/search-common-components/#search-common-components", 
            "text": "Component  Scale  Description  Cluster Address  Metrics  External      Kafka  1  Data Entry       Kafka Manager  1  Kafka Explorer       Elassandra  1 - N  Data Back And Search  elassandra.search.svc:9042      Search Api  1 - N  Api To Search Chain Entities  search-api.search.svc:80  8080:/actuator/metrics  y    Search Api Docs  1 - N  Search Api Docs Based On Swagger  search-api-docs.search.svc:80   y", 
            "title": "Search Common Components"
        }, 
        {
            "location": "/cyber-search/components/search-common-components/#chains-components", 
            "text": "Ethereum  Bitcoin", 
            "title": "Chains Components"
        }, 
        {
            "location": "/cyber-search/components/search-common-components/#kafka", 
            "text": "Kafka  is message queue used as main data entry. All raw data firstly goes into kafka, than \n various services consume it to provide new functionality.", 
            "title": "Kafka"
        }, 
        {
            "location": "/cyber-search/components/search-common-components/#elassandra-elastic-cassandra", 
            "text": "Elassandra  is used as main backend storage with linear growth fulltext-search\n index on source cassandra data.", 
            "title": "Elassandra (Elastic + Cassandra)"
        }, 
        {
            "location": "/cyber-search/components/search-common-components/#search-api", 
            "text": "Main search access endpoint. See  api calls documentations .", 
            "title": "Search Api"
        }, 
        {
            "location": "/cyber-search/contributing/cheat-sheet/", 
            "text": "Kafka\n\n\nStop kafka and delete kafka data(cheat sheet)\n\n\ndocker stop fast-data-dev-search\ndocker rm fast-data-dev-search\n\n\n\n\n\nElassandra\n\n\nStop elassandra and delete elassandra data(cheat sheet)\n\n\ndocker stop elassandra-search\ndocker rm elassandra-search\n\n\n\n\n\nGet indices info\n\n\ncurl -XGET \nlocalhost:9200/_cat/indices?v\npretty\n\n\n\n\n\n\nChains\n\n\nRun parity node\n\n\nsudo  docker run -d -p \n8545\n:8545 --name parity_eth \n\\\n\n-v \n${\nREPLACE_IT_BY_HOST_FOLDER\n}\n:/cyberdata parity/parity:stable \n\\\n\n--db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads \n4\n\n\n\n\n\n\nRun bitcoind node\n\n\ndocker run -d -p 8332:8332 --name bitcoind --restart always \\\n-v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\\n-server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0", 
            "title": "Cheat sheet"
        }, 
        {
            "location": "/cyber-search/contributing/cheat-sheet/#kafka", 
            "text": "", 
            "title": "Kafka"
        }, 
        {
            "location": "/cyber-search/contributing/cheat-sheet/#stop-kafka-and-delete-kafka-datacheat-sheet", 
            "text": "docker stop fast-data-dev-search\ndocker rm fast-data-dev-search", 
            "title": "Stop kafka and delete kafka data(cheat sheet)"
        }, 
        {
            "location": "/cyber-search/contributing/cheat-sheet/#elassandra", 
            "text": "", 
            "title": "Elassandra"
        }, 
        {
            "location": "/cyber-search/contributing/cheat-sheet/#stop-elassandra-and-delete-elassandra-datacheat-sheet", 
            "text": "docker stop elassandra-search\ndocker rm elassandra-search", 
            "title": "Stop elassandra and delete elassandra data(cheat sheet)"
        }, 
        {
            "location": "/cyber-search/contributing/cheat-sheet/#get-indices-info", 
            "text": "curl -XGET  localhost:9200/_cat/indices?v pretty", 
            "title": "Get indices info"
        }, 
        {
            "location": "/cyber-search/contributing/cheat-sheet/#chains", 
            "text": "", 
            "title": "Chains"
        }, 
        {
            "location": "/cyber-search/contributing/cheat-sheet/#run-parity-node", 
            "text": "sudo  docker run -d -p  8545 :8545 --name parity_eth  \\ \n-v  ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable  \\ \n--db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads  4", 
            "title": "Run parity node"
        }, 
        {
            "location": "/cyber-search/contributing/cheat-sheet/#run-bitcoind-node", 
            "text": "docker run -d -p 8332:8332 --name bitcoind --restart always \\\n-v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\\n-server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0", 
            "title": "Run bitcoind node"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/", 
            "text": "Contributing to Cyber Search \n\n\nThank you for considering a contribution to Cyber Search! This guide explains how to:\n\n Get started\n\n Development workflow\n* Get help if you encounter trouble\n\n\nGet in touch\n\n\nBefore starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can \nsave both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining:\n\n\n\n\nWhy is this change done? What's the use case?\n\n\nWhat will the API look like? (For new features)\n\n\nWhat test cases should it have? What could go wrong?\n\n\nHow will it roughly be implemented? (We'll happily provide code pointers to save you time)\n\n\n\n\nDevelopment Workflow\n\n\nDevelopment Setup\n\n\nPlease, use \ndevelopment environment setup guide\n.\n\n\nMake Changes\n\n\nUse this \nArchitecture Overview\n as a start point for making changes.\n\n\nLocal Check\n\n\nSeveral checks should passed to succeed build.\n\n \nDetekt\n code analyze tool should not report any issues\n\n \nJUnit\n tests should pass\n\n\nBefore committing you changes, please, run local project check by:\n\n\n./gradlew build    //linux, mac\ngradlew.bat build  //windows\n\n\n\n\n\nCreating Commits And Writing Commit Messages\n\n\nThe commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages:\n solidity/CONTRIBUTING.md\n\n\n\n\nKeep commits discrete: avoid including multiple unrelated changes in a single commit\n\n\nKeep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation\n\n\nAdd GitHub issue to \nCHANGELOG.md\n\n\nInclude GitHub issue in the commit message on a first line at the beginning. Example:\n\n\n\n\n#123 Refactor CONTRIBUTING.md\n\n--Add Creating Commits And Writing Commit Messages Section\n\n\n\n\n\nSubmitting Your Change\n\n\nAfter you submit your pull request, a core developer will review it. It is normal that this takes several \niterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy.\n\n\nGetting Help\n\n\nIf you run into any trouble, please reach out to us on the issue you are working on.\n\n\nOur Thanks\n\n\nWe deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized\n in the release notes for the version you've contributed to.", 
            "title": "Contributing to Cyber Search [![GitHub contributors](https://img.shields.io/github/contributors/cybercongress/cyber-search.svg?style=flat-square&longCache=true)](https://github.com/cybercongress/cyber-search)"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/#contributing-to-cyber-search", 
            "text": "Thank you for considering a contribution to Cyber Search! This guide explains how to:  Get started  Development workflow\n* Get help if you encounter trouble", 
            "title": "Contributing to Cyber Search"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/#get-in-touch", 
            "text": "Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can \nsave both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining:   Why is this change done? What's the use case?  What will the API look like? (For new features)  What test cases should it have? What could go wrong?  How will it roughly be implemented? (We'll happily provide code pointers to save you time)", 
            "title": "Get in touch"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/#development-workflow", 
            "text": "", 
            "title": "Development Workflow"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/#development-setup", 
            "text": "Please, use  development environment setup guide .", 
            "title": "Development Setup"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/#make-changes", 
            "text": "Use this  Architecture Overview  as a start point for making changes.", 
            "title": "Make Changes"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/#local-check", 
            "text": "Several checks should passed to succeed build.   Detekt  code analyze tool should not report any issues   JUnit  tests should pass  Before committing you changes, please, run local project check by:  ./gradlew build    //linux, mac\ngradlew.bat build  //windows", 
            "title": "Local Check"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/#creating-commits-and-writing-commit-messages", 
            "text": "The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages:\n solidity/CONTRIBUTING.md   Keep commits discrete: avoid including multiple unrelated changes in a single commit  Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation  Add GitHub issue to  CHANGELOG.md  Include GitHub issue in the commit message on a first line at the beginning. Example:   #123 Refactor CONTRIBUTING.md\n\n--Add Creating Commits And Writing Commit Messages Section", 
            "title": "Creating Commits And Writing Commit Messages"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/#submitting-your-change", 
            "text": "After you submit your pull request, a core developer will review it. It is normal that this takes several \niterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy.", 
            "title": "Submitting Your Change"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/#getting-help", 
            "text": "If you run into any trouble, please reach out to us on the issue you are working on.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/cyber-search/contributing/contributing/#our-thanks", 
            "text": "We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized\n in the release notes for the version you've contributed to.", 
            "title": "Our Thanks"
        }, 
        {
            "location": "/cyber-search/contributing/dev-environment/", 
            "text": "Development environment\n\n\nUseful Links\n\n\n\n\ncheat sheet\n\n\n\n\nPrestart\n\n\n\n\nInstall Java 8 JDK\n\n\nInstall Docker and Docker Compose\n\n\nInstall Intellij Idea\n\n\n\n\nRun Kafka, Elassandra, Prometheus and Grafana\n\n\nStart containers(required)\n\n\nFor mac:\n\n\ncd\n dev-environment\ndocker-compose -f env-mac.yml up -d\n\n\n\n\n\nFor linux family:\n\n\ncd\n dev-environment\ndocker-compose -f env.yml up -d\n\n\n\n\n\nRun chain node (only for pumps)\n\n\nIn order to fetch data from chains pumps need chain node to interact with. \nTo run chain node locally using docker use following commands:\n\n\n\n\nParity for Ethereum\n\n\n\n\nsudo  docker run -d -p 8545:8545 --name parity_eth \\\n-v ${REPLACE_IT_BY_HOST_FOLDER}:/cyberdata parity/parity:stable \\\n--db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4\n\n\n\n\n\n\n\nBitcoind for Bitcoin\n\n\n\n\ndocker run -d -p 8332:8332 --name bitcoind --restart always \\\n-v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\\n-server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0\n\n\n\n\n\nOr you could use any public available node (with appropriate settings)\nby passing \nCHAIN_NODE_URL\n environment variable to pump. For example \nCHAIN_NODE_URL=http://127.0.0.1:8545\n.\n\n\nImport project to Intellij Idea\n\n\nOpen Project in idea by selecting: Import Project -\n selecting \nbuild.gradle\n file from the repository root\n\n\n\n\nWait for dependency downloading and indexation\n\n\nRun Ethereum Pump from intellij Idea\n\n\nGo to EthereumPumpApplication.kt and press green triangle on left to the code (on example line 14):\n\n\n\nPump will fail due to lack of \nCHAIN_FAMILY\n environment property, let's define it: Select \"Edit Configuration\"\n\n\n\n\nAdd properties:\n\n\n\n\nNow, run pump one more time, it should start.", 
            "title": "Development environment"
        }, 
        {
            "location": "/cyber-search/contributing/dev-environment/#development-environment", 
            "text": "", 
            "title": "Development environment"
        }, 
        {
            "location": "/cyber-search/contributing/dev-environment/#useful-links", 
            "text": "cheat sheet", 
            "title": "Useful Links"
        }, 
        {
            "location": "/cyber-search/contributing/dev-environment/#prestart", 
            "text": "Install Java 8 JDK  Install Docker and Docker Compose  Install Intellij Idea", 
            "title": "Prestart"
        }, 
        {
            "location": "/cyber-search/contributing/dev-environment/#run-kafka-elassandra-prometheus-and-grafana", 
            "text": "", 
            "title": "Run Kafka, Elassandra, Prometheus and Grafana"
        }, 
        {
            "location": "/cyber-search/contributing/dev-environment/#start-containersrequired", 
            "text": "For mac:  cd  dev-environment\ndocker-compose -f env-mac.yml up -d  For linux family:  cd  dev-environment\ndocker-compose -f env.yml up -d", 
            "title": "Start containers(required)"
        }, 
        {
            "location": "/cyber-search/contributing/dev-environment/#run-chain-node-only-for-pumps", 
            "text": "In order to fetch data from chains pumps need chain node to interact with. \nTo run chain node locally using docker use following commands:   Parity for Ethereum   sudo  docker run -d -p 8545:8545 --name parity_eth \\\n-v ${REPLACE_IT_BY_HOST_FOLDER}:/cyberdata parity/parity:stable \\\n--db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4   Bitcoind for Bitcoin   docker run -d -p 8332:8332 --name bitcoind --restart always \\\n-v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\\n-server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0  Or you could use any public available node (with appropriate settings)\nby passing  CHAIN_NODE_URL  environment variable to pump. For example  CHAIN_NODE_URL=http://127.0.0.1:8545 .", 
            "title": "Run chain node (only for pumps)"
        }, 
        {
            "location": "/cyber-search/contributing/dev-environment/#import-project-to-intellij-idea", 
            "text": "Open Project in idea by selecting: Import Project -  selecting  build.gradle  file from the repository root  \nWait for dependency downloading and indexation", 
            "title": "Import project to Intellij Idea"
        }, 
        {
            "location": "/cyber-search/contributing/dev-environment/#run-ethereum-pump-from-intellij-idea", 
            "text": "Go to EthereumPumpApplication.kt and press green triangle on left to the code (on example line 14):  Pump will fail due to lack of  CHAIN_FAMILY  environment property, let's define it: Select \"Edit Configuration\"   Add properties:   Now, run pump one more time, it should start.", 
            "title": "Run Ethereum Pump from intellij Idea"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/", 
            "text": "Pump Development Guide\n\n\nTo develop your own chain pump you have two deal with two modules:\n\n common\n\n pumps\n\n\nSome explanations\n\n\nTo describe single chain in our system we're using following scheme.\n\n\nEvery chain has \nChainFamily\n. For example \nBITCOIN, ETHEREUM\n families.\n\n\nChainFamily\n has it's own set of entities like \nTX, BLOCK, UNCLE\n and default url of chain node.\n\n\nNext, chain has a name. Chain name used everywhere in modules. Kafka topics, Cassandra keyspaces, API endpoints\nwill be based on chain name. By default chain name is equal to \nChainFamily\n. For example if you have  \nChainFamily == BITCOIN\n\nand you not specifying a chain name it will be set to \nBITCOIN\n. Main goal of chain name is to separate forks\nof one chain that have the same structure. For example  \nBITCOIN\n and \nBITCOIN_CASH\n or \nETHEREUM\n and \nETHEREUM_CLASSIC\n. \n\n\nSo as far as you can understand single chain pump is chain pump for specific \nChainFamily\n\n\nAdding models to common module\n\n\nFirst of all you have to put models representing your blockchain entities in common module.\nThey should be placed in \nfund.cyber.search.model.{your_chain_family_name}\n package.\nThis models will be used to transfer your blockchain data among our microservices (using Kafka).\n\n\nAfter doing so put description of your chain family to \nChainFamily\n enum.\nYou could find it in \nfund.cyber.search.model.chains.ChainInfo.kt\n file. Description includes following info:\n\n\n\n\ndefaultNodeUrl\n - default URL of blockchain node to connect for data.\n\n\nentityTypes\n - map of blockchain entities types (from \nfund.cyber.search.model.chains.ChainEntity\n enum) to their class representation.\n\n\n\n\nIf you can't find needed entity type in \nfund.cyber.search.model.chains.ChainEntity\n enum you have to add it by your own.\n\n\nDeveloping pump\n\n\nNow you're ready to write pump for your chain family. Note that this pump is suitable only for block based chains.\n\n\nWe're using Spring Boot (https://projects.spring.io/spring-boot/). So you also should be familiar with spring beans, spring dependency injection and spring configuration.\n\n\nCreating gradle module\n\n\nTo start with, create new Gradle submodule with name of your chain in \npumps\n module. Include your module in \npumps/build.gradle\n and in \nsettings.gradle\n.\n\n\nExample of \npumps/build.gradle\n file with your module:\n\n\nproject\n(\n:pumps:common\n)\n \n{\n\n\n    \napply\n \nplugin:\n \nio.spring.dependency-management\n\n    \njar\n.\narchiveName\n \n=\n \ncommon-pumps\n\n\n    \ndependencies\n \n{\n\n        \n...\n\n    \n}\n\n\n}\n\n\n\n...\n\n\n\nproject\n(\n:pumps:{your_module_name}\n)\n \n{\n\n\n    \napply\n \nplugin:\n \norg.springframework.boot\n\n\n    \ndependencies\n \n{\n\n\n        \ncompile\n \nproject\n(\n:pumps:common\n)\n\n\n        \n//your module dependencies\n\n    \n}\n\n\n}\n\n\n\n...\n\n\n\n\n\n\nExample of \nsettings.gradle\n file:\n\n\ninclude\n \ncommon\n\n\ninclude\n \ncommon-kafka\n\n\ninclude\n \ncassandra-service\n\n\n\n...\n\n\n\ninclude\n \npumps:common\n\n\ninclude\n \npumps:bitcoin\n\n\ninclude\n \npumps:ethereum\n\n\ninclude\n \npumps:{your_chain_name}\n\n\n\n...\n\n\n\n\n\n\nAll classes should be placed under \nfund.cyber.pump.{your_chain_name}\n package (except main class).\n\n\nCreating spring boot main class\n\n\nThe next step is creating spring boot main class. It should be placed in your module under \nfund.cyber\n package.\n\n\nExample of main class:\n\n\n@SpringBootApplication\n(\nexclude\n \n=\n \n[\nKafkaAutoConfiguration\n::\nclass\n])\n\n\nclass\n \nBitcoinPumpApplication\n \n{\n\n\n    \ncompanion\n \nobject\n \n{\n\n        \n@JvmStatic\n\n        \nfun\n \nmain\n(\nargs\n:\n \nArray\nString\n)\n \n{\n\n\n            \nval\n \napplication\n \n=\n \nSpringApplication\n(\nBitcoinPumpApplication\n::\nclass\n.\njava\n)\n\n            \napplication\n.\nrunPump\n(\nargs\n)\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nDefault spring beans in context\n\n\nAfter spring context start you'll have few already configured beans in context that you could inject in your module classes:\n\n\n\n\norg.springframework.retry.support.RetryTemplate\n - Spring Retry template for retrying failed operations.\n\n\nio.micrometer.core.instrument.MeterRegistry\n - bean for monitoring.\n\n\nfund.cyber.search.model.chains.ChainInfo\n - bean with all needed properties of running chain.\n\n\n\n\nclass\n \nChainInfo\n(\n\n    \nval\n \nfamily\n:\n \nChainFamily\n,\n\n    \nval\n \nname\n:\n \nString\n \n=\n \n,\n\n    \nval\n \nnodeUrl\n:\n \nString\n \n=\n \nfamily\n.\ndefaultNodeUrl\n\n\n)\n \n{\n\n\n    \nval\n \nfullName\n\n        \nget\n()\n \n=\n \nfamily\n.\nname\n \n+\n \nif\n \n(\nname\n.\nisEmpty\n())\n \n \nelse\n \n_$name\n\n\n    \nval\n \nentityTypes\n\n        \nget\n()\n \n=\n \nfamily\n.\nentityTypes\n.\nkeys\n\n\n    \nfun\n \nentityClassByType\n(\ntype\n:\n \nChainEntityType\n)\n \n=\n \nfamily\n.\nentityTypes\n[\ntype\n]\n\n\n}\n\n\n\n\n\n\nChainInfo\n bean constructed from environment properties at start:\n\n \nCHAIN_FAMILY\n - chain family name (matches \nChainFamily\n enum). For example \nCHAIN_FAMILY=BITCOIN\n. \nRequired option\n.\n\n \nCHAIN_NAME\n - name of your specific chain. \nNot necessary\n. By default equals to \nCHAIN_FAMILY\n.\nAll kafka topic will be named accordingly. For example \nCHAIN_NAME=BITCOIN_CASH\n then a topic name will be \nBITCOIN_CASH_TX_PUMP\n.\n* \nCHAIN_NODE_URL\n - URL of node running your chain. \nNot necessary\n. By default will be equal to one that described in your \nChainFamily\n.\n\n\nImplementing pump\n\n\nTo integrate your pump with our system you simply have to implement two interfaces:\n* \nfund.cyber.pump.common.node.BlockBundle\n\n\n/**\n\n\n * Blockchain block with all dependent entities. Should collect all entities in scope of one block.\n\n\n * For example: transactions, uncles, etc..\n\n\n */\n\n\ninterface\n \nBlockBundle\n \n{\n\n    \n/**\n\n\n     * Hash of the block\n\n\n     */\n\n    \nval\n \nhash\n:\n \nString\n\n    \n/**\n\n\n     * Hash of the parent block\n\n\n     */\n\n    \nval\n \nparentHash\n:\n \nString\n\n    \n/**\n\n\n     * Number of the block in blockchain\n\n\n     */\n\n    \nval\n \nnumber\n:\n \nLong\n\n    \n/**\n\n\n     * Size of the block in bytes\n\n\n     */\n\n    \nval\n \nblockSize\n:\n \nInt\n\n\n    \n/**\n\n\n     * Get dependent entity values list by entity type.\n\n\n     *\n\n\n     * @param chainEntityType type of entity (for example: [ChainEntityType.TX])\n\n\n     * @return list of entity values (for example: transactions)\n\n\n     */\n\n    \nfun\n \nentitiesByType\n(\nchainEntityType\n:\n \nChainEntityType\n):\n \nList\nChainEntity\n\n\n}\n\n\n\n\n\n\n\n\nfund.cyber.pump.common.node.BlockchainInterface\n\n\n\n\n/**\n\n\n * Interface representing blockchain\n\n\n *\n\n\n * @param T block bundle of this blockchain\n\n\n */\n\n\ninterface\n \nBlockchainInterface\nout\n \nT\n \n:\n \nBlockBundle\n \n{\n\n    \n/**\n\n\n     * Get last number of the block in blockchain network.\n\n\n     *\n\n\n     * @return block number\n\n\n     */\n\n    \nfun\n \nlastNetworkBlock\n():\n \nLong\n\n\n    \n/**\n\n\n     * Get [BlockBundle] by block number.\n\n\n     *\n\n\n     * @param number block number\n\n\n     * @return block bundle\n\n\n     */\n\n    \nfun\n \nblockBundleByNumber\n(\nnumber\n:\n \nLong\n):\n \nT\n\n\n}\n\n\n\n\n\n\nAlso you should define spring bean of \nfund.cyber.pump.common.node.BlockchainInterface\n implementation either by annotate it with \n@Component\n or defining \n@Bean\n in spring configuration.\n\n\nFor example:\n\n\n@Component\n\n\nclass\n \nBitcoinBlockchainInterface\n(\n\n        \n...\n\n\n)\n \n:\n \nBlockchainInterface\nBitcoinBlockBundle\n \n{\n\n\n    \nprivate\n \nval\n \ndownloadSpeedMonitor\n \n=\n \nmonitoring\n.\ntimer\n(\npump_bundle_download\n)\n\n\n    \noverride\n \nfun\n \nlastNetworkBlock\n():\n \nLong\n \n=\n \nbitcoinJsonRpcClient\n.\ngetLastBlockNumber\n()\n\n\n    \noverride\n \nfun\n \nblockBundleByNumber\n(\nnumber\n:\n \nLong\n):\n \nBitcoinBlockBundle\n \n{\n\n        \nreturn\n \ndownloadSpeedMonitor\n.\nrecordCallable\n \n{\n\n            \nval\n \nblock\n \n=\n \nbitcoinJsonRpcClient\n.\ngetBlockByNumber\n(\nnumber\n)\n!!\n\n            \nreturn\n@recordCallable\n \nrpcToBundleEntitiesConverter\n.\nconvertToBundle\n(\nblock\n)\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nSo, as you can see, you're not care of what happening with data next and how to store it. All you need is just to tell \nBlockBundle\n how to map it's fields on entities and we'll take care of everything else.\n\n\nNote that you should use \ntoSearchHashFormat()\n extension function placed in \nfund.cyber.api.common.Func.kt\n on all fields in hex format when building your chain entities.\n\n\nMemory Pool Pump\n\n\nYou also could add memory pool pumping logic by simply implement \nPoolInterface\n interface\n\n\ninterface\n \nPoolInterface\nT\n:\n \nPoolItem\n \n{\n\n    \nfun\n \nsubscribePool\n():\n \nFlowable\nT\n\n\n}\n\n\n\n\n\n\nIt contains only one method that should return \nio.reactivex.Flowable\n of pool items.\n\n\nCreate Dockerfile\n\n\nPut Docker file in root folder of your module. Here is template of Dockerfile:\n\n\n# Build Stage\n# Container with application\nFROM openjdk:8-jre-slim\nCOPY /build/libs /cyberapp/bin\nENTRYPOINT exec java $JAVA_OPTS -jar /cyberapp/bin/${your_chain_name}.jar\n\n\n\n\n\nUpdate CI\n\n\nFinally, you should update \n.circleci/config.yml\n file with steps for building and pushing docker image.\n* Add deploy job to \njobs\n section. Template:\n\n\ndeploy_chain_pumps_${your_chain_name}_image:\n     \n: *defaults\n     steps:\n       - attach_workspace:\n           at: ~/build\n       - setup_remote_docker:\n           version: 17.11.0-ce\n       - run:\n           name: Build ${your_chain_name} Pump Image\n           command: |\n             docker build -t build/pump-${your_chain_name} -f ./pumps/${your_chain_name}/Dockerfile ./pumps/${your_chain_name}\n             docker login -u $DOCKER_USER -p $DOCKER_PASS\n             docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG\n             docker push cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG\n             docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:latest\n             docker push cybernode/chain-pump-${your_chain_name}:latest\n\n\n\n\n\n\n\nAdd deploy job to \nworkflows.search_build.jobs\n section. Template:\n\n\n\n\n- deploy_chain_pumps_${your_chain_name}_image:\n  \n: *release_filter\n  requires:\n    - build_project", 
            "title": "Pump Development Guide"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#pump-development-guide", 
            "text": "To develop your own chain pump you have two deal with two modules:  common  pumps", 
            "title": "Pump Development Guide"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#some-explanations", 
            "text": "To describe single chain in our system we're using following scheme.  Every chain has  ChainFamily . For example  BITCOIN, ETHEREUM  families.  ChainFamily  has it's own set of entities like  TX, BLOCK, UNCLE  and default url of chain node.  Next, chain has a name. Chain name used everywhere in modules. Kafka topics, Cassandra keyspaces, API endpoints\nwill be based on chain name. By default chain name is equal to  ChainFamily . For example if you have   ChainFamily == BITCOIN \nand you not specifying a chain name it will be set to  BITCOIN . Main goal of chain name is to separate forks\nof one chain that have the same structure. For example   BITCOIN  and  BITCOIN_CASH  or  ETHEREUM  and  ETHEREUM_CLASSIC .   So as far as you can understand single chain pump is chain pump for specific  ChainFamily", 
            "title": "Some explanations"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#adding-models-to-common-module", 
            "text": "First of all you have to put models representing your blockchain entities in common module.\nThey should be placed in  fund.cyber.search.model.{your_chain_family_name}  package.\nThis models will be used to transfer your blockchain data among our microservices (using Kafka).  After doing so put description of your chain family to  ChainFamily  enum.\nYou could find it in  fund.cyber.search.model.chains.ChainInfo.kt  file. Description includes following info:   defaultNodeUrl  - default URL of blockchain node to connect for data.  entityTypes  - map of blockchain entities types (from  fund.cyber.search.model.chains.ChainEntity  enum) to their class representation.   If you can't find needed entity type in  fund.cyber.search.model.chains.ChainEntity  enum you have to add it by your own.", 
            "title": "Adding models to common module"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#developing-pump", 
            "text": "Now you're ready to write pump for your chain family. Note that this pump is suitable only for block based chains.  We're using Spring Boot (https://projects.spring.io/spring-boot/). So you also should be familiar with spring beans, spring dependency injection and spring configuration.", 
            "title": "Developing pump"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#creating-gradle-module", 
            "text": "To start with, create new Gradle submodule with name of your chain in  pumps  module. Include your module in  pumps/build.gradle  and in  settings.gradle .  Example of  pumps/build.gradle  file with your module:  project ( :pumps:common )   { \n\n     apply   plugin:   io.spring.dependency-management \n     jar . archiveName   =   common-pumps \n\n     dependencies   { \n         ... \n     }  }  ...  project ( :pumps:{your_module_name} )   { \n\n     apply   plugin:   org.springframework.boot \n\n     dependencies   { \n\n         compile   project ( :pumps:common ) \n\n         //your module dependencies \n     }  }  ...   Example of  settings.gradle  file:  include   common  include   common-kafka  include   cassandra-service  ...  include   pumps:common  include   pumps:bitcoin  include   pumps:ethereum  include   pumps:{your_chain_name}  ...   All classes should be placed under  fund.cyber.pump.{your_chain_name}  package (except main class).", 
            "title": "Creating gradle module"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#creating-spring-boot-main-class", 
            "text": "The next step is creating spring boot main class. It should be placed in your module under  fund.cyber  package.  Example of main class:  @SpringBootApplication ( exclude   =   [ KafkaAutoConfiguration :: class ])  class   BitcoinPumpApplication   { \n\n     companion   object   { \n         @JvmStatic \n         fun   main ( args :   Array String )   { \n\n             val   application   =   SpringApplication ( BitcoinPumpApplication :: class . java ) \n             application . runPump ( args ) \n         } \n     }  }", 
            "title": "Creating spring boot main class"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#default-spring-beans-in-context", 
            "text": "After spring context start you'll have few already configured beans in context that you could inject in your module classes:   org.springframework.retry.support.RetryTemplate  - Spring Retry template for retrying failed operations.  io.micrometer.core.instrument.MeterRegistry  - bean for monitoring.  fund.cyber.search.model.chains.ChainInfo  - bean with all needed properties of running chain.   class   ChainInfo ( \n     val   family :   ChainFamily , \n     val   name :   String   =   , \n     val   nodeUrl :   String   =   family . defaultNodeUrl  )   { \n\n     val   fullName \n         get ()   =   family . name   +   if   ( name . isEmpty ())     else   _$name \n\n     val   entityTypes \n         get ()   =   family . entityTypes . keys \n\n     fun   entityClassByType ( type :   ChainEntityType )   =   family . entityTypes [ type ]  }   ChainInfo  bean constructed from environment properties at start:   CHAIN_FAMILY  - chain family name (matches  ChainFamily  enum). For example  CHAIN_FAMILY=BITCOIN .  Required option .   CHAIN_NAME  - name of your specific chain.  Not necessary . By default equals to  CHAIN_FAMILY .\nAll kafka topic will be named accordingly. For example  CHAIN_NAME=BITCOIN_CASH  then a topic name will be  BITCOIN_CASH_TX_PUMP .\n*  CHAIN_NODE_URL  - URL of node running your chain.  Not necessary . By default will be equal to one that described in your  ChainFamily .", 
            "title": "Default spring beans in context"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#implementing-pump", 
            "text": "To integrate your pump with our system you simply have to implement two interfaces:\n*  fund.cyber.pump.common.node.BlockBundle  /**   * Blockchain block with all dependent entities. Should collect all entities in scope of one block.   * For example: transactions, uncles, etc..   */  interface   BlockBundle   { \n     /**       * Hash of the block       */ \n     val   hash :   String \n     /**       * Hash of the parent block       */ \n     val   parentHash :   String \n     /**       * Number of the block in blockchain       */ \n     val   number :   Long \n     /**       * Size of the block in bytes       */ \n     val   blockSize :   Int \n\n     /**       * Get dependent entity values list by entity type.       *       * @param chainEntityType type of entity (for example: [ChainEntityType.TX])       * @return list of entity values (for example: transactions)       */ \n     fun   entitiesByType ( chainEntityType :   ChainEntityType ):   List ChainEntity  }    fund.cyber.pump.common.node.BlockchainInterface   /**   * Interface representing blockchain   *   * @param T block bundle of this blockchain   */  interface   BlockchainInterface out   T   :   BlockBundle   { \n     /**       * Get last number of the block in blockchain network.       *       * @return block number       */ \n     fun   lastNetworkBlock ():   Long \n\n     /**       * Get [BlockBundle] by block number.       *       * @param number block number       * @return block bundle       */ \n     fun   blockBundleByNumber ( number :   Long ):   T  }   Also you should define spring bean of  fund.cyber.pump.common.node.BlockchainInterface  implementation either by annotate it with  @Component  or defining  @Bean  in spring configuration.  For example:  @Component  class   BitcoinBlockchainInterface ( \n         ...  )   :   BlockchainInterface BitcoinBlockBundle   { \n\n     private   val   downloadSpeedMonitor   =   monitoring . timer ( pump_bundle_download ) \n\n     override   fun   lastNetworkBlock ():   Long   =   bitcoinJsonRpcClient . getLastBlockNumber () \n\n     override   fun   blockBundleByNumber ( number :   Long ):   BitcoinBlockBundle   { \n         return   downloadSpeedMonitor . recordCallable   { \n             val   block   =   bitcoinJsonRpcClient . getBlockByNumber ( number ) !! \n             return @recordCallable   rpcToBundleEntitiesConverter . convertToBundle ( block ) \n         } \n     }  }   So, as you can see, you're not care of what happening with data next and how to store it. All you need is just to tell  BlockBundle  how to map it's fields on entities and we'll take care of everything else.  Note that you should use  toSearchHashFormat()  extension function placed in  fund.cyber.api.common.Func.kt  on all fields in hex format when building your chain entities.", 
            "title": "Implementing pump"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#memory-pool-pump", 
            "text": "You also could add memory pool pumping logic by simply implement  PoolInterface  interface  interface   PoolInterface T :   PoolItem   { \n     fun   subscribePool ():   Flowable T  }   It contains only one method that should return  io.reactivex.Flowable  of pool items.", 
            "title": "Memory Pool Pump"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#create-dockerfile", 
            "text": "Put Docker file in root folder of your module. Here is template of Dockerfile:  # Build Stage\n# Container with application\nFROM openjdk:8-jre-slim\nCOPY /build/libs /cyberapp/bin\nENTRYPOINT exec java $JAVA_OPTS -jar /cyberapp/bin/${your_chain_name}.jar", 
            "title": "Create Dockerfile"
        }, 
        {
            "location": "/cyber-search/contributing/pump-development/#update-ci", 
            "text": "Finally, you should update  .circleci/config.yml  file with steps for building and pushing docker image.\n* Add deploy job to  jobs  section. Template:  deploy_chain_pumps_${your_chain_name}_image:\n      : *defaults\n     steps:\n       - attach_workspace:\n           at: ~/build\n       - setup_remote_docker:\n           version: 17.11.0-ce\n       - run:\n           name: Build ${your_chain_name} Pump Image\n           command: |\n             docker build -t build/pump-${your_chain_name} -f ./pumps/${your_chain_name}/Dockerfile ./pumps/${your_chain_name}\n             docker login -u $DOCKER_USER -p $DOCKER_PASS\n             docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG\n             docker push cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG\n             docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:latest\n             docker push cybernode/chain-pump-${your_chain_name}:latest   Add deploy job to  workflows.search_build.jobs  section. Template:   - deploy_chain_pumps_${your_chain_name}_image:\n   : *release_filter\n  requires:\n    - build_project", 
            "title": "Update CI"
        }, 
        {
            "location": "/cyberd/cyberd/", 
            "text": "Motivated Search and Evaluation Engine for Permanent Web\n\n\nDima Starodubcev\n\n\nDraft v 0.1\n\n\nJanuary-February 2017, Bali\n\n\nNote: Not updated according to cyberminds mindmap\n\n\nSimplified\n\n\nAbstract\n\n\nExisting general purpose search engines are restrictive centralized databases everybody forced to trust. These search engines were designed primarily for client-server architecture based on DNS, HTTP, and IP protocols. The emergence of content-addressable storage and distributed ledger technology creates an opportunity for the Internet to acquire new features such as more efficient computing, storing and broadband consumption, more resilient, secure and private access, no middleman for digital property. This can shift existing web's ubiquitously used client-server architecture to truly peer-to-peer interactions based on stateless IPFS and variety of stateful consensus computers such as Ethereum. This creates a challenge and opportunity for a search engine based on emerging technologies and specifically designed for them. Surprisingly the blockchain architecture itself allows organizing general purpose search engine in a way inaccessible for previous architectures. In this paper, we discuss opportunities behind blockchain based search engine, challenges of crawling, indexing and evaluation for the next generation web and propose a blockchain based experimental set of smart contracts to address discussed issues.\n\n\nIntroduction\n\n\nLet us start a discussion from disadvantages of conventional general purpose search engines:\n\n\n\n\nNo Transparency. Nobody outside of Google understands how the ranking really works. This creates a market for black and white SEO. The truth is that if e.g. Google discloses complete details of the ranking algorithm it would be easy for adversaries to game organic search results that kill the quality of results and ad revenue streams. Pagerank [PR] has no inherent trust mechanism resistant to Sybil attacks. This problem can be addressed adding transparent and accountable blockchain based ledger with properly designed economic incentives built into the system.\n\n\nNo Access. Currently, all search engines are centralized. Nobody is able to add to index as well as participate in improving the quality of search results. However, Google itself internally uses a workforce of \nsearch evaluators\n. It is our belief that user-generated search engine could have a higher quality of results as in a story with almost every website in existence.\n\n\nBroken Incentives. The vast majority of contribution to a search quality is made by users. Then any user searches something she extends semantic core. Then any user clicks on search results she trains a model. This creates an opportunity to continuously improve ranking model at the expense of users. Then search engines sell users to advertisers at the expense of harming user experience and acquire revenue streams which are not returned back to users at all. This simple loop created Alphabet's $550 billion capitalizations (~$80 per Earth capita) in 18 years. We want to change that.\n\n\nCentral Control. Google become too powerful. It is scary to imagine a future where \neverything\n about \neverybody\n is known and controlled by \nclosed\n AI corporation. Imagine the world where (1) the only country exist, (2) nobody can control its government and (3) everybody should obey the decision of government without any explanation. There should be open, transparent and accessible \nalternative\n with \ndecentralized control\n built on principles of modern distributed interplanetary content-addressable cyberspace [IPFS] and DAO like governance [RALF].\n\n\nAnnoying Ads. Separation for organic and ad search results is unnecessary. In fact, all organic ranking decisions are being made by search authority. But for paid search Google use free market solution to determine a fair ad price for every word in its gigantic semantic core. Historically free market solutions are proven to be more efficient in virtually any area of decision making. Why do not use the same principle for the ranking itself disintermediating annoying ads? Let us imagine that every link can be (1) curated or audited by everybody, (2) based on this trusted metric cyber\u2022rank (page rank based on economically incentivized curation and auditing) is calculated and then (3) everybody can promote this link further by burning some money automatically bringing value for everybody in existence. For every action, everybody earns a share proportionally to contributions. This non-zero-sum game is significantly more Sybil-resistant and that is there we are heading.\n\n\nOne-Way Trust. Everybody use to trust Google, Baidu, and Yandex. But Google, Baidu, and Yandex don't trust users. E.g. you cannot report some kind of proof that given link is a lie and should not be indexed so high. It can count your attention during ranking but can reject to count it. You cannot know what happens inside because Google, Baidu, and Yandex don't trust us. We want to establish a system there trust is bidirectional between the search engine and users because search engine ownership is distributed across all its users based on which all ranking decisions are made.\n\n\nZero Privacy. All search engines will answer you only if they explicitly know how to map your device with your real identity or pseudo-identity which is tracked by RTB [RTB]. Otherwise, you should prove that you are not a robot every time you search. That harm our privacy. Moreover, robot abuse is another hot topic that is about to happen. Nonetheless, nothing should harm our privacy.\n\n\nCensorship. Though it's well known that Google working hard to prevent censorship we all know about China \nGCHINA\n case and \nTransparency Report\n. A good search should be resistant to censorship without exceptions and build for interplanetary scale in mind.\n\n\nOnline only. Worth to note that you cannot search offline even if necessary information is stored next door. If we are cut from the wire or backbone we powerless. Global offline search is not a feature which can be easily deployed even by a multibillion corporation. This goal is nearly impossible to achieve based on centralized architecture. Only accessible distributed systems can solve this fundamental problem for the next generation Internet. This future is not about gateway keepers in form of ISPs but about mesh networking and peer-to-peer communications.\n\n\nWeak Security. What happens if tomorrow my Google account will be blocked? Do we have something to prevent this? Do we have the necessary level of assurance that \nguarantee\n us our security based on math and not on the complicated legal tender? All technical solutions are here but to solve this important issue we need to do a lot of work because security is a foundation for life, liberty, and property.\n\n\n\n\nA pretty huge amount of problems to fix. It would be naive to bootstrap a search engine from a scope where Google, Baidu, and Yandex exist. We need to find a special area where general purpose search engines sucks. A variety of distributed ledgers such as blockchains and tangles can be primary content-addressable data suppliers and this is a scope where current search engines are not the best at work. Moreover, blockchain technology evolves very rapidly and has a lot of promises so it is a sure bet.\n\n\nThe idea is to initially deploy a blockchain based search engine for the purpose of searching against other blockchains so that can be useful from the first day. At the same time, we design the cyber\u2022Fund \napplication\n [CFUND] based on cyber\u2022Chain to solve a problem of trustless realtime blockchain asset valuation which adds some useful capabilities to the search. But we need to design cyber\u2022Chain in a way to be scalable for a more broad definition of a general purpose \nsearch and valuation engine\n, so more applications can emerge. Currently, about 15 trusted (and thousands of not so trusted) and globally available distributed ledgers exist, totaling about 1 billion transactions accumulated. Last year (2016) amount of accumulated blockchain transactions increased tenfold. Not all of them were financial transactions in some sense. E.g. Steem [STM] blockchain and it's Russian sister Golos [GLS] primarily store user generated text such as posts and votes (? votes are text? confusing...). As transactions are the only way of changing states in databases currently used by any web application we foresee distributed ledgers to become the primary source of public information in the 21st century due to tremendous benefits of the technology [ENIGMA, ....].\n\n\nThus we are to declare the \nprinciples\n of a general purpose decentralized and distributed search engine for the upcoming age:\n\n\n\n\nPrivacy and Security. Just it.\n\n\nUbiquitous Ownership and Access. Everybody should have a right to possess a piece of it.\n\n\nMesh networks future proof. It should work in every connected surrounding.\n\n\nInterplanetary scale. It should work on Earth and Mars.\n\n\nTolerant. In the era of machine learning, it should work for any kind of thinking beasts.\n\n\nOpen and Accessible. Everybody should be able to bring a bit to a quality of search results.\n\n\nBlockchain Agnostic. Foundations behind its design should not rely on any protocol or stack rather be explicitly derived from the nature of the information itself.\n\n\nBeautiful. The business model should not harm the user experience.\n\n\nTransparency and Trustfulness. Every piece of its reasoning and behavior must be auditable by everybody.\n\n\nNo Single Point of Failure. Nobody should have a single key to modify or change it.\n\n\nSybil Attacks Resistant. This resistance should be derived from the properties of a free market but not from some single authority.\n\n\nIntelligent. It should answer \nnatural\n questions with \neasy to read\n and \nprovable\n answers no matter text, media or natural numbers should be involved in the answer.\n\n\n\n\nAlongside these principles, there are \nrequirements\n such as performance, usability, and scalability. Keep reading. We discuss everything step by step.\n\n\nDesign Rationale\n\n\nThe idea of a permanent web behind IPFS is beautiful in its simplicity. Every unique \npiece of data\n has the unique address:\n\n\n\n\nUsing this address this piece of data can be found in a global data structure called MerkleDAG [DAG] (logical representation of data storage) across peer-to-peer IPFS network using bulletproof DHT. Nodes are weakly incentivized for fair data exchange using BitSwap protocol. This link can point to \nany\n piece of data such as GIT object, BitTorent link, Bitcoin block, JSON document, picture, video, plain binary data or even a small piece of text:\n\n\n\n\nLet me explain the power of this solution for a search engine:\n\n\nData is unique and self-authenticated\n. If you know this hash is a piece of data you trust, you should not care where it comes from. This property free search engine from the storage of documents.\n\n\nDirect Content Distribution\n. Weak incentivization of the BitSwap protocol has a very interesting side effect: the more popular the file =\n the more people store pieces of it =\n the faster you can get it. This is in contrast with a conventional client-server architecture where the more people want the file =\n the more resources a server needs (and more expensive distribution become) =\n the slower you can get it. This property significantly reduces resource usage to \ndeliver\n results right to a user through distributed network.\n\n\nFlexible Data Discovery\n. Big pieces of data are permanently linkable, thanks to MerkleDAG, so you can trivially reach a small chunk of data. That enable a lot of powerful applications impossible for a peer-to-peer network before. E.g. SQL-like data queries to a database distributed across the network or REST-like queries.\n\n\nThus documents which are located at \n/ipfs\n are \nimmutable\n. But what if we want \nmutability\n? IPFS offer the self-signed naming system which doesn't rely on conventional centralized and slow DNS (URLs we use to use). Everybody can publish a link in a namespace \n/ipns\n with a mutable pointer to any \n/ipfs\n piece of data and sign it with its node's private key:\n\n\n\n\nAfter an owner is able to mutate this link pointer to another /ipfs piece of data. This gives us another important property for advanced search engine:\n\n\nFast Discovery\n. This property free search engine from rescanning work significantly reducing resources necessary for keeping index fresh. Workflow of conventional search index is straightforward: web crawler fetches a page, then follow up links, fetch the following pages and so on. After some loops, web crawler is able to get virtually all links in existence. The information contained in web pages is mutable. Thus search engine should decide then and how to fetch indexed links for new versions of documents. As web pages are loosely structured than it should somehow filter all the noise. In our case publishing is visible in DHT and can be nearly instantly visible for all participants across distributed network without the necessity to continuously send, receive and process HTTP requests. An average request is about 800 bytes long, so do every response [HTTP]. This overhead goes through the wire every time search engine want to know the freshness of the web page and then if the page is new it should crawl all new page even if 1 bit has been changed. Interestingly, though HTTP/2 bring new awesome features it still relies on the plain old location-based addressing and ancient DNS, thus this cannot be faster by design. In our case, we need sniff changes in DHT to know all news from a network. Currently, this is about 100kb/minute in a quite small IPFS network. There is estimation [QGTC] that about 10% of Internet traffic consumption has been made by Google crawlers. It easy to imagine how much overhead can be eliminated across the entire planet. This overhead currently is being paid by businesses around the globe.\n\n\nFlexibility\n. Search results can be not ugly static snippets from 20 century but small dynamic programs \nowned\n by creators. It is hard to overestimate this idea. We will cover this topic further.\n\n\nNo Middleman\n. A search engine should not rely on a DNS middleman and can communicate directly with resource creators.\n\n\nOk. Now we understand that we have a usable way to store, reach and mutate the data across the globe in a more lean way. But what about an ability to \ncomprehend\n what data is behind this meaningless hashes? Interplanetary File System uses a novel multihash, multicodec, multiformat, multibase [\nhttps://github.com/ipld/cid#cidv1\n] formats for Interplanetary Linked Data (it is not a joke either). IPLD is a project aims to define CID that being used across IFPS network. Content IDentifier or CID is a self-describing content-addressed identifier. That creates enormous opportunity for optimization. Imagine a 32-byte link can contain everything to \nindependently\n understand how to reach a piece of data and how to programmatically interpret it. Finally, CID is the thing which makes simplistic design of our proposed search engine possible inside consensus computer.\n\n\nA nice property is that CID are based on well-known cryptography so we don't need to rely on IPFS if something better came. The same addresses can be used to exchange a piece of data in a different peer-to-peer network. Though this doesn't solve a problem of extensibility. CID itself contain version bits, thus if something better came we can switch to it.\n\n\nBut now IPFS and IPLD is a perfect design choice with huge momentum across academia, open source engineering community and (the most important) blockchain engineering teams. It is our belief that it should lay as foundation for the next generation search engine. Everything is perfectly likable (MerkleDAG), fast (DHT), accessible (BitSwap) and comprehendible (IPLD) without a single point of failure. IPFS is a protocol and it can be more useful with a good search engine specifically designed to it. It's possible to compute a PageRank for the whole MerkleDAG. But there are two problems with it:\n\n\n\n\nAn amount of links in MerkleDAG grows \nO(n^2)\n. That is not either conventional web pages with 20-100 links per page. For 1 Mb file can be thousands of links. Once the network starts to take off, complexity inevitably increases.\n\n\nEven if we address some algorithm to extract relevances from these links we should address even more algorithms to extract a meaning.\n\n\n\n\nWhat we need is to find a way to incentivize extraction from this data fog a meaning that is \nrelevant to users queries\n.\n\n\nSearch Workflow\n\n\nOur proposed design of a search engine is based on advanced blockchain technology and enables everybody to participate and be rewarded. Everybody with an account can search. To execute queries user should sign a \nsearch\n transaction with CIDv1 as payload and broadcast it.\n\n\n\n\ncyberd search {CIDv1} hipster true\n[where s the privacy declared before? with every search query signed with account name on a public blockchain? (tomarcafe)]\n\n\n\n\nA document should be a valid CIDv1. Post headers are purposefully unique. The rationale is the following:\n\n\n\n\nstrongly encourage to bring valuable documents first and get a reward\n\n\nsimplify search execution and ranking calculation\n\n\nmake possible a high-performant flat data structure\n\n\n\n\nWorth to note that search request \"buy tesla x\" is also can be represented as CIDv1. Thus search index store queries itself as documents. Hence search queries and target documents both will acquire cyber\u2022rank.\n\n\nThen cyber\u2022chain verifies validity of CIDv1, the correctness of signature, broadband allowance and if all conditions are met either of two things happens:\n\n\n\n\nif CIDv1 is not in search index =\n Write to search index\n\n\nelse broadcast a vote for existing CIDv1 and return sorted links of relevant CIDv1\n\n\n\n\nBased on this data client-side application can deliver documents by 3 ways:\n\n\n\n\nusing full javascript implementation of IPFS [JSIPFS] (fast, but requires initialization)\n\n\nusing REST API provided by IPFS HTTP gateway (depends on, can be fast or not)\n\n\nusing local IPFS node (the fastest)\n\n\n\n\nThis approach is simple and powerful. A developer has enough choices to balance between usability and performance.\n\n\nCIDv1 can mean any piece of data. Different data types that can be returned to a user depending on the query:\n\n\n\n\nPlain text for autocomplete, e.g. ['apple', 'asos', 'amazon']\n\n\nA piece of media content which a user can play without necessity to go somewhere\n\n\nStatic formatted snippet with a link to the conventional web.\n\n\nStatic formatted snippet with a text answer.\n\n\nIPNS link pointing to javascript that can return dynamic snippet.\n\n\n\n\nIt depends on developers (mainly submitting answers) and users (mainly ranking answers) what kind of things they want to answer questions. Possibilities are limited with imagination. Thus we propose a free market for answers on search queries everybody is encouraged to participate. How does it work?\n\n\nInformation about indexed CIDv1 as well as about its rankings is available for everybody. Thus those interested in rewards can monitor the blockchain for semantic core updates and submit links nearly instantly. Everybody can sign \nanswer\n transaction with a link from\n\n\n to \n as payload and broadcast it:\n\n\n\n\nanswer {search CIDv1} {answer CIDv1} hipster true\n\n\n\n\nA document should be a valid CIDv1 and unique outbound link from an answer. So for any given question, the only unique answer is possible. Then cyber\u2022Chain verify the correctness of signature and either of two things happens:\n\n\n\n\nif CIDv1 is not in search index =\n Write to search index\n\n\nif answer CIDv1 has no link to question CIDv1 =\n Write to answer index\n\n\nelse broadcast a vote for existing answer CIDv1\n\n\n\n\n\n\nWe follow black box rule. In order to answer a question right, you need a full comprehension neither the question nor the answer. You just need to match a query with the most relevant links.\n\n\n\n\nIn order to increase the rank, everybody can promote either link or query.\n\n\n\n\npromote_search {CIDv1} hipster 20 true\npromote_answer {search CIDv1} {answer CIDv1} hipster 20 true\n\n\n\n\nEvery sent token for promoting is destroyed thus creating value for every token holder.\n\n\nThat is a core API for the entire blockchain. Other methods accomplish support role for the thing. Such compact design opens huge opportunity for performance optimizations. Also, a clean and comprehensible experience is very important for those who want to be involved. That is. The entire graph of the semantic core with weights is open for everybody and available for data mining or any kind of weird AI stuff. But to make it work we need to find a way to calculate relevance.\n\n\nWe can represent our data structure as directed acyclic graph where vertices are indexed documents and edges are directed links between them.\n\n\n\n\nWe equate terms \ndocument\n / \nquery\n and \nlink\n / \nanswer\n as for our use case these are practically the same. We will stick to \nquery\n and \nanswer\n terms in order to avoid confusion.\n\n\nHence if a user searches a document \nCID 3\n (query) search engine will return links (answers) to \nCID 1\n, \nCID 4\n, \nCID 5\n documents (queries) sorted by cyber\u2022rank. Let us discuss it in details.\n\n\ncyber\u2022rank\n\n\nThe idea is to combine two simple yet powerful algorithms: Google's PageRank and Steem's reward mechanism: \n\n\nWhere \nt_rank\n = \nn_rank\n + \ns_rank\n\n\nn_rank\n, or natural rank is a rank based on Steem reward system. \ns_rank\n, or synthetic is a plain old PageRank used by Google and others.\n\n\nNatural rank is acquired in a process of auditing and curation. Based on this rank payout to those who involved (queries and answers submitting, auditing and curation) a made. Each piece of submitted data gets paid in 7 days. Each piece of data can be voted by cyber\u2022power token holders. We use \nauditing\n term primary for verification of submitted data by automated scripts and \ncuration\n term to denote manual curation. From a technical standpoint, those are primary the same as both utilize the same method for interactions. Implementation of natural rank is almost identical to Steem. Thus details of implementation can be found in our Github [] or Steem whitepaper.\n\n\nSynthetic rank is taken as initial natural rank expressed in \nrshares\n. This makes possible to start calculating PageRank before payouts have started. Conventional search engine work with nearly zero trust data. More than 200 factors used to calculate initial PageRank for a new document in the graph and find relevance to the search terms. Our novel approach allows assigning initial value based on Sybil-resistant voting. It is our belief that proposed approach can significantly simplify ranking and be more precise for information that nature is subjective. Though PageRank calculation is a trivial task we should estimate the feasibility of doing so in a consensus computer such as Steem where scalability is limited with a less performant node and parallel processing [Steem Roadmap] or sharding is yet to be discovered [Ethereum Mauve].\n\n\nA recent study [http://www.vldb.org/pvldb/vol8/p1804-ching.pdf] shows that Facebook scale PageRank computation is doable (using Java based Giraph) for 1 trillion edges and 1.4B vertices in 600 minutes per iteration. Hence consensus computer made of commodity hardware will be able to process 1 iteration per 2 days for a 10B unique document (SWAG for all blockchains + Git + BitTorrent (https://arxiv.org/pdf/1009.3681.pdf) + IPFS). Our implementation is based on C++ thus can be more performant though is not guaranteed. Also, we have an opportunity to use the most performant GPUs available operated by witnesses and not that is being used by cloud providers [http://www.hipc.org/hipc2011/studsym-papers/1569513051.pdf]. Anyway, our estimation proves that it is practically enough to use CPU for proof-of-concept stage. GPU implementation can multiply computation capacity up to 1000x. But further research in the field of parallel consensus computing is necessary to achieve Google scale (10000x more documents) realtime decentralized computation of cyber\u2022rank.\n\n\nOur model is recursive and requires the enormous amount of calculations which are limited within blockchain design. Model recalculation does not happen on a periodic basis rather it continuous. We consider introducing consensus variable, in addition to a block size, in order to target processing capacity of the network. Let's call it a \ncomputing target of documents per block\n or CTD. Any witness will be able to set a number of documents the network should recompute every block. The blockchain takes as input computing target of legitimate witnesses and computes CTD as daily moving average. Based on CTD blockchain can schedule the range of CIDs that should be recomputed by every witness per round.\n\n\nThe semantic core is open. Click-through information is stored on-chain. Every time a user follow a link positive voting transaction is broadcasted e.g. with grade 1. Voting on a protocol level is a number in a range from -100 to 100. Thus application developers have a tool to implement different grades for different kind of interactions. Such design is crucial to train the model and acquire a data about search popularity of semantic core and its volume. Currently, search engines are very careful in revealing this information because this information is the most important part of the ranking. We want to change that. Every time a user click on a snippet developer earn a fair portion of emission and on chain model is trained. Application acquires the more rank the more rank acquired by its links. The more cyber\u2022rank acquired - the more revenue streams for an application developer.\n\n\nBoth algorithms have strong proof in form of Google's $550 B capitalizations in 18 years and Steem $40 M capitalization in 9 months. Combining both it is possible to empower the world with a new kind of search quality that has been (1) designed to index relevant document fast and (2) has inherent Sybil protection.\n\n\nSelf Indexing Dilemma\n\n\nProposed approach has very unexpected limitation. What if we want to index cyber\u2022chain using cyber\u2022chain itself? Let us say that we have an awesome transaction that happens inside the chain and everybody are talking about it. It is popular thus we should display it in our search results. Adding it to an index spawn another transaction which (surprise) also should be indexed. This entanglement creates an infinite loop that bloat cyber\u2022chain. This can not be a problem either. Consensus computer capacity and power are limited by the market forces. So we have two possible decisions:\n\n\n\n\nLet it be. The market is a king. A bit of bloat can be a good piece of a knowledge about itself.\n\n\nStrictly forbid indexing of the blockchain itself. Fortunately, it is not so hard to implement on a consensus level. All we need is to check that CID has not been included in a cyber\u2022chain before. That mean that cyber\u2022chain transaction itself remain unindexed because in order to achieve this we (1) either should mutate data for hashed and timestamped transactions, (2) nor create a possibility for a self-bloat. None of the options is valid. Again, fortunately, direct search without ranking among internal cyber\u2022chain transactions can be available inside search results with the help of CID magic.\n\n\n\n\nIt is our belief it is better to stick to restrictive policy without further research.\n\n\nChallenges and Advantages of Indexing Distributed Ledgers\n\n\nConventional general purpose search engines were built on the \nlast mile assumption\n of stateless HTTP(S) protocol. Indeed the last mile approach worked well on the Internet where information emerges inside stateful private databases and only after become publicly available using HTTP. The emergence of content-addressable systems such as Git and BitTorrent didn't change much as those can not be compared with stateful private databases in any sense even though these protocols are represented origin of a content. But with an emergence of distributed ledger technology become possible to get know about public content in the moment than it actually has been born. In this sense, blockchains and tangles can be viewed as a real alternative to conventional private databases. That breakthrough create enormous opportunity for better and faster indexing but at the same time has inherent problems solving of which create advantages:\n\n\nChain Validation\n. Protocol diversity is the hardest part of solving an issue of chain validation using another chain because solving this requires full implementation of one consensus computer inside another consensus computer. There are two efforts exist that try to solve issues of validating one chain using another. Among them are \nPolkadot\n and \nCosmos\n. Both projects aim to solve a problem of trestles inter blockchain communications. Polkadot aims to design a system which doesn't require a trust to parachains (or interconnected chains). The design of Polkadot is very complex and has inherent scalability limitation. The design of Cosmos is much more simple, but require a trust to zones (interconnected chains). Our design doesn't try to solve a problem of inter blockchain communications rather try to implement probabilistic search across blockchains. Thus it is significantly more simple. Instead of recording information that has been verified from point of view of exogenous protocol we let this information come to index letting market forces and relevance algorithm determine which chains are correct and which are not.\n\n\nMeaning Extraction\n. Practically there are no common fields that are used by all blockchains and tangles. The only common pattern is \ntx_id\n. There is no way to extract any meaningful information from \ntx_id\n. That is practically mean that there is no easy solution exist. For every protocol can be a large number of different approaches to extract useful information. Using advanced machine learning is infeasible for consensus computers at the moment. We offer set of smart contracts that create a free market for extraction of the meaning.\n\n\nThe huge advantage of blockchains that they are implicitly told what happens. Blockchains are highly optimized databases. Every transaction cost money so they are neutrally protected from spam and contain only data that really matter. That reduce noise filtering to nearly zero level. Structured raw data about blocks and transaction is available for everybody. This fact also reduces consumption of resources and make extraction of better meaning possible.\n\n\n\n\nBlockchains provide real-time high-quality structured data which don't require \ncrawling\n in a traditional sense. One node of any given blockchain is enough to provide verified data about transactions within one ledger without the necessity to continuously revisit resources significantly reducing costs.\n\n\nAll these factors create a free market opportunity for emergence the diverse set of highly specialized (on a very limited set of a semantic core) but highly efficient \nbroadcasters\n.\n\n\nProbabilistic Settlement\n. Blockchain designs, especially Proof-of-Work based, implies that finality of a transaction is probabilistic. A moment than a given fact can be considered as truth is blurred. \nPrevious researches\n show that it is expensive to achieve real-time blockchain indexing due to reorganization issues. The intention of discussed in the article design is to answer the question deterministically. Our architecture based on the principle that indexing a system with probabilistic settlement require probabilistic answering. Instead of deciding whether or not this particular block has been included in the canonical chain we can index all of them calculating the probability of finalization using cyber\u2022Rank.\n\n\nSo solve of discussed issued of probabilistic indexing we propose a flexible approach we call lazy oracles.\n\n\nLazy Oracles\n\n\nOne specific ability is crucial for the next generation search engine. Application developers should have a motivation to provide structured arbitrary data feeds. Thus search engine can answer natural questions aggregating data from \nhighly structured and defined\n feeds. This makes possible a user get high-quality \ncalculated\n answers in real-time about the state of reality expressed not only in links (which intelligent agent don't know how to parse) but in actionable numbers based on which it's possible to make independent economic decisions. It is hard to find a tool to agree on publicly available and continuously evolving facts. We propose an approach to solving this.\n\n\nToday different blockchain have the functionality necessary to implement this. For eg. Ethereum enable construction of smart contracts that can validate and incentivize data feeds. But Ethereum has the strong limitation: a price. Due to a network design, every operation should be validated across the network of 5k nodes. For every put operation developer of such contract should pay in hope that somebody in the future will use this feed in the future returning costs. The current cost of a permanent storage inside Ethereum contracts is around $200/megabyte. Worth to note that Ethereum has consensus variable gas limit. Currently, a network load is around 10% of established limit. Once the demand for computation reaches a limit we will have a situation very similar to Bitcoin block size debate and price for storage can reach $2000k/megabyte easily without validation costs. Pretty expensive for unlimited possibilities. There is the alternative - Factom [FACT]. Its consensus design relies on a small amount of paid servers. Thus the cost is around $1/megabyte. Such low price comes with high limitations. You can only put data to Factom and read it. There is no validation and incentivization built-in. There are permissioned blockchain designs such as BigChainDB [BCDB] and Hyperledger [HYPL] which solves validation problem perfectly but require strong efforts for developers to program and establish a network and then somehow monetize it. Lazy Oracles are going to fill this gap providing robust, cheap and reliable way \nfor monetizing\n structured public data. Any cyber\u2022chain account has a share in a broadband depending on its cyber\u2022Power thus granting lifetime assurance of network usage. A network programmed with decaying inflation a part of which goes to all who participate in indexing depending on the valuation of subjective contributions.\n\n\nThe process consist of 5 steps:\n\n\nStep 1: Everybody can declare a soft protocol for data feed by posting a CID with the inbound link to \noracle-defenition\n CID pointing to the document with the following structure:\n\n\n// Basic Validation\ndoc_type: market_update // should be fixed\n  exchange: string // domain name of the exchange\n  base: string // link to a definition of a referenced namespace e.g. ISO or chaingear\n  quote: string // link to a definition of a referenced namespace e.g. ISO or chaingear\n  price: number\n  volume: number\n// Audit Rules\nDocument format and data types should match a protocol\nFor every unique exchange and pair `market_update` report can be submitted no more frequently than once per minute.\nIf either a price or volume for unique exchange and pair has not been changed more than 0.1% a report should not be submitted.\n// Inbound Links\n\noracle\n \nmarket-update\n \nexchange\n \nbase\n \nquote\n\n// Reference Crawler Implementation\nCID\n// Reference Auditing Implementation\nCID\n\n\n\n\n\nProtocol declaration should be agnostic from language implementation and unambiguous.\n\n\nStep 2: Now every reporter can submit data according to a soft protocol. If data begin to be submitted without protocol definition a value of this data can be significantly lower. Thus auditors are encouraged to flag such documents. If false or malicious data come auditors have strong incentive to flag such data. All lazy oracles should be feed with self-descriptive links so other participants will able to faster process auditing.\n\n\nStep 3: Auditors validate any given document by scripts.\n\n\nAs result reporters, auditors (where objective script-based validation is possible) and curators (where subjective human-made evaluation is more appropriate) have strong incentive to (1) bring important data, (2) curate and audit data conscientiously. Proposed approach doesn't have strong guaranty of the truthfulness of data such as Augur does. Rather it is correct to say that we can have strong assurance that data is correct. But it is cheaper, faster and significantly more flexible. The process is robust in its unstructured simplicity.\n\n\nStep 4: Payouts for auditors are made. Payouts information is input information for cyber\u2022rank. Using cyber\u2022rank it is trivial to filter feeds using plugins to give significantly more precise data with a higher level of assurance.\n\n\nStep 5: High-quality data feeds (or lazy oracles) are available for a consensus engine of a search engine (thus cyber\u2022rank can be continuously improved) and everybody on the planet.\n\n\nWe call this type of oracles lazy because they don't require strict rules of validation at the expense of reducing the level of accuracy (but still enough to reason with a high level of assurance). Also, they are lazy because the don't require to think about monetization for participants rather consensus engine print rewards based on a valuation of subjective contributions. This approach is superior to Ledgys [] than reporters should sell encrypted data pieces using costly Ethereum storage.\n\n\nThe most obvious use cases for Lazy Oracles\n\n\n\n\nBlock indexing\n\n\nTransaction indexing\n\n\nBalance calculation\n\n\nIdentity crawling\n\n\nToken valuation\n\n\nToken rating\n\n\nToken description\n\n\nICOs tracking\n\n\nWeather measuring\n\n\nTraffic measuring\n\n\nBusiness performance reporting\n\n\n\n\nWorth to note that use cases are not limited to mentioned above. Data structure and validation rules are arbitrary. So we can think of it as general purpose tool for \npopular structured public data\n auditing and curation. Now we can bootstrap and index with amounts of useful information. But what about meaningful search results?\n\n\nDynamic Snippets\n\n\nThis can be thought as serverless micro javascript(is javascript futureproof? ;)) applications that can dynamically take input data from the following sources:\n\n\n\n\na search query itself\n\n\nasynchronously from background search query\n\n\nfrom a browser APIs\n\n\nfrom device sensory information\n\n\nfrom information about a user stored on cyber\u2022Chain\n\n\nfrom other blockchains\n\n\nfrom IPFS and IPNS\n\n\nfrom conventional HTTPS or WebSocket APIs.\n\n\n\n\nEvery application is CID written to search index and answer index. Before developing an application developer should target it for a specific semantic core before defining it. Semantic core and its statistic are publicity available in a blockchain. Developer shall need to develop an application and submit links to an application using either immutable IPFS documents or mutable IPNS pointers for a targeted semantic core. Thus it is up to a developer to define what search queries are a better fit for a particular application. Keep in mind that dynamic snippets are naturally competing for a higher position in search results. Dynamic snippets can be sorted by cyber\u2022rank, and as result become trustful. Worth to note that developers can significantly reduce spendings on app infrastructure as dynamic snippets can be delivered through content-addressable distributed network. Mutable IPNS pointers allow developing snippets for a targeted semantic core and not for every unique query. An implication of this approach is hard to overestimate. E.g. dynamic snippets combined with blockchain wallet make possible to shop right from search results.\n\n\nThe only potential problem with proposed approach is the safety of third party javascript code. Sandboxed third party code is able to mitigate this risks. Web technologies such as \nweb workers\n and \nweb components\n are being actively developed. E.g. javascript library \njailed\n [https://github.com/asvd/jailed] is able to do exactly what we need. Further adoption of web components is also one of a possible solution to do that safely.\n\n\nDealing with Long Tail\n\n\nIt is well-known fact that every day Google receive up to 20% of new search queries never seen before. Thus we need to find a way to deal with it. Here are 3 popular use cases with simple solutions (surely non-exhaustive):\n- \nMisspellings\n. These queries contribute fair half to ever-growing unique query set. But it is not a rocket science to stem such queries client side without any indexed knowledge. After a user clicks on client side suggestion correct link can be submitted to cyber\u2022chain thus improving the global model.\n- \nPhrases\n. E.g. \nforest gump imdb\n. These queries can be a combination of well-known terms and contribute another half to ever-growing unique query set. Even if we get relevant links from indexed \nforest\n, \ngump\n and \nimdb\n separately we would not able to combine answer to return meaningful movie rating to a user. But we can find \nthe closest documents\n between \nimdb\n, \nforest\n and \ngump\n and sort them by relevance. Those likely be the most relevant answers. This simple method can significantly increase efficiency for long tail queries without rocket science. As API is open for everybody there is no limits on using advanced technics.\n- \nUnique queries\n. These are about 10% of never-seen-before queries. We expect that the market of linking will create a segment for on-demand answers. We remember that any query can be seen in memory pool by every network participant. Thus opportunity to earn can create a healthy and competitive market for an on-demand answer in an environment without technical limitations.\n\n\nSpam Protection\n\n\nIn the center of spam protection system is an assumption that write operations can be executed only by those who have vested interest in the success of the search engine. Every 1% of stake in search engine gives the ability to use 1% of possible network broadband. As nobody uses all possessed broadband we use fractional reserves while limiting broadband like ISPs do. Details of an approach can be found in a Steem white paper.\n\n\nAuditing and curation are based on Steem reward mechanism. It is Sybil-resistant approach as votes are quadratic based on principle 1 token in system = 1 vote. In order to vote one should vest in shares for at least for 20 weeks. That solve a problem entirely because those who have a right to vote are strongly incentivized in a growth of his wealth. In order to prevent abuse of auditing and curation voting power decay implemented exactly as in Steem.\n\n\nApplications\n\n\nIt is hard to imagine what kind of applications can be built on top of proposed foundation. I'd like to mention some outstanding opportunities which can be build using cyber\u2022Chain and IPFS:\n\n\n\n\nRelevance everywhere\n\n\nBlockchain browser\n_ Multi-protocol wallets\n\n\nOffline search\n\n\nSmart command tools\n\n\nAutonomous robots\n\n\nLanguage convergence\n\n\n\n\nRelevance Everywhere\n. Proposed approach enable social, geo, money or anything aware search inside any application. It is trivial to implement a search relevant to a particular identity using proposed algorithm. The more a user train a model the more behavioral data can be associated with her. This personalized information can be stored locally for (1) faster retrieval and (2) offline access.\n\n\nBlockchain browser\n. It easy to imagine the emergence of a full-blown blockchain browser. Currently, there are several efforts for developing browsers around blockchains and distributed tech. Among them are Beaker, Mist and Brave .. All of them suffer from very limited functionality. Our developments can be useful for teams who are developing such tools.\n\n\nMulti-protocol wallets\n. Currently, there are several efforts for developing easy to use the universal wallet for blockchain assets. Jaxx and Exodus are among them. Developers of such applications suffer from a diversity of protocols around blockchain tech. There is no fully functional multi-asset wallet yet. Our developments can help teams who are developing such tools.\n\n\nActions in search\n. Proposed design enable native support for blockchain asset related activity. It is possible to design applications which are (1) owned by creators, (2) appear right in search results and (3) allow a transact-able call to actions with (4) provable attribution of a conversion to search query. e-Commerce has never been so easy for everybody.\n\n\nOffline search\n. IPFS make possible easy retrieval of documents from surroundings without the internet connection. cyber\u2022Chain itself can be distributed using IPFS. That create a possibility for ubiquitous offline search.\n\n\nSmart Command Tools\n. Command line tools can rely on relevant and structured answers from a search engine. That practically means that the following CLI tool is possible to implement\n\n\n  mylovelybot earn using hdd -100GB\n\nsearching for opportunities:\ncyberd search \nearn using hdd\n\n\nThe following answers received:\n- apt install siad /// 0.0001 btc per month per GB\n- apt install storjd /// 0.00008 btc per month per GB\n- apt install filecoind /// 0.00006 btc per month\n...\n\nMade a decision try `apt install siad`\nGit clone ...\nBuilding siad\nStarting siad\nCreating wallet using your standard seed\nYou address is ....\nPlacing bids ...\nWaiting for incoming storage requests ...\n\n\n\n\n\nSearch from CLI tools will inevitably create a highly competitive market of a dedicated semantic core for bots.\n\n\nAutonomous robots\n.\nBlockchain technology enables creation of devices which are able to earn, store, spend and invest digital assets by themselves.\n\n\n\n\nIf a robot can earn, store, spend and invest she can do everything you can do\n\n\n\n\nWhat is needed is simple yet powerful API about the state of reality evaluated in transact-able assets. Our solution offers minimalistic but continuously self-improving API that provides necessary tools for programming economically rational robots.\n\n\nLanguage convergence\n. A programmer should not care about what language do the user use. We don't need to have knowledge of what language user is searching in. Entire UTF-8 spectrum is at work. A semantic core is open so competition for answering can become distributed across different domain-specific areas, including semantic cores of different languages. The unified approach creates an opportunity for cyber\u2022Bahasa. Since the Internet, we observe a process of rapid language convergence. We use more truly global words across the entire planet independently of our nationality, language and race, Name the Internet. The dream of truly global language is hard to deploy because it is hard to agree on what mean what. But we have tools to make that dream come true. It is not hard to predict that the shorter a word the more it's cyber\u2022rank will be. Global publicly available list of symbols, words and phrases sorted by cyber\u2022rank with corresponding links provided by cyber\u2022chain can be the foundation for the emergence of truly global language everybody can accept. Recent scientific advances in machine translation [GNMT] are breathtaking but meaningless for those who wish to apply them without Google scale trained model. Proposed cyber\u2022rank semantic core offers exactly this.\n\n\nThis is sure not the exhaustive list of possible applications but very exciting, though.\n\n\nIncentive Structure and Distribution Mechanism\n\n\nTo make cyber\u2022rank economically resistant to Sybil attack and to incentivize all participant for rational behavior a system uses 3 types of tokens: CYBER (or cybers), CP (or cyber\u2022power) and CD (cyber\u2022dollar)\n\n\nCYBER is a transferable equity token which is analog of STEEM. The intrinsic value of CYBER came from the ability to convert it to CP.\n\n\nCP is a non-transferrable equity token which is analog of SP in Steem. CP can be converted to CYBER in 20 weeks using proportional weekly payments. The intrinsic value of CP came from the right to (1) write to an index according to a bandwidth limit, (2) rank objects, (3) promote objects (4) make consensus decisions. CP can be converted to CYBER in one year.\n\n\nCD is a debt token with a relatively stable value which came from an ability to convert it into CYBER within 3 days by the price submitted by witnesses and calculated according to cyber\u2022rating methodology [] (don't confuse). 1 CD tracks 1/10^12 of \nprovable\n blockchain economy.\n\n\nReward Pool is defined as 100% of emission and split among the following groups:\n\n\nInfrastructure Reward Pool\n\n\n\n\nWitnesses - 5%\n\n\nInvestors - 10%\n\n\n\n\nIndexing Reward Pool - 30%\n\n\n\n\nReporters - 10%\n\n\nAuditors - 20%\n\n\n\n\nLinking Reward Pool - 65%\n\n\n\n\nResponders ~ 20%\n\n\nTrainers ~ 40%\n\n\n\n\nOur implementation also offers an incentive for CD holders. They receive APR on holding according to rate defined by witnesses.\n\n\nAs our primary focus is developers of decentralized and distributed applications we offer a convenient way of revenue generation in addition to possible revenue on dynamic snippets development. Every search and answer transaction can define a smart contract with up to 6 beneficiaries with a distribution of potential reward from this particular action. This creates a solid foundation for bootstrapping application ecosystem across a conventional web, mobile app stores and upcoming VR platforms.\n\n\nA conventional advertiser is also has a tool to participate. Any link can be promoted thus increasing cyber\u2022rank. Revenue from the promotion is burning thus decreasing the supply of CYBER and bringing value to all shareholders of a system. We do believe that this approach is fair and don't harm user experience but open opportunity to compete with conventional search ad a bit.\n\n\nVirtual loop of the business models for our decentralized autonomous organization is pretty simple: \nMore indexing =\n More people search =\n More developers build =\n More people earn, rank and promote =\n Better infrastructure =\n More indexing\n.\n\n\nSince inception, a network prints 3 CYBER every block. Every 1 million blocks it reduces print rate on 1%. Thus starting from ~35% print rate per year inflation begin to reduce gradually until it reaches 1%.\n\n\nThere is one problem with proposed incentive structure. We call it \nlanguage incentivization bias\n. In the core of cyber\u2022chain is quadratic voting. The system needs it to effectively incentivize participants for quality ranking. But that natively leads to weak incentives across different language groups. E.g blockchain Golos was deployed as Russian alternative to Steem because Russian posts acquired only 0.2% of rewards though providing 10% of the content. The idea of deploying cyber\u2022Chain is great if it can be truly global from the start. The only way to overcome this bias is a global deployment from day 0, because otherwise we need significantly increase the complexity of the reward system. We offer good incentives for translation of this white paper to 50 languages worldwide as well as call for action to all blockchain communities across the globe.\n\n\nExtensibility and Governance\n\n\nCurrently, our implementation has the following functionality available for application developers.\n\n\n\n\nCustom Operations. Better alternative for OP-RETURN\n\n\nPlugins. Allow implementing API based on custom operations.\n\n\nEscrow. The core smart-contract enables 3d party arbitrage for arbitrary transactions.\n\n\nPrivate Messaging. Enable private communications between accounts.\n\n\nDynamic Account Permissions. You can think about it as better multi-sig.\n\n\n\n\nThe following possibilities can be available in a distant future:\n\n\n\n\nSidechains\n\n\nState channels\n\n\nPermissionless smart contracts\n\n\n\n\nConsensus can be changed in case of 17 of 21 elected delegates accept a hard fork.\n\n\nSearch and Evaluation Appliance\n\n\nFor bootstrapping a network we are going to offer software configuration (Cybernode - Github) on top of well tested open source specs for hardware configuration of commodity computer (Enterprise - Github) which cost around $10k-$30k depending on RAM and SSD capacity and is able to participate and earn by itself executing different network tasks:\n- operate as witness node\n- operate as indexer and auditor\n- operate as answering node\n- operate as fast and cheap backend for decentralized and distributed application\n\n\nWe need a network of high performant computers in order to achieve our goals. Necessity comes from  the following assumptions:\n- all blockchain nodes and IPFS should live inside one machine to remove slow network communications from all middleware. Vast information for processing can be in memory.\nCommunications inside one bus enable to execute required tasks significantly faster [[https://gist.github.com/jboner/2841832]]. Thus we can achieve nearly live indexing of reality from the very beginning.\n- extension with GPU. Currently, data centers cannot compete with commodity GPU. E.g. Amazon offers very expensing professional Nvidia Tesla cards. For our purposes, commodity cards such as GTX 1080 are much more cost effective.\n\n\nEnterprise\n. Currently, it is not hard to assemble a 2 CPU computer with 1 TB of RAM and 40 TB of SSD using commodity hardware. Such appliance can cost about $30k so we can think of it as affordable for those who are seriously want to be involved in a project. Also, we have an option to extend the capability of proposed search appliance based on 2 CPU motherboards built on Intel C612 chipset. Usually, it has 7 PCI-E slots for GPU which can be dedicated for cyber\u2022rank calculation. Thus a price for an ultimate (2 CPU * Xeon E5 * 22 cores + 7 GPU * GTX 1080 * 2560 Cuda cores) search and evaluation appliance can be around $50k. Currently, such computer will be able to process, index, audit and linking all blockchains.\n\n\nCybernode\n. We implementing the following software configuration that is based on docker containers.\n\n\n\n\nCybernode allows everybody fast deployment of decentralized and distributed application powered with cyber\u2022chain search capabilities.\n\n\nPerformance and Scalability\n\n\nProposed blockchain design is based on DPOS consensus algorithm and has fast and predictable 3 seconds block confirmation time and 1 minute finality time. Average confirmation timeframe is 1,5 seconds thus conformations can be asynchronous and nearly invisible for users. A good thing is that users don't need confirmations at all before getting search response as there is no risk associated with that.\n\n\nCurrent node implementation theoretically [https://bitshares.org/blog/2015/06/08/measuring-performance/] can process about 100k transactions per second. This theoretical bound is primarily limited with the possibility to replay a blockchain [https://steemit.com/blockchain/@dantheman/how-to-process-100m-transfers-second-on-a-single-blockchain]. As of now, all blockchains are about 1B immutable documents which size is about 200 GB with average tx 200 kb. We need to store all hashes which are on average 64 bytes long. We estimated that storing in the index all blockchain documents as CIDs and votes are roughly the same as storing all raw blockchain data. Linking 1B documents create significant overhead as blockchain index size can be up to 100 times more. Given this, we can assume that indexing all existing blockchains require about 4TB of SSD space. This is affordable for commodity hardware with 10x scaling capability without a necessity for sharding across several machines. We assume this is enough scalability margin for proof-of-concept.\n\n\nInitial indexing of 1B documents and 100B links will require a continuous load of the network at the upper bound of its capacity in the first year of its existence. If we assume that network will be able to process 10k transactions per second with 2MB block size we will be able to index all blockchains in 4 months. Further operations will require significantly less capacity as currently, not more than 1000 transactions per second happen among all blockchains.\n\n\nBased on the proposed search appliance we estimate that participants will require investing around $1M for dedicated hardware (21 witnesses) and the same amount for backup nodes. Thus overall costs of hardware network infrastructure can be around $2M. after full deployment.\n\n\nWorth to note that the network doesn't require ultimate configuration at the start and is able to optimize initial investments by the costs of time to index all blockchains. Thus costs at launch can be around $200k. Given that mining industry has been rapidly developed last years this can not be a showstopper for a project. We expect huge interest from miners as slots are limited with 21 fully paid nodes and ~20 of partially paid nodes (depend on the market).\n\n\nPossible scalability improvements include:\n- Hardware. This year Intel Optane [http://www.intel.com/content/www/us/en/architecture-and-technology/intel-optane-technology.html] That creates an opportunity to converge RAM and SSD. Our design has 3-year hardware margin for moving to cheaper and more dense next generation memory.\n- Software. The future of consensus computer optimization is in parallel processing. We are going to seriously invest in research of this field. Solving this issue will enable the network to scale nearly infinitely.\n\n\nDeployment\n\n\nWe split a process of network deployment into the following milestones which are not bounded to any timeframe at the moment:\n\n\n\n\nExploration Phase\n\n\na white paper published\n\n\nCID verification on consensus level\n\n\ncyber\u2022rank implementation\n\n\ncyber\u2022node release\n\n\n\n\nThe purpose of seed stage is to implement the blockchain and prepare it for technical launch.\n\n\n\n\nValidation Phase\n\n\nBlockchain launched\n\n\ncyber\u2022Fund basic application release\n\n\nToken distribution\n\n\n\n\nThe purpose of validation phase is to verify the feasibility of an idea and prospects of technological design across blockchain community. Successful deployment of MVP in the form of basic technical infrastructure in a decentralized fashion and quality of support from the blockchain community and investors around idea will be enough to understand what kind of future the blockchain has. Objective metric is an amount of bitcoins raised during crowd sale for already working blockchain design.\n\n\n\n\nBuild Phase\n\n\nIndexing 10 blockchains\n\n\nIndexing 1000 market pairs\n\n\nEvaluating 1000 tokens\n\n\nGPU cyber\u2022rank calculation implemented\n\n\nHistorical records of balance valuations is available for indexed blockchain\n\n\nDevelopers run 10 experiments\n\n\nFirst payouts to indexers and auditors\n\n\n\n\nThe purpose of build phase is to reach a very \nbasic\n product/market fit around \none specific use case\n or this use case will emerge from experiments. A number of payouts which will be calculated based on current capitalization is objective metric. 6 month is expected the duration of phase. If build phase will be successful there are infinite opportunities ahead.\n\n\n\n\nScaling Phase\n\n\nIndexing all blockchains\n\n\nIndexing Git, BitTorent, IPFS and DAT\n\n\nIndexing 10 blockchains\n\n\nAutocomplete is fully functional\n\n\nTop 1 mln. search queries return useful answers\n\n\n\n\nThis is an infinite phase in which the network start continuously grow indexing more and more relevant and meaningful data and the most important answering questions better and for the better. A key scope of work during this stage is to continuously improve developers experience:\n\n\n\n\nMore indexing =\n More people search =\n *More developers build* =\n More people earn, rank and promote =\n Better infrastructure =\n More indexing\n.\n\n\n\n\nThe power of cyber\u2022Chain\n\n\nThe key purpose of our proposed design is not just replicate abilities of existing search engines which return only links but enable answering new class of question:\n\n\n\n\nHow much value of X do I possess now?\n\n\nWhat probability of event Y?\n\n\nWhat packages do I need to install in order to improve ROI on available resources?\n\n\n\n\nOur proposed design has all necessary components to bootstrap a market for a new generation of answer applications.\n\n\nProposed economics model disintermediate conventional ad model there users are sold to an advertiser and enable any business or people or robot benefit from pure peer-to-peer interactions which bring value for every involved participant.\n\n\nFree Market of Indexing and Auditing\n. Everybody can connect any blockchain or content-addressable protocol. A decentralized approach to indexing and auditing create an opportunity for those who want to earn on the contributions to cyber\u2022Chain. Proposed solution is not more than a way to \noutsource\n these complicated and unstructured efforts for the entire community.\n\n\nFree Market of Answering. After all, we have recent advances in machine learning enable to reason about a piece of data quite well. All these algorithms require enormous highly distributed computation which as nearly impossible to achieve in a trestles consensus computer. With the current state of blockchain technology implementing these algorithms using decentralized computational network seems unfeasible. We find a way to _outsource\n this computation for the entire community.\n\n\nSelf Hosted Search API\n. Everybody can deploy self-hosted API. In comparison with what Google offer ($5 per 1000 answers). Our solution can be much more cost effective for high performant applications. One node can process at least 10k queries per second in a read-only mode. That is about 1B requests per month. That is about 100 times cheaper ($0,05 per 1000 answers) even if payback period of search and evaluation appliance ($50k) will be one month. In reality, the affordable payback period is about 10 months. Thus self-hosted search, in theory, can be 1000x more cost effective than Google offering.\n\n\nConclusion\n\n\nWe describe and implement a motivated blockchain based search engine for the permanent web. A search engine is based on the content-addressable peer-to-peer paradigm and uses IPFS as a foundation. IPFS provide significant benefits in terms of resources consumption. CIDs as a primary object is robust in its simplicity. For every CID cyber\u2022rank is computed by a consensus computer with no single point of failure. cyber\u2022rank is a combination of Google's PageRank and Steem's rewards system. cyber\u2022Rank is resistant to Sybil attacks and is computed based on interactions with a graph of CIDs and it's internal relations. Embedded smart contracts offer fair compensations for those who participate in indexing, linking, auditing and curation process. The primary goal is indexing of peer-to-peer systems with self-authenticated data either stateless, such as IFSS, DAT, GIT, BitTorent, or stateful such as Bitcoin, Ethereum and other blockchains and tangles. Proposed market of linking offers necessary incentives for outsourcing computing part responsible for finding meaningful relations between objects. The proposed market of curation and auditing creates essential incentives for ranking high-quality links and objects. Dynamic snippets in search results make possible functionality necessary for the next generation search. Lazy oracles enable indexing of structured publicly verifiable data feeds in a highly competitive environment. A source code of a search engine is open source. Every bit of data accumulated by a blockchain is available for everybody for free. The performance of proposed software-hardware implementation is sufficient for seamless user interactions. Scalability of proposed implementation is enough to index all self-authenticated data that exist today. The blockchain is managed by a decentralized autonomous organization which functions under DPOS consensus algorithm. Thought a system provide necessary utility to offer an alternative for conventional search engines it is not limited to this use case either. The system is extendable for numerous applications and makes possible to design economically rational self-owned robots to spawn a market for AI outsourcing.\n\n\nReferences - not ready yet\n\n\n[QGTC] \nhttps://www.quora.com/How-many-pages-is-Google-crawling-every-day\n PR [] RALF \nhttp://merkle.com/papers/DAOdemocracyDraft.pdf\n\n\nENIGMA \nhttps://www2.deloitte.com/content/dam/Deloitte/uk/Documents/Innovation/deloitte-uk-blockchain-full-report.pdf\n\n\nRTB \nhttps://en.wikipedia.org/wiki/Real-time_bidding\n\n\nTODO\n\n\nAuditing and Curation\n. Probably need more details\n\n\nAnonymity\n. Explain an economic difference between read search queries and write search queries.", 
            "title": "Motivated Search and Evaluation Engine for Permanent Web"
        }, 
        {
            "location": "/cyberd/cyberd/#motivated-search-and-evaluation-engine-for-permanent-web", 
            "text": "Dima Starodubcev  Draft v 0.1  January-February 2017, Bali  Note: Not updated according to cyberminds mindmap  Simplified", 
            "title": "Motivated Search and Evaluation Engine for Permanent Web"
        }, 
        {
            "location": "/cyberd/cyberd/#abstract", 
            "text": "Existing general purpose search engines are restrictive centralized databases everybody forced to trust. These search engines were designed primarily for client-server architecture based on DNS, HTTP, and IP protocols. The emergence of content-addressable storage and distributed ledger technology creates an opportunity for the Internet to acquire new features such as more efficient computing, storing and broadband consumption, more resilient, secure and private access, no middleman for digital property. This can shift existing web's ubiquitously used client-server architecture to truly peer-to-peer interactions based on stateless IPFS and variety of stateful consensus computers such as Ethereum. This creates a challenge and opportunity for a search engine based on emerging technologies and specifically designed for them. Surprisingly the blockchain architecture itself allows organizing general purpose search engine in a way inaccessible for previous architectures. In this paper, we discuss opportunities behind blockchain based search engine, challenges of crawling, indexing and evaluation for the next generation web and propose a blockchain based experimental set of smart contracts to address discussed issues.", 
            "title": "Abstract"
        }, 
        {
            "location": "/cyberd/cyberd/#introduction", 
            "text": "Let us start a discussion from disadvantages of conventional general purpose search engines:   No Transparency. Nobody outside of Google understands how the ranking really works. This creates a market for black and white SEO. The truth is that if e.g. Google discloses complete details of the ranking algorithm it would be easy for adversaries to game organic search results that kill the quality of results and ad revenue streams. Pagerank [PR] has no inherent trust mechanism resistant to Sybil attacks. This problem can be addressed adding transparent and accountable blockchain based ledger with properly designed economic incentives built into the system.  No Access. Currently, all search engines are centralized. Nobody is able to add to index as well as participate in improving the quality of search results. However, Google itself internally uses a workforce of  search evaluators . It is our belief that user-generated search engine could have a higher quality of results as in a story with almost every website in existence.  Broken Incentives. The vast majority of contribution to a search quality is made by users. Then any user searches something she extends semantic core. Then any user clicks on search results she trains a model. This creates an opportunity to continuously improve ranking model at the expense of users. Then search engines sell users to advertisers at the expense of harming user experience and acquire revenue streams which are not returned back to users at all. This simple loop created Alphabet's $550 billion capitalizations (~$80 per Earth capita) in 18 years. We want to change that.  Central Control. Google become too powerful. It is scary to imagine a future where  everything  about  everybody  is known and controlled by  closed  AI corporation. Imagine the world where (1) the only country exist, (2) nobody can control its government and (3) everybody should obey the decision of government without any explanation. There should be open, transparent and accessible  alternative  with  decentralized control  built on principles of modern distributed interplanetary content-addressable cyberspace [IPFS] and DAO like governance [RALF].  Annoying Ads. Separation for organic and ad search results is unnecessary. In fact, all organic ranking decisions are being made by search authority. But for paid search Google use free market solution to determine a fair ad price for every word in its gigantic semantic core. Historically free market solutions are proven to be more efficient in virtually any area of decision making. Why do not use the same principle for the ranking itself disintermediating annoying ads? Let us imagine that every link can be (1) curated or audited by everybody, (2) based on this trusted metric cyber\u2022rank (page rank based on economically incentivized curation and auditing) is calculated and then (3) everybody can promote this link further by burning some money automatically bringing value for everybody in existence. For every action, everybody earns a share proportionally to contributions. This non-zero-sum game is significantly more Sybil-resistant and that is there we are heading.  One-Way Trust. Everybody use to trust Google, Baidu, and Yandex. But Google, Baidu, and Yandex don't trust users. E.g. you cannot report some kind of proof that given link is a lie and should not be indexed so high. It can count your attention during ranking but can reject to count it. You cannot know what happens inside because Google, Baidu, and Yandex don't trust us. We want to establish a system there trust is bidirectional between the search engine and users because search engine ownership is distributed across all its users based on which all ranking decisions are made.  Zero Privacy. All search engines will answer you only if they explicitly know how to map your device with your real identity or pseudo-identity which is tracked by RTB [RTB]. Otherwise, you should prove that you are not a robot every time you search. That harm our privacy. Moreover, robot abuse is another hot topic that is about to happen. Nonetheless, nothing should harm our privacy.  Censorship. Though it's well known that Google working hard to prevent censorship we all know about China  GCHINA  case and  Transparency Report . A good search should be resistant to censorship without exceptions and build for interplanetary scale in mind.  Online only. Worth to note that you cannot search offline even if necessary information is stored next door. If we are cut from the wire or backbone we powerless. Global offline search is not a feature which can be easily deployed even by a multibillion corporation. This goal is nearly impossible to achieve based on centralized architecture. Only accessible distributed systems can solve this fundamental problem for the next generation Internet. This future is not about gateway keepers in form of ISPs but about mesh networking and peer-to-peer communications.  Weak Security. What happens if tomorrow my Google account will be blocked? Do we have something to prevent this? Do we have the necessary level of assurance that  guarantee  us our security based on math and not on the complicated legal tender? All technical solutions are here but to solve this important issue we need to do a lot of work because security is a foundation for life, liberty, and property.   A pretty huge amount of problems to fix. It would be naive to bootstrap a search engine from a scope where Google, Baidu, and Yandex exist. We need to find a special area where general purpose search engines sucks. A variety of distributed ledgers such as blockchains and tangles can be primary content-addressable data suppliers and this is a scope where current search engines are not the best at work. Moreover, blockchain technology evolves very rapidly and has a lot of promises so it is a sure bet.  The idea is to initially deploy a blockchain based search engine for the purpose of searching against other blockchains so that can be useful from the first day. At the same time, we design the cyber\u2022Fund  application  [CFUND] based on cyber\u2022Chain to solve a problem of trustless realtime blockchain asset valuation which adds some useful capabilities to the search. But we need to design cyber\u2022Chain in a way to be scalable for a more broad definition of a general purpose  search and valuation engine , so more applications can emerge. Currently, about 15 trusted (and thousands of not so trusted) and globally available distributed ledgers exist, totaling about 1 billion transactions accumulated. Last year (2016) amount of accumulated blockchain transactions increased tenfold. Not all of them were financial transactions in some sense. E.g. Steem [STM] blockchain and it's Russian sister Golos [GLS] primarily store user generated text such as posts and votes (? votes are text? confusing...). As transactions are the only way of changing states in databases currently used by any web application we foresee distributed ledgers to become the primary source of public information in the 21st century due to tremendous benefits of the technology [ENIGMA, ....].  Thus we are to declare the  principles  of a general purpose decentralized and distributed search engine for the upcoming age:   Privacy and Security. Just it.  Ubiquitous Ownership and Access. Everybody should have a right to possess a piece of it.  Mesh networks future proof. It should work in every connected surrounding.  Interplanetary scale. It should work on Earth and Mars.  Tolerant. In the era of machine learning, it should work for any kind of thinking beasts.  Open and Accessible. Everybody should be able to bring a bit to a quality of search results.  Blockchain Agnostic. Foundations behind its design should not rely on any protocol or stack rather be explicitly derived from the nature of the information itself.  Beautiful. The business model should not harm the user experience.  Transparency and Trustfulness. Every piece of its reasoning and behavior must be auditable by everybody.  No Single Point of Failure. Nobody should have a single key to modify or change it.  Sybil Attacks Resistant. This resistance should be derived from the properties of a free market but not from some single authority.  Intelligent. It should answer  natural  questions with  easy to read  and  provable  answers no matter text, media or natural numbers should be involved in the answer.   Alongside these principles, there are  requirements  such as performance, usability, and scalability. Keep reading. We discuss everything step by step.", 
            "title": "Introduction"
        }, 
        {
            "location": "/cyberd/cyberd/#design-rationale", 
            "text": "The idea of a permanent web behind IPFS is beautiful in its simplicity. Every unique  piece of data  has the unique address:   Using this address this piece of data can be found in a global data structure called MerkleDAG [DAG] (logical representation of data storage) across peer-to-peer IPFS network using bulletproof DHT. Nodes are weakly incentivized for fair data exchange using BitSwap protocol. This link can point to  any  piece of data such as GIT object, BitTorent link, Bitcoin block, JSON document, picture, video, plain binary data or even a small piece of text:   Let me explain the power of this solution for a search engine:  Data is unique and self-authenticated . If you know this hash is a piece of data you trust, you should not care where it comes from. This property free search engine from the storage of documents.  Direct Content Distribution . Weak incentivization of the BitSwap protocol has a very interesting side effect: the more popular the file =  the more people store pieces of it =  the faster you can get it. This is in contrast with a conventional client-server architecture where the more people want the file =  the more resources a server needs (and more expensive distribution become) =  the slower you can get it. This property significantly reduces resource usage to  deliver  results right to a user through distributed network.  Flexible Data Discovery . Big pieces of data are permanently linkable, thanks to MerkleDAG, so you can trivially reach a small chunk of data. That enable a lot of powerful applications impossible for a peer-to-peer network before. E.g. SQL-like data queries to a database distributed across the network or REST-like queries.  Thus documents which are located at  /ipfs  are  immutable . But what if we want  mutability ? IPFS offer the self-signed naming system which doesn't rely on conventional centralized and slow DNS (URLs we use to use). Everybody can publish a link in a namespace  /ipns  with a mutable pointer to any  /ipfs  piece of data and sign it with its node's private key:   After an owner is able to mutate this link pointer to another /ipfs piece of data. This gives us another important property for advanced search engine:  Fast Discovery . This property free search engine from rescanning work significantly reducing resources necessary for keeping index fresh. Workflow of conventional search index is straightforward: web crawler fetches a page, then follow up links, fetch the following pages and so on. After some loops, web crawler is able to get virtually all links in existence. The information contained in web pages is mutable. Thus search engine should decide then and how to fetch indexed links for new versions of documents. As web pages are loosely structured than it should somehow filter all the noise. In our case publishing is visible in DHT and can be nearly instantly visible for all participants across distributed network without the necessity to continuously send, receive and process HTTP requests. An average request is about 800 bytes long, so do every response [HTTP]. This overhead goes through the wire every time search engine want to know the freshness of the web page and then if the page is new it should crawl all new page even if 1 bit has been changed. Interestingly, though HTTP/2 bring new awesome features it still relies on the plain old location-based addressing and ancient DNS, thus this cannot be faster by design. In our case, we need sniff changes in DHT to know all news from a network. Currently, this is about 100kb/minute in a quite small IPFS network. There is estimation [QGTC] that about 10% of Internet traffic consumption has been made by Google crawlers. It easy to imagine how much overhead can be eliminated across the entire planet. This overhead currently is being paid by businesses around the globe.  Flexibility . Search results can be not ugly static snippets from 20 century but small dynamic programs  owned  by creators. It is hard to overestimate this idea. We will cover this topic further.  No Middleman . A search engine should not rely on a DNS middleman and can communicate directly with resource creators.  Ok. Now we understand that we have a usable way to store, reach and mutate the data across the globe in a more lean way. But what about an ability to  comprehend  what data is behind this meaningless hashes? Interplanetary File System uses a novel multihash, multicodec, multiformat, multibase [ https://github.com/ipld/cid#cidv1 ] formats for Interplanetary Linked Data (it is not a joke either). IPLD is a project aims to define CID that being used across IFPS network. Content IDentifier or CID is a self-describing content-addressed identifier. That creates enormous opportunity for optimization. Imagine a 32-byte link can contain everything to  independently  understand how to reach a piece of data and how to programmatically interpret it. Finally, CID is the thing which makes simplistic design of our proposed search engine possible inside consensus computer.  A nice property is that CID are based on well-known cryptography so we don't need to rely on IPFS if something better came. The same addresses can be used to exchange a piece of data in a different peer-to-peer network. Though this doesn't solve a problem of extensibility. CID itself contain version bits, thus if something better came we can switch to it.  But now IPFS and IPLD is a perfect design choice with huge momentum across academia, open source engineering community and (the most important) blockchain engineering teams. It is our belief that it should lay as foundation for the next generation search engine. Everything is perfectly likable (MerkleDAG), fast (DHT), accessible (BitSwap) and comprehendible (IPLD) without a single point of failure. IPFS is a protocol and it can be more useful with a good search engine specifically designed to it. It's possible to compute a PageRank for the whole MerkleDAG. But there are two problems with it:   An amount of links in MerkleDAG grows  O(n^2) . That is not either conventional web pages with 20-100 links per page. For 1 Mb file can be thousands of links. Once the network starts to take off, complexity inevitably increases.  Even if we address some algorithm to extract relevances from these links we should address even more algorithms to extract a meaning.   What we need is to find a way to incentivize extraction from this data fog a meaning that is  relevant to users queries .", 
            "title": "Design Rationale"
        }, 
        {
            "location": "/cyberd/cyberd/#search-workflow", 
            "text": "Our proposed design of a search engine is based on advanced blockchain technology and enables everybody to participate and be rewarded. Everybody with an account can search. To execute queries user should sign a  search  transaction with CIDv1 as payload and broadcast it.   cyberd search {CIDv1} hipster true\n[where s the privacy declared before? with every search query signed with account name on a public blockchain? (tomarcafe)]   A document should be a valid CIDv1. Post headers are purposefully unique. The rationale is the following:   strongly encourage to bring valuable documents first and get a reward  simplify search execution and ranking calculation  make possible a high-performant flat data structure   Worth to note that search request \"buy tesla x\" is also can be represented as CIDv1. Thus search index store queries itself as documents. Hence search queries and target documents both will acquire cyber\u2022rank.  Then cyber\u2022chain verifies validity of CIDv1, the correctness of signature, broadband allowance and if all conditions are met either of two things happens:   if CIDv1 is not in search index =  Write to search index  else broadcast a vote for existing CIDv1 and return sorted links of relevant CIDv1   Based on this data client-side application can deliver documents by 3 ways:   using full javascript implementation of IPFS [JSIPFS] (fast, but requires initialization)  using REST API provided by IPFS HTTP gateway (depends on, can be fast or not)  using local IPFS node (the fastest)   This approach is simple and powerful. A developer has enough choices to balance between usability and performance.  CIDv1 can mean any piece of data. Different data types that can be returned to a user depending on the query:   Plain text for autocomplete, e.g. ['apple', 'asos', 'amazon']  A piece of media content which a user can play without necessity to go somewhere  Static formatted snippet with a link to the conventional web.  Static formatted snippet with a text answer.  IPNS link pointing to javascript that can return dynamic snippet.   It depends on developers (mainly submitting answers) and users (mainly ranking answers) what kind of things they want to answer questions. Possibilities are limited with imagination. Thus we propose a free market for answers on search queries everybody is encouraged to participate. How does it work?  Information about indexed CIDv1 as well as about its rankings is available for everybody. Thus those interested in rewards can monitor the blockchain for semantic core updates and submit links nearly instantly. Everybody can sign  answer  transaction with a link from   to   as payload and broadcast it:   answer {search CIDv1} {answer CIDv1} hipster true   A document should be a valid CIDv1 and unique outbound link from an answer. So for any given question, the only unique answer is possible. Then cyber\u2022Chain verify the correctness of signature and either of two things happens:   if CIDv1 is not in search index =  Write to search index  if answer CIDv1 has no link to question CIDv1 =  Write to answer index  else broadcast a vote for existing answer CIDv1    We follow black box rule. In order to answer a question right, you need a full comprehension neither the question nor the answer. You just need to match a query with the most relevant links.   In order to increase the rank, everybody can promote either link or query.   promote_search {CIDv1} hipster 20 true\npromote_answer {search CIDv1} {answer CIDv1} hipster 20 true   Every sent token for promoting is destroyed thus creating value for every token holder.  That is a core API for the entire blockchain. Other methods accomplish support role for the thing. Such compact design opens huge opportunity for performance optimizations. Also, a clean and comprehensible experience is very important for those who want to be involved. That is. The entire graph of the semantic core with weights is open for everybody and available for data mining or any kind of weird AI stuff. But to make it work we need to find a way to calculate relevance.  We can represent our data structure as directed acyclic graph where vertices are indexed documents and edges are directed links between them.   We equate terms  document  /  query  and  link  /  answer  as for our use case these are practically the same. We will stick to  query  and  answer  terms in order to avoid confusion.  Hence if a user searches a document  CID 3  (query) search engine will return links (answers) to  CID 1 ,  CID 4 ,  CID 5  documents (queries) sorted by cyber\u2022rank. Let us discuss it in details.", 
            "title": "Search Workflow"
        }, 
        {
            "location": "/cyberd/cyberd/#cyberrank", 
            "text": "The idea is to combine two simple yet powerful algorithms: Google's PageRank and Steem's reward mechanism:   Where  t_rank  =  n_rank  +  s_rank  n_rank , or natural rank is a rank based on Steem reward system.  s_rank , or synthetic is a plain old PageRank used by Google and others.  Natural rank is acquired in a process of auditing and curation. Based on this rank payout to those who involved (queries and answers submitting, auditing and curation) a made. Each piece of submitted data gets paid in 7 days. Each piece of data can be voted by cyber\u2022power token holders. We use  auditing  term primary for verification of submitted data by automated scripts and  curation  term to denote manual curation. From a technical standpoint, those are primary the same as both utilize the same method for interactions. Implementation of natural rank is almost identical to Steem. Thus details of implementation can be found in our Github [] or Steem whitepaper.  Synthetic rank is taken as initial natural rank expressed in  rshares . This makes possible to start calculating PageRank before payouts have started. Conventional search engine work with nearly zero trust data. More than 200 factors used to calculate initial PageRank for a new document in the graph and find relevance to the search terms. Our novel approach allows assigning initial value based on Sybil-resistant voting. It is our belief that proposed approach can significantly simplify ranking and be more precise for information that nature is subjective. Though PageRank calculation is a trivial task we should estimate the feasibility of doing so in a consensus computer such as Steem where scalability is limited with a less performant node and parallel processing [Steem Roadmap] or sharding is yet to be discovered [Ethereum Mauve].  A recent study [http://www.vldb.org/pvldb/vol8/p1804-ching.pdf] shows that Facebook scale PageRank computation is doable (using Java based Giraph) for 1 trillion edges and 1.4B vertices in 600 minutes per iteration. Hence consensus computer made of commodity hardware will be able to process 1 iteration per 2 days for a 10B unique document (SWAG for all blockchains + Git + BitTorrent (https://arxiv.org/pdf/1009.3681.pdf) + IPFS). Our implementation is based on C++ thus can be more performant though is not guaranteed. Also, we have an opportunity to use the most performant GPUs available operated by witnesses and not that is being used by cloud providers [http://www.hipc.org/hipc2011/studsym-papers/1569513051.pdf]. Anyway, our estimation proves that it is practically enough to use CPU for proof-of-concept stage. GPU implementation can multiply computation capacity up to 1000x. But further research in the field of parallel consensus computing is necessary to achieve Google scale (10000x more documents) realtime decentralized computation of cyber\u2022rank.  Our model is recursive and requires the enormous amount of calculations which are limited within blockchain design. Model recalculation does not happen on a periodic basis rather it continuous. We consider introducing consensus variable, in addition to a block size, in order to target processing capacity of the network. Let's call it a  computing target of documents per block  or CTD. Any witness will be able to set a number of documents the network should recompute every block. The blockchain takes as input computing target of legitimate witnesses and computes CTD as daily moving average. Based on CTD blockchain can schedule the range of CIDs that should be recomputed by every witness per round.  The semantic core is open. Click-through information is stored on-chain. Every time a user follow a link positive voting transaction is broadcasted e.g. with grade 1. Voting on a protocol level is a number in a range from -100 to 100. Thus application developers have a tool to implement different grades for different kind of interactions. Such design is crucial to train the model and acquire a data about search popularity of semantic core and its volume. Currently, search engines are very careful in revealing this information because this information is the most important part of the ranking. We want to change that. Every time a user click on a snippet developer earn a fair portion of emission and on chain model is trained. Application acquires the more rank the more rank acquired by its links. The more cyber\u2022rank acquired - the more revenue streams for an application developer.  Both algorithms have strong proof in form of Google's $550 B capitalizations in 18 years and Steem $40 M capitalization in 9 months. Combining both it is possible to empower the world with a new kind of search quality that has been (1) designed to index relevant document fast and (2) has inherent Sybil protection.", 
            "title": "cyber\u2022rank"
        }, 
        {
            "location": "/cyberd/cyberd/#self-indexing-dilemma", 
            "text": "Proposed approach has very unexpected limitation. What if we want to index cyber\u2022chain using cyber\u2022chain itself? Let us say that we have an awesome transaction that happens inside the chain and everybody are talking about it. It is popular thus we should display it in our search results. Adding it to an index spawn another transaction which (surprise) also should be indexed. This entanglement creates an infinite loop that bloat cyber\u2022chain. This can not be a problem either. Consensus computer capacity and power are limited by the market forces. So we have two possible decisions:   Let it be. The market is a king. A bit of bloat can be a good piece of a knowledge about itself.  Strictly forbid indexing of the blockchain itself. Fortunately, it is not so hard to implement on a consensus level. All we need is to check that CID has not been included in a cyber\u2022chain before. That mean that cyber\u2022chain transaction itself remain unindexed because in order to achieve this we (1) either should mutate data for hashed and timestamped transactions, (2) nor create a possibility for a self-bloat. None of the options is valid. Again, fortunately, direct search without ranking among internal cyber\u2022chain transactions can be available inside search results with the help of CID magic.   It is our belief it is better to stick to restrictive policy without further research.", 
            "title": "Self Indexing Dilemma"
        }, 
        {
            "location": "/cyberd/cyberd/#challenges-and-advantages-of-indexing-distributed-ledgers", 
            "text": "Conventional general purpose search engines were built on the  last mile assumption  of stateless HTTP(S) protocol. Indeed the last mile approach worked well on the Internet where information emerges inside stateful private databases and only after become publicly available using HTTP. The emergence of content-addressable systems such as Git and BitTorrent didn't change much as those can not be compared with stateful private databases in any sense even though these protocols are represented origin of a content. But with an emergence of distributed ledger technology become possible to get know about public content in the moment than it actually has been born. In this sense, blockchains and tangles can be viewed as a real alternative to conventional private databases. That breakthrough create enormous opportunity for better and faster indexing but at the same time has inherent problems solving of which create advantages:  Chain Validation . Protocol diversity is the hardest part of solving an issue of chain validation using another chain because solving this requires full implementation of one consensus computer inside another consensus computer. There are two efforts exist that try to solve issues of validating one chain using another. Among them are  Polkadot  and  Cosmos . Both projects aim to solve a problem of trestles inter blockchain communications. Polkadot aims to design a system which doesn't require a trust to parachains (or interconnected chains). The design of Polkadot is very complex and has inherent scalability limitation. The design of Cosmos is much more simple, but require a trust to zones (interconnected chains). Our design doesn't try to solve a problem of inter blockchain communications rather try to implement probabilistic search across blockchains. Thus it is significantly more simple. Instead of recording information that has been verified from point of view of exogenous protocol we let this information come to index letting market forces and relevance algorithm determine which chains are correct and which are not.  Meaning Extraction . Practically there are no common fields that are used by all blockchains and tangles. The only common pattern is  tx_id . There is no way to extract any meaningful information from  tx_id . That is practically mean that there is no easy solution exist. For every protocol can be a large number of different approaches to extract useful information. Using advanced machine learning is infeasible for consensus computers at the moment. We offer set of smart contracts that create a free market for extraction of the meaning.  The huge advantage of blockchains that they are implicitly told what happens. Blockchains are highly optimized databases. Every transaction cost money so they are neutrally protected from spam and contain only data that really matter. That reduce noise filtering to nearly zero level. Structured raw data about blocks and transaction is available for everybody. This fact also reduces consumption of resources and make extraction of better meaning possible.   Blockchains provide real-time high-quality structured data which don't require  crawling  in a traditional sense. One node of any given blockchain is enough to provide verified data about transactions within one ledger without the necessity to continuously revisit resources significantly reducing costs.  All these factors create a free market opportunity for emergence the diverse set of highly specialized (on a very limited set of a semantic core) but highly efficient  broadcasters .  Probabilistic Settlement . Blockchain designs, especially Proof-of-Work based, implies that finality of a transaction is probabilistic. A moment than a given fact can be considered as truth is blurred.  Previous researches  show that it is expensive to achieve real-time blockchain indexing due to reorganization issues. The intention of discussed in the article design is to answer the question deterministically. Our architecture based on the principle that indexing a system with probabilistic settlement require probabilistic answering. Instead of deciding whether or not this particular block has been included in the canonical chain we can index all of them calculating the probability of finalization using cyber\u2022Rank.  So solve of discussed issued of probabilistic indexing we propose a flexible approach we call lazy oracles.", 
            "title": "Challenges and Advantages of Indexing Distributed Ledgers"
        }, 
        {
            "location": "/cyberd/cyberd/#lazy-oracles", 
            "text": "One specific ability is crucial for the next generation search engine. Application developers should have a motivation to provide structured arbitrary data feeds. Thus search engine can answer natural questions aggregating data from  highly structured and defined  feeds. This makes possible a user get high-quality  calculated  answers in real-time about the state of reality expressed not only in links (which intelligent agent don't know how to parse) but in actionable numbers based on which it's possible to make independent economic decisions. It is hard to find a tool to agree on publicly available and continuously evolving facts. We propose an approach to solving this.  Today different blockchain have the functionality necessary to implement this. For eg. Ethereum enable construction of smart contracts that can validate and incentivize data feeds. But Ethereum has the strong limitation: a price. Due to a network design, every operation should be validated across the network of 5k nodes. For every put operation developer of such contract should pay in hope that somebody in the future will use this feed in the future returning costs. The current cost of a permanent storage inside Ethereum contracts is around $200/megabyte. Worth to note that Ethereum has consensus variable gas limit. Currently, a network load is around 10% of established limit. Once the demand for computation reaches a limit we will have a situation very similar to Bitcoin block size debate and price for storage can reach $2000k/megabyte easily without validation costs. Pretty expensive for unlimited possibilities. There is the alternative - Factom [FACT]. Its consensus design relies on a small amount of paid servers. Thus the cost is around $1/megabyte. Such low price comes with high limitations. You can only put data to Factom and read it. There is no validation and incentivization built-in. There are permissioned blockchain designs such as BigChainDB [BCDB] and Hyperledger [HYPL] which solves validation problem perfectly but require strong efforts for developers to program and establish a network and then somehow monetize it. Lazy Oracles are going to fill this gap providing robust, cheap and reliable way  for monetizing  structured public data. Any cyber\u2022chain account has a share in a broadband depending on its cyber\u2022Power thus granting lifetime assurance of network usage. A network programmed with decaying inflation a part of which goes to all who participate in indexing depending on the valuation of subjective contributions.  The process consist of 5 steps:  Step 1: Everybody can declare a soft protocol for data feed by posting a CID with the inbound link to  oracle-defenition  CID pointing to the document with the following structure:  // Basic Validation\ndoc_type: market_update // should be fixed\n  exchange: string // domain name of the exchange\n  base: string // link to a definition of a referenced namespace e.g. ISO or chaingear\n  quote: string // link to a definition of a referenced namespace e.g. ISO or chaingear\n  price: number\n  volume: number\n// Audit Rules\nDocument format and data types should match a protocol\nFor every unique exchange and pair `market_update` report can be submitted no more frequently than once per minute.\nIf either a price or volume for unique exchange and pair has not been changed more than 0.1% a report should not be submitted.\n// Inbound Links oracle   market-update   exchange   base   quote \n// Reference Crawler Implementation\nCID\n// Reference Auditing Implementation\nCID  Protocol declaration should be agnostic from language implementation and unambiguous.  Step 2: Now every reporter can submit data according to a soft protocol. If data begin to be submitted without protocol definition a value of this data can be significantly lower. Thus auditors are encouraged to flag such documents. If false or malicious data come auditors have strong incentive to flag such data. All lazy oracles should be feed with self-descriptive links so other participants will able to faster process auditing.  Step 3: Auditors validate any given document by scripts.  As result reporters, auditors (where objective script-based validation is possible) and curators (where subjective human-made evaluation is more appropriate) have strong incentive to (1) bring important data, (2) curate and audit data conscientiously. Proposed approach doesn't have strong guaranty of the truthfulness of data such as Augur does. Rather it is correct to say that we can have strong assurance that data is correct. But it is cheaper, faster and significantly more flexible. The process is robust in its unstructured simplicity.  Step 4: Payouts for auditors are made. Payouts information is input information for cyber\u2022rank. Using cyber\u2022rank it is trivial to filter feeds using plugins to give significantly more precise data with a higher level of assurance.  Step 5: High-quality data feeds (or lazy oracles) are available for a consensus engine of a search engine (thus cyber\u2022rank can be continuously improved) and everybody on the planet.  We call this type of oracles lazy because they don't require strict rules of validation at the expense of reducing the level of accuracy (but still enough to reason with a high level of assurance). Also, they are lazy because the don't require to think about monetization for participants rather consensus engine print rewards based on a valuation of subjective contributions. This approach is superior to Ledgys [] than reporters should sell encrypted data pieces using costly Ethereum storage.  The most obvious use cases for Lazy Oracles   Block indexing  Transaction indexing  Balance calculation  Identity crawling  Token valuation  Token rating  Token description  ICOs tracking  Weather measuring  Traffic measuring  Business performance reporting   Worth to note that use cases are not limited to mentioned above. Data structure and validation rules are arbitrary. So we can think of it as general purpose tool for  popular structured public data  auditing and curation. Now we can bootstrap and index with amounts of useful information. But what about meaningful search results?", 
            "title": "Lazy Oracles"
        }, 
        {
            "location": "/cyberd/cyberd/#dynamic-snippets", 
            "text": "This can be thought as serverless micro javascript(is javascript futureproof? ;)) applications that can dynamically take input data from the following sources:   a search query itself  asynchronously from background search query  from a browser APIs  from device sensory information  from information about a user stored on cyber\u2022Chain  from other blockchains  from IPFS and IPNS  from conventional HTTPS or WebSocket APIs.   Every application is CID written to search index and answer index. Before developing an application developer should target it for a specific semantic core before defining it. Semantic core and its statistic are publicity available in a blockchain. Developer shall need to develop an application and submit links to an application using either immutable IPFS documents or mutable IPNS pointers for a targeted semantic core. Thus it is up to a developer to define what search queries are a better fit for a particular application. Keep in mind that dynamic snippets are naturally competing for a higher position in search results. Dynamic snippets can be sorted by cyber\u2022rank, and as result become trustful. Worth to note that developers can significantly reduce spendings on app infrastructure as dynamic snippets can be delivered through content-addressable distributed network. Mutable IPNS pointers allow developing snippets for a targeted semantic core and not for every unique query. An implication of this approach is hard to overestimate. E.g. dynamic snippets combined with blockchain wallet make possible to shop right from search results.  The only potential problem with proposed approach is the safety of third party javascript code. Sandboxed third party code is able to mitigate this risks. Web technologies such as  web workers  and  web components  are being actively developed. E.g. javascript library  jailed  [https://github.com/asvd/jailed] is able to do exactly what we need. Further adoption of web components is also one of a possible solution to do that safely.", 
            "title": "Dynamic Snippets"
        }, 
        {
            "location": "/cyberd/cyberd/#dealing-with-long-tail", 
            "text": "It is well-known fact that every day Google receive up to 20% of new search queries never seen before. Thus we need to find a way to deal with it. Here are 3 popular use cases with simple solutions (surely non-exhaustive):\n-  Misspellings . These queries contribute fair half to ever-growing unique query set. But it is not a rocket science to stem such queries client side without any indexed knowledge. After a user clicks on client side suggestion correct link can be submitted to cyber\u2022chain thus improving the global model.\n-  Phrases . E.g.  forest gump imdb . These queries can be a combination of well-known terms and contribute another half to ever-growing unique query set. Even if we get relevant links from indexed  forest ,  gump  and  imdb  separately we would not able to combine answer to return meaningful movie rating to a user. But we can find  the closest documents  between  imdb ,  forest  and  gump  and sort them by relevance. Those likely be the most relevant answers. This simple method can significantly increase efficiency for long tail queries without rocket science. As API is open for everybody there is no limits on using advanced technics.\n-  Unique queries . These are about 10% of never-seen-before queries. We expect that the market of linking will create a segment for on-demand answers. We remember that any query can be seen in memory pool by every network participant. Thus opportunity to earn can create a healthy and competitive market for an on-demand answer in an environment without technical limitations.", 
            "title": "Dealing with Long Tail"
        }, 
        {
            "location": "/cyberd/cyberd/#spam-protection", 
            "text": "In the center of spam protection system is an assumption that write operations can be executed only by those who have vested interest in the success of the search engine. Every 1% of stake in search engine gives the ability to use 1% of possible network broadband. As nobody uses all possessed broadband we use fractional reserves while limiting broadband like ISPs do. Details of an approach can be found in a Steem white paper.  Auditing and curation are based on Steem reward mechanism. It is Sybil-resistant approach as votes are quadratic based on principle 1 token in system = 1 vote. In order to vote one should vest in shares for at least for 20 weeks. That solve a problem entirely because those who have a right to vote are strongly incentivized in a growth of his wealth. In order to prevent abuse of auditing and curation voting power decay implemented exactly as in Steem.", 
            "title": "Spam Protection"
        }, 
        {
            "location": "/cyberd/cyberd/#applications", 
            "text": "It is hard to imagine what kind of applications can be built on top of proposed foundation. I'd like to mention some outstanding opportunities which can be build using cyber\u2022Chain and IPFS:   Relevance everywhere  Blockchain browser\n_ Multi-protocol wallets  Offline search  Smart command tools  Autonomous robots  Language convergence   Relevance Everywhere . Proposed approach enable social, geo, money or anything aware search inside any application. It is trivial to implement a search relevant to a particular identity using proposed algorithm. The more a user train a model the more behavioral data can be associated with her. This personalized information can be stored locally for (1) faster retrieval and (2) offline access.  Blockchain browser . It easy to imagine the emergence of a full-blown blockchain browser. Currently, there are several efforts for developing browsers around blockchains and distributed tech. Among them are Beaker, Mist and Brave .. All of them suffer from very limited functionality. Our developments can be useful for teams who are developing such tools.  Multi-protocol wallets . Currently, there are several efforts for developing easy to use the universal wallet for blockchain assets. Jaxx and Exodus are among them. Developers of such applications suffer from a diversity of protocols around blockchain tech. There is no fully functional multi-asset wallet yet. Our developments can help teams who are developing such tools.  Actions in search . Proposed design enable native support for blockchain asset related activity. It is possible to design applications which are (1) owned by creators, (2) appear right in search results and (3) allow a transact-able call to actions with (4) provable attribution of a conversion to search query. e-Commerce has never been so easy for everybody.  Offline search . IPFS make possible easy retrieval of documents from surroundings without the internet connection. cyber\u2022Chain itself can be distributed using IPFS. That create a possibility for ubiquitous offline search.  Smart Command Tools . Command line tools can rely on relevant and structured answers from a search engine. That practically means that the following CLI tool is possible to implement    mylovelybot earn using hdd -100GB\n\nsearching for opportunities:\ncyberd search  earn using hdd \n\nThe following answers received:\n- apt install siad /// 0.0001 btc per month per GB\n- apt install storjd /// 0.00008 btc per month per GB\n- apt install filecoind /// 0.00006 btc per month\n...\n\nMade a decision try `apt install siad`\nGit clone ...\nBuilding siad\nStarting siad\nCreating wallet using your standard seed\nYou address is ....\nPlacing bids ...\nWaiting for incoming storage requests ...  Search from CLI tools will inevitably create a highly competitive market of a dedicated semantic core for bots.  Autonomous robots .\nBlockchain technology enables creation of devices which are able to earn, store, spend and invest digital assets by themselves.   If a robot can earn, store, spend and invest she can do everything you can do   What is needed is simple yet powerful API about the state of reality evaluated in transact-able assets. Our solution offers minimalistic but continuously self-improving API that provides necessary tools for programming economically rational robots.  Language convergence . A programmer should not care about what language do the user use. We don't need to have knowledge of what language user is searching in. Entire UTF-8 spectrum is at work. A semantic core is open so competition for answering can become distributed across different domain-specific areas, including semantic cores of different languages. The unified approach creates an opportunity for cyber\u2022Bahasa. Since the Internet, we observe a process of rapid language convergence. We use more truly global words across the entire planet independently of our nationality, language and race, Name the Internet. The dream of truly global language is hard to deploy because it is hard to agree on what mean what. But we have tools to make that dream come true. It is not hard to predict that the shorter a word the more it's cyber\u2022rank will be. Global publicly available list of symbols, words and phrases sorted by cyber\u2022rank with corresponding links provided by cyber\u2022chain can be the foundation for the emergence of truly global language everybody can accept. Recent scientific advances in machine translation [GNMT] are breathtaking but meaningless for those who wish to apply them without Google scale trained model. Proposed cyber\u2022rank semantic core offers exactly this.  This is sure not the exhaustive list of possible applications but very exciting, though.", 
            "title": "Applications"
        }, 
        {
            "location": "/cyberd/cyberd/#incentive-structure-and-distribution-mechanism", 
            "text": "To make cyber\u2022rank economically resistant to Sybil attack and to incentivize all participant for rational behavior a system uses 3 types of tokens: CYBER (or cybers), CP (or cyber\u2022power) and CD (cyber\u2022dollar)  CYBER is a transferable equity token which is analog of STEEM. The intrinsic value of CYBER came from the ability to convert it to CP.  CP is a non-transferrable equity token which is analog of SP in Steem. CP can be converted to CYBER in 20 weeks using proportional weekly payments. The intrinsic value of CP came from the right to (1) write to an index according to a bandwidth limit, (2) rank objects, (3) promote objects (4) make consensus decisions. CP can be converted to CYBER in one year.  CD is a debt token with a relatively stable value which came from an ability to convert it into CYBER within 3 days by the price submitted by witnesses and calculated according to cyber\u2022rating methodology [] (don't confuse). 1 CD tracks 1/10^12 of  provable  blockchain economy.  Reward Pool is defined as 100% of emission and split among the following groups:  Infrastructure Reward Pool   Witnesses - 5%  Investors - 10%   Indexing Reward Pool - 30%   Reporters - 10%  Auditors - 20%   Linking Reward Pool - 65%   Responders ~ 20%  Trainers ~ 40%   Our implementation also offers an incentive for CD holders. They receive APR on holding according to rate defined by witnesses.  As our primary focus is developers of decentralized and distributed applications we offer a convenient way of revenue generation in addition to possible revenue on dynamic snippets development. Every search and answer transaction can define a smart contract with up to 6 beneficiaries with a distribution of potential reward from this particular action. This creates a solid foundation for bootstrapping application ecosystem across a conventional web, mobile app stores and upcoming VR platforms.  A conventional advertiser is also has a tool to participate. Any link can be promoted thus increasing cyber\u2022rank. Revenue from the promotion is burning thus decreasing the supply of CYBER and bringing value to all shareholders of a system. We do believe that this approach is fair and don't harm user experience but open opportunity to compete with conventional search ad a bit.  Virtual loop of the business models for our decentralized autonomous organization is pretty simple:  More indexing =  More people search =  More developers build =  More people earn, rank and promote =  Better infrastructure =  More indexing .  Since inception, a network prints 3 CYBER every block. Every 1 million blocks it reduces print rate on 1%. Thus starting from ~35% print rate per year inflation begin to reduce gradually until it reaches 1%.  There is one problem with proposed incentive structure. We call it  language incentivization bias . In the core of cyber\u2022chain is quadratic voting. The system needs it to effectively incentivize participants for quality ranking. But that natively leads to weak incentives across different language groups. E.g blockchain Golos was deployed as Russian alternative to Steem because Russian posts acquired only 0.2% of rewards though providing 10% of the content. The idea of deploying cyber\u2022Chain is great if it can be truly global from the start. The only way to overcome this bias is a global deployment from day 0, because otherwise we need significantly increase the complexity of the reward system. We offer good incentives for translation of this white paper to 50 languages worldwide as well as call for action to all blockchain communities across the globe.", 
            "title": "Incentive Structure and Distribution Mechanism"
        }, 
        {
            "location": "/cyberd/cyberd/#extensibility-and-governance", 
            "text": "Currently, our implementation has the following functionality available for application developers.   Custom Operations. Better alternative for OP-RETURN  Plugins. Allow implementing API based on custom operations.  Escrow. The core smart-contract enables 3d party arbitrage for arbitrary transactions.  Private Messaging. Enable private communications between accounts.  Dynamic Account Permissions. You can think about it as better multi-sig.   The following possibilities can be available in a distant future:   Sidechains  State channels  Permissionless smart contracts   Consensus can be changed in case of 17 of 21 elected delegates accept a hard fork.", 
            "title": "Extensibility and Governance"
        }, 
        {
            "location": "/cyberd/cyberd/#search-and-evaluation-appliance", 
            "text": "For bootstrapping a network we are going to offer software configuration (Cybernode - Github) on top of well tested open source specs for hardware configuration of commodity computer (Enterprise - Github) which cost around $10k-$30k depending on RAM and SSD capacity and is able to participate and earn by itself executing different network tasks:\n- operate as witness node\n- operate as indexer and auditor\n- operate as answering node\n- operate as fast and cheap backend for decentralized and distributed application  We need a network of high performant computers in order to achieve our goals. Necessity comes from  the following assumptions:\n- all blockchain nodes and IPFS should live inside one machine to remove slow network communications from all middleware. Vast information for processing can be in memory.\nCommunications inside one bus enable to execute required tasks significantly faster [[https://gist.github.com/jboner/2841832]]. Thus we can achieve nearly live indexing of reality from the very beginning.\n- extension with GPU. Currently, data centers cannot compete with commodity GPU. E.g. Amazon offers very expensing professional Nvidia Tesla cards. For our purposes, commodity cards such as GTX 1080 are much more cost effective.  Enterprise . Currently, it is not hard to assemble a 2 CPU computer with 1 TB of RAM and 40 TB of SSD using commodity hardware. Such appliance can cost about $30k so we can think of it as affordable for those who are seriously want to be involved in a project. Also, we have an option to extend the capability of proposed search appliance based on 2 CPU motherboards built on Intel C612 chipset. Usually, it has 7 PCI-E slots for GPU which can be dedicated for cyber\u2022rank calculation. Thus a price for an ultimate (2 CPU * Xeon E5 * 22 cores + 7 GPU * GTX 1080 * 2560 Cuda cores) search and evaluation appliance can be around $50k. Currently, such computer will be able to process, index, audit and linking all blockchains.  Cybernode . We implementing the following software configuration that is based on docker containers.   Cybernode allows everybody fast deployment of decentralized and distributed application powered with cyber\u2022chain search capabilities.", 
            "title": "Search and Evaluation Appliance"
        }, 
        {
            "location": "/cyberd/cyberd/#performance-and-scalability", 
            "text": "Proposed blockchain design is based on DPOS consensus algorithm and has fast and predictable 3 seconds block confirmation time and 1 minute finality time. Average confirmation timeframe is 1,5 seconds thus conformations can be asynchronous and nearly invisible for users. A good thing is that users don't need confirmations at all before getting search response as there is no risk associated with that.  Current node implementation theoretically [https://bitshares.org/blog/2015/06/08/measuring-performance/] can process about 100k transactions per second. This theoretical bound is primarily limited with the possibility to replay a blockchain [https://steemit.com/blockchain/@dantheman/how-to-process-100m-transfers-second-on-a-single-blockchain]. As of now, all blockchains are about 1B immutable documents which size is about 200 GB with average tx 200 kb. We need to store all hashes which are on average 64 bytes long. We estimated that storing in the index all blockchain documents as CIDs and votes are roughly the same as storing all raw blockchain data. Linking 1B documents create significant overhead as blockchain index size can be up to 100 times more. Given this, we can assume that indexing all existing blockchains require about 4TB of SSD space. This is affordable for commodity hardware with 10x scaling capability without a necessity for sharding across several machines. We assume this is enough scalability margin for proof-of-concept.  Initial indexing of 1B documents and 100B links will require a continuous load of the network at the upper bound of its capacity in the first year of its existence. If we assume that network will be able to process 10k transactions per second with 2MB block size we will be able to index all blockchains in 4 months. Further operations will require significantly less capacity as currently, not more than 1000 transactions per second happen among all blockchains.  Based on the proposed search appliance we estimate that participants will require investing around $1M for dedicated hardware (21 witnesses) and the same amount for backup nodes. Thus overall costs of hardware network infrastructure can be around $2M. after full deployment.  Worth to note that the network doesn't require ultimate configuration at the start and is able to optimize initial investments by the costs of time to index all blockchains. Thus costs at launch can be around $200k. Given that mining industry has been rapidly developed last years this can not be a showstopper for a project. We expect huge interest from miners as slots are limited with 21 fully paid nodes and ~20 of partially paid nodes (depend on the market).  Possible scalability improvements include:\n- Hardware. This year Intel Optane [http://www.intel.com/content/www/us/en/architecture-and-technology/intel-optane-technology.html] That creates an opportunity to converge RAM and SSD. Our design has 3-year hardware margin for moving to cheaper and more dense next generation memory.\n- Software. The future of consensus computer optimization is in parallel processing. We are going to seriously invest in research of this field. Solving this issue will enable the network to scale nearly infinitely.", 
            "title": "Performance and Scalability"
        }, 
        {
            "location": "/cyberd/cyberd/#deployment", 
            "text": "We split a process of network deployment into the following milestones which are not bounded to any timeframe at the moment:   Exploration Phase  a white paper published  CID verification on consensus level  cyber\u2022rank implementation  cyber\u2022node release   The purpose of seed stage is to implement the blockchain and prepare it for technical launch.   Validation Phase  Blockchain launched  cyber\u2022Fund basic application release  Token distribution   The purpose of validation phase is to verify the feasibility of an idea and prospects of technological design across blockchain community. Successful deployment of MVP in the form of basic technical infrastructure in a decentralized fashion and quality of support from the blockchain community and investors around idea will be enough to understand what kind of future the blockchain has. Objective metric is an amount of bitcoins raised during crowd sale for already working blockchain design.   Build Phase  Indexing 10 blockchains  Indexing 1000 market pairs  Evaluating 1000 tokens  GPU cyber\u2022rank calculation implemented  Historical records of balance valuations is available for indexed blockchain  Developers run 10 experiments  First payouts to indexers and auditors   The purpose of build phase is to reach a very  basic  product/market fit around  one specific use case  or this use case will emerge from experiments. A number of payouts which will be calculated based on current capitalization is objective metric. 6 month is expected the duration of phase. If build phase will be successful there are infinite opportunities ahead.   Scaling Phase  Indexing all blockchains  Indexing Git, BitTorent, IPFS and DAT  Indexing 10 blockchains  Autocomplete is fully functional  Top 1 mln. search queries return useful answers   This is an infinite phase in which the network start continuously grow indexing more and more relevant and meaningful data and the most important answering questions better and for the better. A key scope of work during this stage is to continuously improve developers experience:   More indexing =  More people search =  *More developers build* =  More people earn, rank and promote =  Better infrastructure =  More indexing .", 
            "title": "Deployment"
        }, 
        {
            "location": "/cyberd/cyberd/#the-power-of-cyberchain", 
            "text": "The key purpose of our proposed design is not just replicate abilities of existing search engines which return only links but enable answering new class of question:   How much value of X do I possess now?  What probability of event Y?  What packages do I need to install in order to improve ROI on available resources?   Our proposed design has all necessary components to bootstrap a market for a new generation of answer applications.  Proposed economics model disintermediate conventional ad model there users are sold to an advertiser and enable any business or people or robot benefit from pure peer-to-peer interactions which bring value for every involved participant.  Free Market of Indexing and Auditing . Everybody can connect any blockchain or content-addressable protocol. A decentralized approach to indexing and auditing create an opportunity for those who want to earn on the contributions to cyber\u2022Chain. Proposed solution is not more than a way to  outsource  these complicated and unstructured efforts for the entire community.  Free Market of Answering. After all, we have recent advances in machine learning enable to reason about a piece of data quite well. All these algorithms require enormous highly distributed computation which as nearly impossible to achieve in a trestles consensus computer. With the current state of blockchain technology implementing these algorithms using decentralized computational network seems unfeasible. We find a way to _outsource  this computation for the entire community.  Self Hosted Search API . Everybody can deploy self-hosted API. In comparison with what Google offer ($5 per 1000 answers). Our solution can be much more cost effective for high performant applications. One node can process at least 10k queries per second in a read-only mode. That is about 1B requests per month. That is about 100 times cheaper ($0,05 per 1000 answers) even if payback period of search and evaluation appliance ($50k) will be one month. In reality, the affordable payback period is about 10 months. Thus self-hosted search, in theory, can be 1000x more cost effective than Google offering.", 
            "title": "The power of cyber\u2022Chain"
        }, 
        {
            "location": "/cyberd/cyberd/#conclusion", 
            "text": "We describe and implement a motivated blockchain based search engine for the permanent web. A search engine is based on the content-addressable peer-to-peer paradigm and uses IPFS as a foundation. IPFS provide significant benefits in terms of resources consumption. CIDs as a primary object is robust in its simplicity. For every CID cyber\u2022rank is computed by a consensus computer with no single point of failure. cyber\u2022rank is a combination of Google's PageRank and Steem's rewards system. cyber\u2022Rank is resistant to Sybil attacks and is computed based on interactions with a graph of CIDs and it's internal relations. Embedded smart contracts offer fair compensations for those who participate in indexing, linking, auditing and curation process. The primary goal is indexing of peer-to-peer systems with self-authenticated data either stateless, such as IFSS, DAT, GIT, BitTorent, or stateful such as Bitcoin, Ethereum and other blockchains and tangles. Proposed market of linking offers necessary incentives for outsourcing computing part responsible for finding meaningful relations between objects. The proposed market of curation and auditing creates essential incentives for ranking high-quality links and objects. Dynamic snippets in search results make possible functionality necessary for the next generation search. Lazy oracles enable indexing of structured publicly verifiable data feeds in a highly competitive environment. A source code of a search engine is open source. Every bit of data accumulated by a blockchain is available for everybody for free. The performance of proposed software-hardware implementation is sufficient for seamless user interactions. Scalability of proposed implementation is enough to index all self-authenticated data that exist today. The blockchain is managed by a decentralized autonomous organization which functions under DPOS consensus algorithm. Thought a system provide necessary utility to offer an alternative for conventional search engines it is not limited to this use case either. The system is extendable for numerous applications and makes possible to design economically rational self-owned robots to spawn a market for AI outsourcing.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/cyberd/cyberd/#references-not-ready-yet", 
            "text": "[QGTC]  https://www.quora.com/How-many-pages-is-Google-crawling-every-day  PR [] RALF  http://merkle.com/papers/DAOdemocracyDraft.pdf  ENIGMA  https://www2.deloitte.com/content/dam/Deloitte/uk/Documents/Innovation/deloitte-uk-blockchain-full-report.pdf  RTB  https://en.wikipedia.org/wiki/Real-time_bidding", 
            "title": "References - not ready yet"
        }, 
        {
            "location": "/cyberd/cyberd/#todo", 
            "text": "Auditing and Curation . Probably need more details  Anonymity . Explain an economic difference between read search queries and write search queries.", 
            "title": "TODO"
        }, 
        {
            "location": "/cyberd/roadmap/", 
            "text": "Research directions\n\n\ncyber\u2022Congress: Helping humanity to evolve\ncyber\u2022Search: Building connected graph for humans, machines and other digital entities\nEva: Just another scalable, performant and intelligent consensus\ncybernode: Smart node manager\ncyber-search: Centralized index for decentralized systems\ncyberd: Motivated search engine for distributed web\n- quadratic voting\ncyber-markets: Measuring cybernomics in real time\n- measurement versus proof\nchaingear: Expensive registry for the digital age\ncyb: Architecture for better browser\ncyber.js: Java script library for cyber\u2022Search", 
            "title": "Research directions"
        }, 
        {
            "location": "/cyberd/roadmap/#research-directions", 
            "text": "cyber\u2022Congress: Helping humanity to evolve\ncyber\u2022Search: Building connected graph for humans, machines and other digital entities\nEva: Just another scalable, performant and intelligent consensus\ncybernode: Smart node manager\ncyber-search: Centralized index for decentralized systems\ncyberd: Motivated search engine for distributed web\n- quadratic voting\ncyber-markets: Measuring cybernomics in real time\n- measurement versus proof\nchaingear: Expensive registry for the digital age\ncyb: Architecture for better browser\ncyber.js: Java script library for cyber\u2022Search", 
            "title": "Research directions"
        }, 
        {
            "location": "/cybernode/about/", 
            "text": "", 
            "title": "About"
        }, 
        {
            "location": "/cybernode/dev-setup/", 
            "text": "", 
            "title": "Dev setup"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/", 
            "text": "K8s cheat sheet\n\n\nk8s dashboard\n\n\nLocal dashboard proxy:\n\n\nkubectl proxy\n\n\n\n\n\nGet cluster access token:\n\n\nkubectl config view \n|\n grep -A10 \nname: \n$(\nkubectl config current-context\n)\n \n|\n awk \n$1==\naccess-token:\n{print $2}\n\n\n\n\n\n\nGet logs of previously running container (if it failed and then restarts):\n\n\nkubectl logs mypod --previous\n\n\n\n\n\nReset GKE Node\n\n\ngcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko\n\n\n\n\n\nForward pod port on localhost\n\n\nkubectl port-forward -n monitoring prometheus-kube-prometheus-0 \n9090\n\n\n\n\n\n\nForward port of first found pod with specific namespace and label: \n\n\nkubectl port-forward -n chains \n$(\nkubectl get pod -n chains -l \nchain\n=\nbitcoind-btc -o \njsonpath\n=\n{.items[0].metadata.name}\n)\n \n8332\n\n\n\n\n\n\nElassandra commands\n\n\nGet nodes status:\n\n\nkubectl \nexec\n -n search elassandra-0 -- nodetool status\n\n\n\n\n\nOpen cqlsh CLI tool\n\n\nkubectl \nexec\n -it -n search elassandra-0 -- cqlsh\n\n\n\n\n\nDive into elassandra docker container shell(index stats, delete index commands examples)\n\n\nkubectl \nexec\n -n search -it elassandra-0 bash\ncurl -XGET \nlocalhost:9200/_cat/indices?v\npretty\n\ncurl -XDELETE \nlocalhost:9200/twitter?pretty", 
            "title": "K8s cheat sheet"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/#k8s-cheat-sheet", 
            "text": "", 
            "title": "K8s cheat sheet"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/#k8s-dashboard", 
            "text": "Local dashboard proxy:  kubectl proxy  Get cluster access token:  kubectl config view  |  grep -A10  name:  $( kubectl config current-context )   |  awk  $1== access-token: {print $2}   Get logs of previously running container (if it failed and then restarts):  kubectl logs mypod --previous", 
            "title": "k8s dashboard"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/#reset-gke-node", 
            "text": "gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko", 
            "title": "Reset GKE Node"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/#forward-pod-port-on-localhost", 
            "text": "kubectl port-forward -n monitoring prometheus-kube-prometheus-0  9090   Forward port of first found pod with specific namespace and label:   kubectl port-forward -n chains  $( kubectl get pod -n chains -l  chain = bitcoind-btc -o  jsonpath = {.items[0].metadata.name} )   8332", 
            "title": "Forward pod port on localhost"
        }, 
        {
            "location": "/cybernode/k8s-cheat-sheet/#elassandra-commands", 
            "text": "Get nodes status:  kubectl  exec  -n search elassandra-0 -- nodetool status  Open cqlsh CLI tool  kubectl  exec  -it -n search elassandra-0 -- cqlsh  Dive into elassandra docker container shell(index stats, delete index commands examples)  kubectl  exec  -n search -it elassandra-0 bash\ncurl -XGET  localhost:9200/_cat/indices?v pretty \ncurl -XDELETE  localhost:9200/twitter?pretty", 
            "title": "Elassandra commands"
        }, 
        {
            "location": "/cybernode/requirements/", 
            "text": "Requirements\n\n\nFor near future(1.0 release) cybernode will act as a helping tool around docker images to instantiate p2p systems and app around them. For the first release, we consider to do next:\n\n\n1\n)\n cybernode settings\n   cybernode \nhelp\n\n\n\n2\n)\n cybernode status   \n\n\n3\n)\n cybernode chains ethereum\n|\nethereum_kovan\n|\nethereum_classic start \n(\n--light --autostartup\n)\n\n   cybernode chains ethereum stop\n\n   cybernode chains bitcoin \nhelp\n\n   cybernode chains bitcoin start \n(\n--autostartup\n)\n\n   cybernode chains bitcoin stop\n\n   cybernode chains status\n   cybernide chains \nhelp\n\n\n\n4\n)\n cybernode p2p ipfs start \n(\n--autostartup\n)\n\n   cybernode p2p ipfs stop\n\n\n5\n)\n cybernode apps\n|\nplatforms cyb start\n   cybernode apps\n|\nplatforms cyb stop\n\n\n\n\n\nFirst run initialization\n\n\nFor first run of cybernode, user should specify where to store data for applications:\n\n\ncybernode.data.location\n=\n~/.cybernode/data\n\n\n\n\n\nBy default, all cybernode configs|state will be stored in \n~/.cybernode\n path. Nonchangeble.\n\n\nResearch:\n- [] Path autocompletion?\n\n\ncybernode chains\n subcomands\n\n\nUser shoud be able to run chains with specified options.\nResearch:\n- [] UI for chains\n- [] Light clients for chains\n\n\ncybernode p2p\n subcomands\n\n\nUser shoud be able to run p2p systems with specified options.\nResearch:\n- [] Key pair managment if chain reuqired\n\n\napps|platforms\n subcomands\n\n\nUser should be able to run DApps. Each DApp also have dependencies on p2p or chains entities. This subcommand should start only missing entities.\n\n\ncybernode status\n\n\nShould display to user cybernode running|stopped entities.\nAlso should be available for all subcommands.\n\n\ncybernode help\n\n\nShould display help message to the user.\nAlso should be available for all subcommands.\n\n\ncybernode settings\n\n\nShould print current settings.\n\n\nResearch:\n- [] Possible solutions for autocomplete for MacOS, Linux systems\n- [] Language and framework for CLI\n- [] Package distribution for various platforms(brew, npm, deb etc)\n- [] Initial Message??", 
            "title": "Requirements"
        }, 
        {
            "location": "/cybernode/requirements/#requirements", 
            "text": "For near future(1.0 release) cybernode will act as a helping tool around docker images to instantiate p2p systems and app around them. For the first release, we consider to do next:  1 )  cybernode settings\n   cybernode  help  2 )  cybernode status    3 )  cybernode chains ethereum | ethereum_kovan | ethereum_classic start  ( --light --autostartup ) \n   cybernode chains ethereum stop\n\n   cybernode chains bitcoin  help \n   cybernode chains bitcoin start  ( --autostartup ) \n   cybernode chains bitcoin stop\n\n   cybernode chains status\n   cybernide chains  help  4 )  cybernode p2p ipfs start  ( --autostartup ) \n   cybernode p2p ipfs stop 5 )  cybernode apps | platforms cyb start\n   cybernode apps | platforms cyb stop", 
            "title": "Requirements"
        }, 
        {
            "location": "/cybernode/requirements/#first-run-initialization", 
            "text": "For first run of cybernode, user should specify where to store data for applications:  cybernode.data.location = ~/.cybernode/data  By default, all cybernode configs|state will be stored in  ~/.cybernode  path. Nonchangeble.  Research:\n- [] Path autocompletion?", 
            "title": "First run initialization"
        }, 
        {
            "location": "/cybernode/requirements/#cybernode-chains-subcomands", 
            "text": "User shoud be able to run chains with specified options.\nResearch:\n- [] UI for chains\n- [] Light clients for chains", 
            "title": "cybernode chains subcomands"
        }, 
        {
            "location": "/cybernode/requirements/#cybernode-p2p-subcomands", 
            "text": "User shoud be able to run p2p systems with specified options.\nResearch:\n- [] Key pair managment if chain reuqired", 
            "title": "cybernode p2p subcomands"
        }, 
        {
            "location": "/cybernode/requirements/#appsplatforms-subcomands", 
            "text": "User should be able to run DApps. Each DApp also have dependencies on p2p or chains entities. This subcommand should start only missing entities.", 
            "title": "apps|platforms subcomands"
        }, 
        {
            "location": "/cybernode/requirements/#cybernode-status", 
            "text": "Should display to user cybernode running|stopped entities.\nAlso should be available for all subcommands.", 
            "title": "cybernode status"
        }, 
        {
            "location": "/cybernode/requirements/#cybernode-help", 
            "text": "Should display help message to the user.\nAlso should be available for all subcommands.", 
            "title": "cybernode help"
        }, 
        {
            "location": "/cybernode/requirements/#cybernode-settings", 
            "text": "Should print current settings.  Research:\n- [] Possible solutions for autocomplete for MacOS, Linux systems\n- [] Language and framework for CLI\n- [] Package distribution for various platforms(brew, npm, deb etc)\n- [] Initial Message??", 
            "title": "cybernode settings"
        }, 
        {
            "location": "/cybernode/chains/bitcoin/", 
            "text": "", 
            "title": "Bitcoin"
        }, 
        {
            "location": "/cybernode/chains/ethereum/", 
            "text": "Known issues\n\n\n\n\nFor \n2806264 block\n there is a transaction with size more than 1mb, general rule, size of item can be \n1mb.", 
            "title": "Ethereum"
        }, 
        {
            "location": "/cybernode/chains/ethereum/#known-issues", 
            "text": "For  2806264 block  there is a transaction with size more than 1mb, general rule, size of item can be  1mb.", 
            "title": "Known issues"
        }, 
        {
            "location": "/cybernode/components/chain-nodes-components/", 
            "text": "Chain Nodes Components\n\n\n\n\n\n\n\n\nComponent\n\n\nCluster Address\n\n\n\n\n\n\n\n\n\n\nParity 1.9.6 eth\n\n\nparity-eth.chains.svc:8545\n\n\n\n\n\n\nParity 1.9.6 etc\n\n\nparity-et\u0441.chains.svc:8545\n\n\n\n\n\n\nBitcoind 0.16.0\n\n\nbitcoind-btc.chains.svc:8332", 
            "title": "Chain Nodes Components"
        }, 
        {
            "location": "/cybernode/components/chain-nodes-components/#chain-nodes-components", 
            "text": "Component  Cluster Address      Parity 1.9.6 eth  parity-eth.chains.svc:8545    Parity 1.9.6 etc  parity-et\u0441.chains.svc:8545    Bitcoind 0.16.0  bitcoind-btc.chains.svc:8332", 
            "title": "Chain Nodes Components"
        }, 
        {
            "location": "/cybernode/components/components-requirments/", 
            "text": "Components requirements\n\n\nResources requirements to run components in gcloud cluster\n\n\n\n\n\n\n\n\nComponent\n\n\nMin CPU\n\n\nMax CPU\n\n\nMin RAM\n\n\nMax RAM\n\n\n\n\n\n\n\n\n\n\nKafka Broker\n\n\n1.5\n\n\n2\n\n\n10GB\n\n\n10GB\n\n\n\n\n\n\nKafka Exporter\n\n\n0.1\n\n\n0.2\n\n\n1GB\n\n\n2GB\n\n\n\n\n\n\nKafka Manager\n\n\n0.1\n\n\n0.2\n\n\n1GB\n\n\n2GB\n\n\n\n\n\n\nZoo Keeper\n\n\n0.1\n\n\n0.2\n\n\n1.25GB\n\n\n1.25GB\n\n\n\n\n\n\nElassandra\n\n\n3\n\n\n3.5\n\n\n20GB\n\n\n25GB\n\n\n\n\n\n\nGrafana\n\n\n0.1\n\n\n0.2\n\n\n100MB\n\n\n200MB\n\n\n\n\n\n\nPrometheus\n\n\n0.25\n\n\n1\n\n\n3GB\n\n\n3GB\n\n\n\n\n\n\nPrometheus Operator\n\n\n0.1\n\n\n0.2\n\n\n100MB\n\n\n200MB\n\n\n\n\n\n\nParity ETH\n\n\n3\n\n\n3\n\n\n15GB\n\n\n20GB\n\n\n\n\n\n\nParity ETC\n\n\n1\n\n\n1.5\n\n\n10GB\n\n\n15GB\n\n\n\n\n\n\nETH Pump\n\n\n1.5\n\n\n3\n\n\n5GB\n\n\n6GB\n\n\n\n\n\n\nETH Cassandra Dump\n\n\n1.5\n\n\n3\n\n\n3.75GB\n\n\n3.75GB\n\n\n\n\n\n\nETH Contract Summary\n\n\n1.5\n\n\n3\n\n\n3.75GB\n\n\n3.75GB\n\n\n\n\n\n\nETC Pump\n\n\n1.5\n\n\n3\n\n\n3.75GB\n\n\n3.75GB\n\n\n\n\n\n\nETC Cassandra Dump\n\n\n1.5\n\n\n3\n\n\n3.75GB\n\n\n3.75GB\n\n\n\n\n\n\nETC Contract Summary\n\n\n1.5\n\n\n3\n\n\n3.75GB\n\n\n3.75GB\n\n\n\n\n\n\nBitcoind\n\n\n2\n\n\n3\n\n\n20GB\n\n\n30GB\n\n\n\n\n\n\nBTC Pump*\n\n\n2.5\n\n\n2.5\n\n\n20GB\n\n\n20GB\n\n\n\n\n\n\nBTC Dump\n\n\n3\n\n\n3\n\n\n3.75GB\n\n\n3.75GB\n\n\n\n\n\n\nBTC Contract Summary\n\n\n1.5\n\n\n3\n\n\n3.75GB\n\n\n3.75GB\n\n\n\n\n\n\nSearch Api\n\n\n0.5\n\n\n1\n\n\n3.75GB\n\n\n3.75GB\n\n\n\n\n\n\nSearch Api Docs\n\n\n0.1\n\n\n0.2\n\n\n100MB\n\n\n200MB\n\n\n\n\n\n\n\n\n* - With cache size = 10GB", 
            "title": "Components requirements"
        }, 
        {
            "location": "/cybernode/components/components-requirments/#components-requirements", 
            "text": "Resources requirements to run components in gcloud cluster     Component  Min CPU  Max CPU  Min RAM  Max RAM      Kafka Broker  1.5  2  10GB  10GB    Kafka Exporter  0.1  0.2  1GB  2GB    Kafka Manager  0.1  0.2  1GB  2GB    Zoo Keeper  0.1  0.2  1.25GB  1.25GB    Elassandra  3  3.5  20GB  25GB    Grafana  0.1  0.2  100MB  200MB    Prometheus  0.25  1  3GB  3GB    Prometheus Operator  0.1  0.2  100MB  200MB    Parity ETH  3  3  15GB  20GB    Parity ETC  1  1.5  10GB  15GB    ETH Pump  1.5  3  5GB  6GB    ETH Cassandra Dump  1.5  3  3.75GB  3.75GB    ETH Contract Summary  1.5  3  3.75GB  3.75GB    ETC Pump  1.5  3  3.75GB  3.75GB    ETC Cassandra Dump  1.5  3  3.75GB  3.75GB    ETC Contract Summary  1.5  3  3.75GB  3.75GB    Bitcoind  2  3  20GB  30GB    BTC Pump*  2.5  2.5  20GB  20GB    BTC Dump  3  3  3.75GB  3.75GB    BTC Contract Summary  1.5  3  3.75GB  3.75GB    Search Api  0.5  1  3.75GB  3.75GB    Search Api Docs  0.1  0.2  100MB  200MB     * - With cache size = 10GB", 
            "title": "Components requirements"
        }, 
        {
            "location": "/cybernode/components/monitoring-components/", 
            "text": "Monitoring Components\n\n\nDuring lifetime cybernode collects various metrics using following components:\n\n\n\n\n\n\n\n\nComponent\n\n\nDescription\n\n\nCluster Address\n\n\nExternal\n\n\n\n\n\n\n\n\n\n\nPrometheus Operator(PO)\n\n\nManages Prometheus Configuration\n\n\n\n\n\n\n\n\n\n\nPrometheus\n\n\nMetrics Storage\n\n\nprometheus.monitoring.svc:9090\n\n\n\n\n\n\n\n\nDefault Service Monitor\n\n\nDefault Service Monitor for PO\n\n\n\n\n\n\n\n\n\n\nGrafana\n\n\nMetrics Alerts and Web UI\n\n\ngrafana.monitoring.svc:3000\n\n\ny\n\n\n\n\n\n\n\n\nPrometheus\n\n\nPrometheus\n is used as a main metrics storage. Components collect metrics and expose them via \n HTTP endpoint, that Prometheus pulls at configured interval(our is 15s).\n\n\nPrometheus Operator\n\n\nTo deploy Prometheus atop Kubernetes, we use \nPrometheus Operator\n.\n It consists of Prometheus Operator itself, that introduced to cluster \n \nCRDs\n: Prometheus,\n Service Monitor. PO automatically(dynamically) generate monitoring target configurations based on familiar Kubernetes label queries; \n no need to learn a Prometheus specific configuration language.\n\n\nDefault Service Monitor\n\n\nTo enable dynamic mapping for new pods to prometheus, service monitor(SM) is used. Each SM defines set of kubernetes \n label filters.\n\n\nGrafana\n\n\nTo visualize metrics collected by Prometheus we use \nGrafana\n. In order to configure Grafana, three\n \nconfig maps\n are used:\n\n\n\n\ngrafana-datasources\n. Define Prometheus datasource. \n\n\ngrafana-dashboards\n. Gather predefined dashboards into single folder. \n\n\ngrafana-dashboards-providers\n. Configuration for importing dashboards. \n\n\n\n\nGrafana Alerts\n\n\nAlso Grafana supports checking metrics satisfied defined criteria. If criteria is violated, than notification is raised \n to setup alert channels(in our case Telegram Group). To view list of available alerts check \n \"{grafana_host}/alerting/list\" page.", 
            "title": "Monitoring Components"
        }, 
        {
            "location": "/cybernode/components/monitoring-components/#monitoring-components", 
            "text": "During lifetime cybernode collects various metrics using following components:     Component  Description  Cluster Address  External      Prometheus Operator(PO)  Manages Prometheus Configuration      Prometheus  Metrics Storage  prometheus.monitoring.svc:9090     Default Service Monitor  Default Service Monitor for PO      Grafana  Metrics Alerts and Web UI  grafana.monitoring.svc:3000  y", 
            "title": "Monitoring Components"
        }, 
        {
            "location": "/cybernode/components/monitoring-components/#prometheus", 
            "text": "Prometheus  is used as a main metrics storage. Components collect metrics and expose them via \n HTTP endpoint, that Prometheus pulls at configured interval(our is 15s).", 
            "title": "Prometheus"
        }, 
        {
            "location": "/cybernode/components/monitoring-components/#prometheus-operator", 
            "text": "To deploy Prometheus atop Kubernetes, we use  Prometheus Operator .\n It consists of Prometheus Operator itself, that introduced to cluster \n  CRDs : Prometheus,\n Service Monitor. PO automatically(dynamically) generate monitoring target configurations based on familiar Kubernetes label queries; \n no need to learn a Prometheus specific configuration language.", 
            "title": "Prometheus Operator"
        }, 
        {
            "location": "/cybernode/components/monitoring-components/#default-service-monitor", 
            "text": "To enable dynamic mapping for new pods to prometheus, service monitor(SM) is used. Each SM defines set of kubernetes \n label filters.", 
            "title": "Default Service Monitor"
        }, 
        {
            "location": "/cybernode/components/monitoring-components/#grafana", 
            "text": "To visualize metrics collected by Prometheus we use  Grafana . In order to configure Grafana, three\n  config maps  are used:   grafana-datasources . Define Prometheus datasource.   grafana-dashboards . Gather predefined dashboards into single folder.   grafana-dashboards-providers . Configuration for importing dashboards.", 
            "title": "Grafana"
        }, 
        {
            "location": "/cybernode/components/monitoring-components/#grafana-alerts", 
            "text": "Also Grafana supports checking metrics satisfied defined criteria. If criteria is violated, than notification is raised \n to setup alert channels(in our case Telegram Group). To view list of available alerts check \n \"{grafana_host}/alerting/list\" page.", 
            "title": "Grafana Alerts"
        }, 
        {
            "location": "/cybernode/staging/chains/", 
            "text": "Current chains nodes tables\n\n\n\n\n\n\n\n\nApp\n\n\ndata path\n\n\nport\n\n\ncreds\n\n\ncurrent size\n\n\n\n\n\n\n\n\n\n\nbitcoind - btc\n\n\n/cyberdata/ssd05tb/bitcoind\n\n\n8332\n\n\ncyber:cyber\n\n\n184 gb\n\n\n\n\n\n\nabc    -   bth\n\n\n/cyberdata/abc\n\n\n18332\n\n\nbitcoin:password\n\n\n156 gb\n\n\n\n\n\n\nparity -   eth\n\n\n/cyberdata/ssd05tb/eth\n\n\n8545\n\n\n\n\n56  gb\n\n\n\n\n\n\nparity -   eth_c\n\n\n/cyberdata/ssd05tb/eth_c\n\n\n18545\n\n\n\n\n5.3 gb\n\n\n\n\n\n\n\n\nLocal forwarding port for chains\n\n\nssh -L 18332:localhost:18332 -L 8332:localhost:8332 \\\n-L 18545:localhost:18545 -L 8545:localhost:8545 \\\nmars@staging.cyber.fund  -p 33322\n\n\n\n\n\nCommands used to run chain and live probe\n\n\nBitcoind\n\n\nRun:\n\n\nsudo  docker run -d -p 8332:8332 --name bitcoind  --restart always \\\n-v /cyberdata/ssd05tb/bitcoind:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\\n-server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0\n\n\n\n\n\nProbe:\n\n\ncurl -X POST -H \nContent-Type: application/json\n \\\n--data \n{\njsonrpc\n:\n2.0\n,\nmethod\n:\ngetblockhash\n,\nparams\n:[1],\nid\n:1}\n \\\nhttp://127.0.0.1:8332 -u cyber:cyber\n\n\n\n\n\nBitcoinABC\n\n\nRun:\n\n\nsudo  docker run -d -p 18332:8332 --name abc  --restart always \n-v /cyberdata/abc:/data zquestz/bitcoin-abc:0.16.1 bitcoind -server -rest\n\n\n\n\n\nProbe:\n\n\ncurl -X POST -H \nContent-Type: application/json\n \\\n--data \n{\njsonrpc\n:\n2.0\n,\nmethod\n:\ngetblockhash\n,\nparams\n:[1],\nid\n:1}\n \\\nhttp://127.0.0.1:18332 -u cyber:cyber\n\n\n\n\n\nParity\n\n\nRun:\n\n\nsudo  docker run -d -p 8545:8545 --name parity_eth  --restart always \\\n-v /cyberdata/ssd05tb/eth:/cyberdata parity/parity:stable \\\n--db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 10\n\n\n\n\n\nProbe:\n\n\ncurl -X POST -H \nContent-Type: application/json\n \\\n--data \n{\njsonrpc\n:\n2.0\n,\nmethod\n:\neth_getBlockByNumber\n,\nparams\n:[\n0x1\n, true],\nid\n:1}\n\\\n http://127.0.0.1:8545\n\n\n\n\n\nParity --chain classic\n\n\nRun:\n\n\nsudo  docker run -d -p 18545:8545 --name parity_etc  --restart always \\\n-v /cyberdata/ssd05tb/eth_c:/cyberdata parity/parity:stable --db-path /cyberdata \\\n--jsonrpc-hosts all --jsonrpc-interface all --chain classic --jsonrpc-threads 10\n\n\n\n\n\nProbe:\n\n\ncurl -X POST -H \nContent-Type: application/json\n \\\n--data \n{\njsonrpc\n:\n2.0\n,\nmethod\n:\neth_getBlockByNumber\n,\nparams\n:[\n0x1\n, true],\nid\n:1}\n \\\nhttp://127.0.0.1:18545", 
            "title": "Chains"
        }, 
        {
            "location": "/cybernode/staging/chains/#current-chains-nodes-tables", 
            "text": "App  data path  port  creds  current size      bitcoind - btc  /cyberdata/ssd05tb/bitcoind  8332  cyber:cyber  184 gb    abc    -   bth  /cyberdata/abc  18332  bitcoin:password  156 gb    parity -   eth  /cyberdata/ssd05tb/eth  8545   56  gb    parity -   eth_c  /cyberdata/ssd05tb/eth_c  18545   5.3 gb", 
            "title": "Current chains nodes tables"
        }, 
        {
            "location": "/cybernode/staging/chains/#local-forwarding-port-for-chains", 
            "text": "ssh -L 18332:localhost:18332 -L 8332:localhost:8332 \\\n-L 18545:localhost:18545 -L 8545:localhost:8545 \\\nmars@staging.cyber.fund  -p 33322", 
            "title": "Local forwarding port for chains"
        }, 
        {
            "location": "/cybernode/staging/chains/#commands-used-to-run-chain-and-live-probe", 
            "text": "", 
            "title": "Commands used to run chain and live probe"
        }, 
        {
            "location": "/cybernode/staging/chains/#bitcoind", 
            "text": "Run:  sudo  docker run -d -p 8332:8332 --name bitcoind  --restart always \\\n-v /cyberdata/ssd05tb/bitcoind:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\\n-server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0  Probe:  curl -X POST -H  Content-Type: application/json  \\\n--data  { jsonrpc : 2.0 , method : getblockhash , params :[1], id :1}  \\\nhttp://127.0.0.1:8332 -u cyber:cyber", 
            "title": "Bitcoind"
        }, 
        {
            "location": "/cybernode/staging/chains/#bitcoinabc", 
            "text": "Run:  sudo  docker run -d -p 18332:8332 --name abc  --restart always \n-v /cyberdata/abc:/data zquestz/bitcoin-abc:0.16.1 bitcoind -server -rest  Probe:  curl -X POST -H  Content-Type: application/json  \\\n--data  { jsonrpc : 2.0 , method : getblockhash , params :[1], id :1}  \\\nhttp://127.0.0.1:18332 -u cyber:cyber", 
            "title": "BitcoinABC"
        }, 
        {
            "location": "/cybernode/staging/chains/#parity", 
            "text": "Run:  sudo  docker run -d -p 8545:8545 --name parity_eth  --restart always \\\n-v /cyberdata/ssd05tb/eth:/cyberdata parity/parity:stable \\\n--db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 10  Probe:  curl -X POST -H  Content-Type: application/json  \\\n--data  { jsonrpc : 2.0 , method : eth_getBlockByNumber , params :[ 0x1 , true], id :1} \\\n http://127.0.0.1:8545", 
            "title": "Parity"
        }, 
        {
            "location": "/cybernode/staging/chains/#parity-chain-classic", 
            "text": "Run:  sudo  docker run -d -p 18545:8545 --name parity_etc  --restart always \\\n-v /cyberdata/ssd05tb/eth_c:/cyberdata parity/parity:stable --db-path /cyberdata \\\n--jsonrpc-hosts all --jsonrpc-interface all --chain classic --jsonrpc-threads 10  Probe:  curl -X POST -H  Content-Type: application/json  \\\n--data  { jsonrpc : 2.0 , method : eth_getBlockByNumber , params :[ 0x1 , true], id :1}  \\\nhttp://127.0.0.1:18545", 
            "title": "Parity --chain classic"
        }, 
        {
            "location": "/cybernode/staging/continuous-delivery/", 
            "text": "Circle Ci job example\n\n\nTo enable auto staging-redeploy for your application you should:\n\n\n\n\n\n\nEnable \nstaging\n docker images build. For example, you can use dockerhub tag \"STAGING\". \n\n\n\n\n\n\nAdd to \nthe repo\n you image running script.\n\n\n\n\n\n\nAdd a CircleCi job, depended on docker build\npush job. It's all.\n\n\n\n\n\n\nExample: build and deploy for all commits in master\n\n\naliases:\n - \nstaging_filter\n    filters:\n      tags:\n        only: /.*/\n      branches:\n        only: master\n\n\njobs:        \n  build_and_push_image_staging_image:\n    working_directory: ~/build\n    docker:\n      - image: docker:17.05.0-ce-git\n    steps:\n      - checkout\n      - setup_remote_docker:\n          version: 17.05.0-ce\n      - run:\n          name: Build and upload staging images\n          command: |\n            docker build -t build/cs-search-api -f ./devops/search-api/search-api ./\n            docker login -u $DOCKER_USER -p $DOCKER_PASS\n            docker tag build/cs-search-api cybernode/cs-search-api:staging\n            docker push cybernode/cs-search-api:staging\n\n    deploy_stagin_image:\n      working_directory: ~/build\n      docker:\n        - image: docker:17.05.0-ce-git\n      steps:\n        - run:\n            name: Rerun image on staging\n            command: \n-\n              ssh mars@staging.cyber.fund -p 33322 -o \nStrictHostKeyChecking no\n\n              \ncd /cyberdata/cybernode \n git pull \n sh /cyberdata/cybernode/up.search.sh\n\n\n\nworkflows:\n  version: 2\n  staging_cd:\n    jobs:\n      - build_and_push_image_staging_image:\n          \n: *staging_filter\n      - deploy_stagin_image:\n          \n: *staging_filter\n          requires:\n            - build_and_push_image_staging_image", 
            "title": "Continuous delivery"
        }, 
        {
            "location": "/cybernode/staging/continuous-delivery/#circle-ci-job-example", 
            "text": "To enable auto staging-redeploy for your application you should:    Enable  staging  docker images build. For example, you can use dockerhub tag \"STAGING\".     Add to  the repo  you image running script.    Add a CircleCi job, depended on docker build push job. It's all.", 
            "title": "Circle Ci job example"
        }, 
        {
            "location": "/cybernode/staging/continuous-delivery/#example-build-and-deploy-for-all-commits-in-master", 
            "text": "aliases:\n -  staging_filter\n    filters:\n      tags:\n        only: /.*/\n      branches:\n        only: master\n\n\njobs:        \n  build_and_push_image_staging_image:\n    working_directory: ~/build\n    docker:\n      - image: docker:17.05.0-ce-git\n    steps:\n      - checkout\n      - setup_remote_docker:\n          version: 17.05.0-ce\n      - run:\n          name: Build and upload staging images\n          command: |\n            docker build -t build/cs-search-api -f ./devops/search-api/search-api ./\n            docker login -u $DOCKER_USER -p $DOCKER_PASS\n            docker tag build/cs-search-api cybernode/cs-search-api:staging\n            docker push cybernode/cs-search-api:staging\n\n    deploy_stagin_image:\n      working_directory: ~/build\n      docker:\n        - image: docker:17.05.0-ce-git\n      steps:\n        - run:\n            name: Rerun image on staging\n            command:  -\n              ssh mars@staging.cyber.fund -p 33322 -o  StrictHostKeyChecking no \n               cd /cyberdata/cybernode   git pull   sh /cyberdata/cybernode/up.search.sh \n\n\nworkflows:\n  version: 2\n  staging_cd:\n    jobs:\n      - build_and_push_image_staging_image:\n           : *staging_filter\n      - deploy_stagin_image:\n           : *staging_filter\n          requires:\n            - build_and_push_image_staging_image", 
            "title": "Example: build and deploy for all commits in master"
        }, 
        {
            "location": "/cybernode/staging/kubernetes/", 
            "text": "Prerequisites\n\n\n\n\n\n\nInstall \nkubectl\n\nfor interacting with Kubernetes.\n\n\n\n\n\n\nInstall \nminikube\n if you want to\ntest setup locally. Make sure to install driver for your OS or else slow\nVirtualBox will be used.\n\n\n\n\n\n\nRunning \nminikube\n for local testing\n\n\n$ minikube start --vm-driver kvm2\nStarting local Kubernetes v1.9.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.\n\n\n\n\n\nCheck status.\n\n\n$ minikube status\nminikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.39.6\n\n\n\n\n\nRun dashboard.\n\n\n$ minikube dashboard\nOpening kubernetes dashboard in default browser...\n\n\n\n\n\nRun single container in a cluster\n\n\n$ kubectl run miniecho --image=k8s.gcr.io/echoserver:1.9 --port=8080\n\n\n\n\n\nThis adds \ndeployment\n called \nminiecho\n to cluster, runs Docker \nechoserver\n\nimage in it and tells docker to open port 8080. The port is not accessible\nuntil we create \nservice\n.\n\n\n$ kubectl expose deployment miniecho --type=NodePort\nservice \nminiecho\n exposed\n\n\n\n\n\nThis makes \nminiecho\n service accessible from your machine on random port.\n\n\n$ minikube service list\n|-------------|----------------------|---------------------------|\n|  NAMESPACE  |         NAME         |            URL            |\n|-------------|----------------------|---------------------------|\n| default     | kubernetes           | No node port              |\n| default     | miniecho             | http://192.168.39.6:30426 |\n| kube-system | kube-dns             | No node port              |\n| kube-system | kubernetes-dashboard | http://192.168.39.6:30000 |\n|-------------|----------------------|---------------------------|", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/cybernode/staging/kubernetes/#prerequisites", 
            "text": "Install  kubectl \nfor interacting with Kubernetes.    Install  minikube  if you want to\ntest setup locally. Make sure to install driver for your OS or else slow\nVirtualBox will be used.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/cybernode/staging/kubernetes/#running-minikube-for-local-testing", 
            "text": "$ minikube start --vm-driver kvm2\nStarting local Kubernetes v1.9.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.  Check status.  $ minikube status\nminikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.39.6  Run dashboard.  $ minikube dashboard\nOpening kubernetes dashboard in default browser...", 
            "title": "Running minikube for local testing"
        }, 
        {
            "location": "/cybernode/staging/kubernetes/#run-single-container-in-a-cluster", 
            "text": "$ kubectl run miniecho --image=k8s.gcr.io/echoserver:1.9 --port=8080  This adds  deployment  called  miniecho  to cluster, runs Docker  echoserver \nimage in it and tells docker to open port 8080. The port is not accessible\nuntil we create  service .  $ kubectl expose deployment miniecho --type=NodePort\nservice  miniecho  exposed  This makes  miniecho  service accessible from your machine on random port.  $ minikube service list\n|-------------|----------------------|---------------------------|\n|  NAMESPACE  |         NAME         |            URL            |\n|-------------|----------------------|---------------------------|\n| default     | kubernetes           | No node port              |\n| default     | miniecho             | http://192.168.39.6:30426 |\n| kube-system | kube-dns             | No node port              |\n| kube-system | kubernetes-dashboard | http://192.168.39.6:30000 |\n|-------------|----------------------|---------------------------|", 
            "title": "Run single container in a cluster"
        }, 
        {
            "location": "/cybernode/staging/run/", 
            "text": "Port mapping\n\n\nPorts that are not accessible from internet. Use ssh forwarding.\n\n\n\n\n8332 - chain btc\n\n\n18332 - chain bth\n\n\n8545 - chain eth\n\n\n\n\n18545 - chain etc\n\n\n\n\n\n\n2181 - zookeper\n\n\n\n\n9092 - kafka\n\n\n9042 - elassandra search\n\n\n9043 - elassandra markets\n\n\n9200 - elastic rest search\n\n\n9201 - elastic rest markets\n\n\n9300 - elastic transport search\n\n\n9301 - elastic transport markets\n\n\n\n\nPorts open for internet.\n\n\n\n\n32001 - portainer\n\n\n32002 - grafana (monitoring)\n\n\n32500 - browser-ui\n\n\n32600 - chaingear 1.0 api\n\n\n32800 - markets rest api\n\n\n32801 - markets stream api\n\n\n32901 - search api\n\n\n\n\nDocker data\n\n\nRun chains\n\n\nChains should be run manually once and ideally should not require addition work. To run chains, execute:\n\n\nsudo bash /cyberdata/cybernode/chains/up.sh\n\n\n\n\n\nRun portainer.io\n\n\nRun once\n\n\ndocker run -d -p 32001:9000 --name portainer --restart always \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /cyberdata/portainer:/data portainer/portainer\n\n\n\n\n\nRun components\n\n\nsudo bash /cyberdata/cybernode/up.browser.sh\nsudo bash /cyberdata/cybernode/up.chaingear.sh\nsudo bash /cyberdata/cybernode/up.ethereum.index.sh\nsudo bash /cyberdata/cybernode/up.search.api.sh\n\n\n\n\n\nThen go into each \nkafka\n, \nelassandra\n and run\n\n\ndocker-compose up -d\n\n\n\n\n\nTo access \nmonitoring\n dashboard, run \ndocker-compose up -d\n\nin \nmonitoring\n as well.", 
            "title": "Run"
        }, 
        {
            "location": "/cybernode/staging/run/#port-mapping", 
            "text": "Ports that are not accessible from internet. Use ssh forwarding.   8332 - chain btc  18332 - chain bth  8545 - chain eth   18545 - chain etc    2181 - zookeper   9092 - kafka  9042 - elassandra search  9043 - elassandra markets  9200 - elastic rest search  9201 - elastic rest markets  9300 - elastic transport search  9301 - elastic transport markets   Ports open for internet.   32001 - portainer  32002 - grafana (monitoring)  32500 - browser-ui  32600 - chaingear 1.0 api  32800 - markets rest api  32801 - markets stream api  32901 - search api", 
            "title": "Port mapping"
        }, 
        {
            "location": "/cybernode/staging/run/#docker-data", 
            "text": "", 
            "title": "Docker data"
        }, 
        {
            "location": "/cybernode/staging/run/#run-chains", 
            "text": "Chains should be run manually once and ideally should not require addition work. To run chains, execute:  sudo bash /cyberdata/cybernode/chains/up.sh", 
            "title": "Run chains"
        }, 
        {
            "location": "/cybernode/staging/run/#run-portainerio", 
            "text": "Run once  docker run -d -p 32001:9000 --name portainer --restart always \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /cyberdata/portainer:/data portainer/portainer", 
            "title": "Run portainer.io"
        }, 
        {
            "location": "/cybernode/staging/run/#run-components", 
            "text": "sudo bash /cyberdata/cybernode/up.browser.sh\nsudo bash /cyberdata/cybernode/up.chaingear.sh\nsudo bash /cyberdata/cybernode/up.ethereum.index.sh\nsudo bash /cyberdata/cybernode/up.search.api.sh  Then go into each  kafka ,  elassandra  and run  docker-compose up -d  To access  monitoring  dashboard, run  docker-compose up -d \nin  monitoring  as well.", 
            "title": "Run components"
        }, 
        {
            "location": "/cybernode/staging/setup/", 
            "text": "Mars is our \nstaging\n server. You may reuse its config for your own.\n\n\nCurrent Mars storage setup\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nSize\n\n\nLABEL\n\n\nMapping\n\n\n\n\n\n\n\n\n\n\nnvme0n1p1\n\n\nssd\n\n\n1 tb\n\n\nSDD1\n\n\n/cyberdata/ssd1tb\n\n\n\n\n\n\nnvme2n1p1\n\n\nssd\n\n\n500 gb\n\n\nSDD2\n\n\n/cyberdata/ssd05tb\n\n\n\n\n\n\nsdc1\n\n\nhdd\n\n\n3.5 tb\n\n\nHDD3\n\n\n/cyberdata/elassandra-markets\n\n\n\n\n\n\nsdb1\n\n\nhdd\n\n\n3.5 tb\n\n\nHDD2\n\n\n/cyberdata/elassandra-search\n\n\n\n\n\n\nsda1\n\n\nhdd\n\n\n3.5 tb\n\n\nHDD1\n\n\n/backupsData\n\n\n\n\n\n\nsdd1\n\n\nhdd\n\n\n3.5 tb\n\n\ncyberdata\n\n\n/cyberdata\n\n\n\n\n\n\n\n\n/cyberdata\n contents\n\n\n\n\n/cyberdata/portainer/\n - portainer data directory\n\n\n/cyberdata/cybernode/\n - clone of repo https://github.com/cyberFund/cybernode\n\n\n\n\nUseful commands\n\n\nFormat partition:\n\n\nsudo mkfs -t ext4 -L SSD1 /dev/nvme0n1p1\nsudo mkfs -t ext4 -L SSD2 /dev/nvme2n1p1\n\n\n\n\n\n/etc/fstab addition lines:\n\n\nLABEL=cyberdata /cyberdata ext4 defaults 0 0\nLABEL=SSD2 /cyberdata/ssd05tb ext4 nofail 0 0\nLABEL=SSD1 /cyberdata/ssd1tb ext4 nofail 0 0\nLABEL=HDD1 /backupsData ext4 defaults 0 0\nLABEL=HDD2 /cyberdata/elassandra-search ext4 defaults 0 0 \nLABEL=HDD3 /cyberdata/elassandra-markets ext4 defaults 0 0 \n\n\n\n\n\nCopy data:\n\n\nsudo cp -R --verbose /backupsData/chain/eth/eth /cyberdata/ssd05tb/eth/\nsudo cp -R --verbose /cyberdata/bitcoind /cyberdata/ssd05tb/bitcoind/", 
            "title": "Setup"
        }, 
        {
            "location": "/cybernode/staging/setup/#current-mars-storage-setup", 
            "text": "Name  Type  Size  LABEL  Mapping      nvme0n1p1  ssd  1 tb  SDD1  /cyberdata/ssd1tb    nvme2n1p1  ssd  500 gb  SDD2  /cyberdata/ssd05tb    sdc1  hdd  3.5 tb  HDD3  /cyberdata/elassandra-markets    sdb1  hdd  3.5 tb  HDD2  /cyberdata/elassandra-search    sda1  hdd  3.5 tb  HDD1  /backupsData    sdd1  hdd  3.5 tb  cyberdata  /cyberdata", 
            "title": "Current Mars storage setup"
        }, 
        {
            "location": "/cybernode/staging/setup/#cyberdata-contents", 
            "text": "/cyberdata/portainer/  - portainer data directory  /cyberdata/cybernode/  - clone of repo https://github.com/cyberFund/cybernode", 
            "title": "/cyberdata contents"
        }, 
        {
            "location": "/cybernode/staging/setup/#useful-commands", 
            "text": "Format partition:  sudo mkfs -t ext4 -L SSD1 /dev/nvme0n1p1\nsudo mkfs -t ext4 -L SSD2 /dev/nvme2n1p1  /etc/fstab addition lines:  LABEL=cyberdata /cyberdata ext4 defaults 0 0\nLABEL=SSD2 /cyberdata/ssd05tb ext4 nofail 0 0\nLABEL=SSD1 /cyberdata/ssd1tb ext4 nofail 0 0\nLABEL=HDD1 /backupsData ext4 defaults 0 0\nLABEL=HDD2 /cyberdata/elassandra-search ext4 defaults 0 0 \nLABEL=HDD3 /cyberdata/elassandra-markets ext4 defaults 0 0   Copy data:  sudo cp -R --verbose /backupsData/chain/eth/eth /cyberdata/ssd05tb/eth/\nsudo cp -R --verbose /cyberdata/bitcoind /cyberdata/ssd05tb/bitcoind/", 
            "title": "Useful commands"
        }, 
        {
            "location": "/\u0441haingear/contracts/", 
            "text": "Contracts Overview\n\n\n/chaingear\n\n\n\n\nChaingear\n allows any user to create his own registry. Building fee is collecting by new registry creation. All builded registries are tokenized with ERC721 NFT token standard and saved in Chaingear metaregistry with registry metainformation. Creator of registry may transfer tokenized ownership of registry and destroy registry with token burning. Chaingear supports multiple benefitiaries witch have access to collected fees.\n\n\n\n\n###### depends on:\n    - \nERC721Token\n\n    - \nSplitPaymentChangeable\n\n    - \nChaingearCore\n\n    - \nRegistry (int)\n\n\n\n\nChaingearCore\n holds general logic of Chaingear. Allows change chaingear's metainformation, amount of registration fee, set registry creator contract.\n\n\n\n\n###### depends on:\n    - \nRegistryBase\n\n    - \nIPFSeable\n\n    - \nDestructible\n\n    - \nPausable\n\n    - \nRegistryCreator (int)\n\n\n\n\n\n\nRegistryBase\n holds struct of data with describes registry metainformation which associated with token, views function for registry metainformation.\n\n\n\n\n\n\nRegistryCreator\n contains the bytecode of current version of Registry. This bytecode used by Chaingear for Registry Creation. Registry Creator address can be changed in Chaingear by it's owner. Creation function can be only called by setted out chaingear address.\n\n\n\n\n\n\n###### depends on:\n    - \nRegistry\n\n    - \nOwnable\n\n\n/common\n\n\n\n\n\n\nSeriality\n is a library for serializing and de-serializing all the Solidity types in a very efficient way which mostly written in solidity-assembly.\n\n\n\n\n\n\nIPFSeable\n contains logic which allows view and save links to CID in IPFS with ABI, source code and contract metainformation. Inherited by Chaingear and Registry.\n\n\n\n\n\n\n###### depends on:\n    - \nOwnable\n\n\n\n\nRegistySafe\n allows inherited contract transfer ETHs to safe and client claim from safe, accounting logic holded by inherited contract. Inherited by Chaingear and Registry.\n\n\n\n\n###### depends on:\n    - \nOwnable\n\n    - \nDestructible\n\n\n\n\nSplitPaymentChangeable\n allows add beneficiaries to contract (addresses and shares amount) and change payee address. Beneficiaries can claim ETHs from contract proportionally to their shares. Inherited by Chaingear and Registry.\n\n\n\n\n###### depends on:\n    - \nOwnable\n\n    - \nSplitPayment\n\n\n/registry\n\n\n\n\nChaingeareable\n holds basic logic of Registry as registry basic information, balance and fees amount. Contains getters and setters for registry name, desciption, tags, entry base address.\n\n\n\n\n###### depends on:\n    - \nIPFSeable\n\n    - \nRegistryAccessControl\n\n\n\n\n\n\nEntryBase\n base for \nEntryCore\n. Holds entry metainformation and interfaces of functions (\nC\n \nR\n \nUD\n) which should be implemented in \nEntryCore\n.\n\n\n\n\n\n\nEntryCore\n partilly codegenerated contract where registry creator setup their custom entry structure and setters/getters. \nEntryCore\n then goes to Registry constructor as bytecode, where \nEntryCore\n contract creates. Registry goes as owner of contract (and acts as proxy) with entries creating, transfering and deleting.\n\n\n\n\n\n\n###### depends on:\n    - \nEntryBase\n\n    - \nOwnable\n\n    - \nSeriality\n\n\n\n\nRegistry\n contract witch tokenize entries as NFT tokens via ERC721 standard. Users can create entries with structure described in \nEntryCore\n sent serialized params according to entry access policy. Also users can fund entries with ETHs which send to \nRegistrySafe\n where owner of entry can claim funds.\n\n\n\n\n###### depends on:\n    - \nChaingeareable\n\n    - \nERC721Token\n\n    - \nSplitPaymentChangeable\n\n    - \nEntryBase (int)\n\n\n\n\nRegistryAccessControl\n holds logic of controlling registry and accessing to entries creation. Policy options to entries creation are OnlyCreator, Whitelist, AllUsers. Chaingear acts as owner of registry and creator of registry acts of creator with separated policies to Registry functions.\n\n\n\n\n###### depends on:\n    - \nOwnable\n\n    - \nDestructible", 
            "title": "Contracts Overview"
        }, 
        {
            "location": "/\u0441haingear/contracts/#contracts-overview", 
            "text": "", 
            "title": "Contracts Overview"
        }, 
        {
            "location": "/\u0441haingear/contracts/#chaingear", 
            "text": "Chaingear  allows any user to create his own registry. Building fee is collecting by new registry creation. All builded registries are tokenized with ERC721 NFT token standard and saved in Chaingear metaregistry with registry metainformation. Creator of registry may transfer tokenized ownership of registry and destroy registry with token burning. Chaingear supports multiple benefitiaries witch have access to collected fees.   ###### depends on:\n    -  ERC721Token \n    -  SplitPaymentChangeable \n    -  ChaingearCore \n    -  Registry (int)   ChaingearCore  holds general logic of Chaingear. Allows change chaingear's metainformation, amount of registration fee, set registry creator contract.   ###### depends on:\n    -  RegistryBase \n    -  IPFSeable \n    -  Destructible \n    -  Pausable \n    -  RegistryCreator (int)    RegistryBase  holds struct of data with describes registry metainformation which associated with token, views function for registry metainformation.    RegistryCreator  contains the bytecode of current version of Registry. This bytecode used by Chaingear for Registry Creation. Registry Creator address can be changed in Chaingear by it's owner. Creation function can be only called by setted out chaingear address.    ###### depends on:\n    -  Registry \n    -  Ownable", 
            "title": "/chaingear"
        }, 
        {
            "location": "/\u0441haingear/contracts/#common", 
            "text": "Seriality  is a library for serializing and de-serializing all the Solidity types in a very efficient way which mostly written in solidity-assembly.    IPFSeable  contains logic which allows view and save links to CID in IPFS with ABI, source code and contract metainformation. Inherited by Chaingear and Registry.    ###### depends on:\n    -  Ownable   RegistySafe  allows inherited contract transfer ETHs to safe and client claim from safe, accounting logic holded by inherited contract. Inherited by Chaingear and Registry.   ###### depends on:\n    -  Ownable \n    -  Destructible   SplitPaymentChangeable  allows add beneficiaries to contract (addresses and shares amount) and change payee address. Beneficiaries can claim ETHs from contract proportionally to their shares. Inherited by Chaingear and Registry.   ###### depends on:\n    -  Ownable \n    -  SplitPayment", 
            "title": "/common"
        }, 
        {
            "location": "/\u0441haingear/contracts/#registry", 
            "text": "Chaingeareable  holds basic logic of Registry as registry basic information, balance and fees amount. Contains getters and setters for registry name, desciption, tags, entry base address.   ###### depends on:\n    -  IPFSeable \n    -  RegistryAccessControl    EntryBase  base for  EntryCore . Holds entry metainformation and interfaces of functions ( C   R   UD ) which should be implemented in  EntryCore .    EntryCore  partilly codegenerated contract where registry creator setup their custom entry structure and setters/getters.  EntryCore  then goes to Registry constructor as bytecode, where  EntryCore  contract creates. Registry goes as owner of contract (and acts as proxy) with entries creating, transfering and deleting.    ###### depends on:\n    -  EntryBase \n    -  Ownable \n    -  Seriality   Registry  contract witch tokenize entries as NFT tokens via ERC721 standard. Users can create entries with structure described in  EntryCore  sent serialized params according to entry access policy. Also users can fund entries with ETHs which send to  RegistrySafe  where owner of entry can claim funds.   ###### depends on:\n    -  Chaingeareable \n    -  ERC721Token \n    -  SplitPaymentChangeable \n    -  EntryBase (int)   RegistryAccessControl  holds logic of controlling registry and accessing to entries creation. Policy options to entries creation are OnlyCreator, Whitelist, AllUsers. Chaingear acts as owner of registry and creator of registry acts of creator with separated policies to Registry functions.   ###### depends on:\n    -  Ownable \n    -  Destructible", 
            "title": "/registry"
        }, 
        {
            "location": "/\u0441haingear/development/", 
            "text": "Configuring, development and deploying\n\n\nDeploy contract:\n\n\nparity ui --chain=kovan\n\ntruffle migrate --network=kovan\n\n\n\n\n\nPS: approve transaction in parity ui (http://127.0.0.1:8180/)\n\n\nBuild contract in file:\n\n\ntruffle-flattener contracts/common/Chaingeareable.sol \n app/src/Chaingeareable.sol\n\n\n\n\n\nLinting:\n\n\nnpm install -g solium\n\nsolium -d contracts\n\n\n\n\n\nDevelopment environment\n\n\nRecommending to use \nRemix Ethereum Online IDE\n  or \ndesktop electron-based Remix IDE\n\n\nPS: to import to IDE open-zeppelin contacts follow this:\n\n\nimport \ngithub.com/openZeppelin/zeppelin-solidity/contracts/ownership/Ownable.sol\n;\n\n\n\n\n\nTruffle + Ganache workflow\n\n\nInstall Ganache from \nlatest release\n, then =\n\n\nnpm install -g ganache-cli\n\n\n\n\n\nConfigure development config in truffle.js and launch Ganache (configure them too if needed) and:\n\n\nganache-cli -p 7545 (in first tab)\ntruffle migrate --network development --reset (in second tab)\ntruffle console --network development (in second tab)\n\n\n\n\n\nCreate new registry\n\n\nvar chaingear = Chaingear.at(Chaingear.address)\n\nvar beneficiaries = []\nvar shares = []\nvar buildingFee = 1000000\nvar gas = 10000000\n\nchaingear.registerRegistry(beneficiaries, shares, \nBlockchainRegistry\n, \nBLR\n, \n, EntryCore.bytecode, {value: buildingFee, gas: 10000000})", 
            "title": "Configuring, development and deploying"
        }, 
        {
            "location": "/\u0441haingear/development/#configuring-development-and-deploying", 
            "text": "", 
            "title": "Configuring, development and deploying"
        }, 
        {
            "location": "/\u0441haingear/development/#deploy-contract", 
            "text": "parity ui --chain=kovan\n\ntruffle migrate --network=kovan  PS: approve transaction in parity ui (http://127.0.0.1:8180/)  Build contract in file:  truffle-flattener contracts/common/Chaingeareable.sol   app/src/Chaingeareable.sol", 
            "title": "Deploy contract:"
        }, 
        {
            "location": "/\u0441haingear/development/#linting", 
            "text": "npm install -g solium\n\nsolium -d contracts", 
            "title": "Linting:"
        }, 
        {
            "location": "/\u0441haingear/development/#development-environment", 
            "text": "Recommending to use  Remix Ethereum Online IDE   or  desktop electron-based Remix IDE  PS: to import to IDE open-zeppelin contacts follow this:  import  github.com/openZeppelin/zeppelin-solidity/contracts/ownership/Ownable.sol ;", 
            "title": "Development environment"
        }, 
        {
            "location": "/\u0441haingear/development/#truffle-ganache-workflow", 
            "text": "Install Ganache from  latest release , then =  npm install -g ganache-cli  Configure development config in truffle.js and launch Ganache (configure them too if needed) and:  ganache-cli -p 7545 (in first tab)\ntruffle migrate --network development --reset (in second tab)\ntruffle console --network development (in second tab)", 
            "title": "Truffle + Ganache workflow"
        }, 
        {
            "location": "/\u0441haingear/development/#create-new-registry", 
            "text": "var chaingear = Chaingear.at(Chaingear.address)\n\nvar beneficiaries = []\nvar shares = []\nvar buildingFee = 1000000\nvar gas = 10000000\n\nchaingear.registerRegistry(beneficiaries, shares,  BlockchainRegistry ,  BLR ,  , EntryCore.bytecode, {value: buildingFee, gas: 10000000})", 
            "title": "Create new registry"
        }, 
        {
            "location": "/\u0441haingear/overview/", 
            "text": "Most expensive Registry\n\n\n\n\n  Chaingear is an Ethereum ERC721-based registries framework.\n\n\n\n\n\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n \n\n\n\n\n\n\n  \nBuilt by\n  \ncyber\u2022Search\n and\n  \n\n    contributors\n  \n\n\n\n\n\nOverview\n\n\nThis project allows you to create your own Registry of general purpose entries on Ethereum blockchain.\nEntry type is defined during creation, so you can put into entry any custom logic you want (validation, entry-level permission control). Entries are tokenized as NFT.\n\n\nYour creating your registry in Chaingear - metaregistry, which are one point of access to all registries. Registries on chaingear level are tokenized as NFT. Chaingear is most expensive registry, so you should pay for your registry creation.\n\n\nFeatures\n\n\nChaingear\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\nCustom registry\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\nChaingear in browser\n\n\n\n\n1\n\n\n2\n\n\n3", 
            "title": "Overview"
        }, 
        {
            "location": "/\u0441haingear/overview/#overview", 
            "text": "This project allows you to create your own Registry of general purpose entries on Ethereum blockchain.\nEntry type is defined during creation, so you can put into entry any custom logic you want (validation, entry-level permission control). Entries are tokenized as NFT.  Your creating your registry in Chaingear - metaregistry, which are one point of access to all registries. Registries on chaingear level are tokenized as NFT. Chaingear is most expensive registry, so you should pay for your registry creation.", 
            "title": "Overview"
        }, 
        {
            "location": "/\u0441haingear/overview/#features", 
            "text": "", 
            "title": "Features"
        }, 
        {
            "location": "/\u0441haingear/overview/#chaingear", 
            "text": "1  2  3", 
            "title": "Chaingear"
        }, 
        {
            "location": "/\u0441haingear/overview/#custom-registry", 
            "text": "1  2  3", 
            "title": "Custom registry"
        }, 
        {
            "location": "/\u0441haingear/overview/#chaingear-in-browser", 
            "text": "1  2  3", 
            "title": "Chaingear in browser"
        }
    ]
}